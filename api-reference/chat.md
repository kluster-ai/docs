---
title: Chat API Reference
description: The Chat Completion endpoint generates dynamic, context-aware responses that can be integrated with the Batch API for large-scale processing.
---

## Create chat completion

`POST https://api.kluster.ai/v1/chat/completions`

To create a chat completion, send a request to the `chat/completions` endpoint.

<div class="grid" markdown>
<div markdown>

**Request**

`model` ++"string"++ <span class="required" markdown>++"required"++</span>

ID of the model to use. You can use the `models` endpoint to retrieve the [list of supported models](/api-reference/models/#list-supported-models){target=\_blank}.

---

`messages` ++"array"++ <span class="required" markdown>++"required"++</span>

A list of messages comprising the conversation so far. The `messages` object can be one of `system`, `user`, or `assistant`.

??? child "Show possible types"

    System message ++"object"++
    
    ??? child "Show properties"

        `content` ++"string or array"++

        The contents of the system message.  

        ---
       
        `role` ++"string or null"++ <span class="required" markdown>++"required"++</span>

        The role of the messages author, in this case, `system`.

    ---

    User message ++"object"++

    ??? child "Show properties"

        `content` ++"string or array"++

        The contents of the user message.  

        ---
       
        `role` ++"string or null"++ <span class="required" markdown>++"required"++</span>

        The role of the messages author, in this case, `user`.

    ---

    Assistant message ++"object"++

    ??? child "Show properties"

        `content` ++"string or array"++

        The contents of the assistant message.  

        ---

        `role` ++"string or null"++ <span class="required" markdown>++"required"++</span>

        The role of the messages author, in this case, `assistant`.

---

`frequency_penalty` ++"number or null"++

Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood of repeating the same line verbatim. Defaults to `0`.

---

`logit_bias` ++"map"++

Modify the likelihood of specified tokens appearing in the completion. Defaults to `null`.

Accepts a JSON object that maps tokens (specified by their token ID in the tokenizer) to an associated bias value from -100 to 100. Mathematically, the bias is added to the logits generated by the model prior to sampling. The exact effect will vary per model, but values between -1 and 1 should decrease or increase the likelihood of selection; values like -100 or 100 should result in a ban or exclusive selection of the relevant token.

---

`logprobs` ++"boolean or null"++
   
Whether to return log probabilities of the output tokens or not. If true, returns the log probabilities of each output token returned in the `content` of `message`. Defaults to `false`.

---

`top_logprobs` ++"integer or null"++

An integer between 0 and 20 specifying the number of most likely tokens to return at each token position, each with an associated log probability. `logprobs` must be set to `true` if this parameter is used.

---

`max_completion_tokens` ++"integer or null"++

An upper bound for the number of tokens that can be generated for a completion, including visible output tokens and reasoning tokens.

---

`presence_penalty` ++"number or null"++

Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics. Defaults to `0`.

---

`seed` ++"integer or null"++

If specified, our system will make a best effort to sample deterministically, such that repeated requests with the same `seed` and parameters should return the same result. Determinism is not guaranteed.

---

`stop` ++"string or array or null"++

Up to four sequences where the API will stop generating further tokens. Defaults to `null`.

---

`stream` ++"boolean or null"++

If set, partial message deltas will be sent. Tokens will be sent as data-only server-sent events as they become available, with the stream terminated by a `data: [DONE]` message. Defaults to `false`.

---

`temperature` ++"number or null"++

The sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. Defaults to `1`.

It is generally recommended to alter this or `top_p` but not both.

---

`top_p` ++"number or null"++

An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered. Defaults to `1`.

It is generally recommended to alter this or `temperature` but not both.

---

**Returns**

The created [Chat completion](#batch-object) object.

</div>
<div markdown>

=== "Python"

    ```python title="Example request"
    from openai import OpenAI

    # Configure OpenAI client
    client = OpenAI(
        base_url="https://api.kluster.ai/v1", 
        api_key="INSERT_API_KEY" # Replace with your actual API key
    )

    chat_completion = client.chat.completions.create(
        model="klusterai/Meta-Llama-3.1-8B-Instruct-Turbo",
        messages=[
            {"role": "system", "content": "You are a helpful assistant."},
            {"role": "user", "content": "What is the capital of Argentina?"},
        ],
    )

    print(chat_completion.to_dict())
    ```

=== "curl"

    ```bash title="Example request"
    curl -s https://api.kluster.ai/v1/chat/completions \
        -H "Authorization: Bearer 4532c187-d275-4a6b-940c-5d92f9b20ea6" \
        -H "Content-Type: application/json" \
        -d '{
            "model": "klusterai/Meta-Llama-3.1-8B-Instruct-Turbo",
            "messages": [
                {
                    "role": "system",
                    "content": "You are a helpful assistant."
                },
                {
                    "role": "user",
                    "content": "What is the capital of Argentina?"
                }
            ]
        }'
    ```

```Json title="Response"
{
    "id": "chat-d187c103e189483485b3bcd3eb899c62",
    "object": "chat.completion",
    "created": 1736136422,
    "model": "klusterai/Meta-Llama-3.1-8B-Instruct-Turbo",
    "choices": [
        {
            "index": 0,
            "message": {
                "role": "assistant",
                "content": "The capital of Argentina is Buenos Aires.",
                "tool_calls": []
            },
            "logprobs": null,
            "finish_reason": "stop",
            "stop_reason": null
        }
    ],
    "usage": {
        "prompt_tokens": 48,
        "total_tokens": 57,
        "completion_tokens": 9
    },
    "prompt_logprobs": null
}
```

</div>
</div>

---

## Chat completion object

<div class="grid" markdown>
<div markdown>

`id` ++"string"++

Unique identifier for the chat completion.

---

`object` ++"string"++

The object type, which is always `chat.completion`.

---

`created` ++"integer"++

The Unix timestamp (in seconds) of when the chat completion was created.

---

`model` ++"string"++

The model used for the chat completion. You can use the `models` endpoint to retrieve the [list of supported models](/api-reference/models/#list-supported-models){target=\_blank}.

---

`choices` ++"array"++

A list of chat completion choices.

??? child "Show properties"

    `index` ++"integer"++

    The index of the choice in the list of returned choices.

    ---

    `message` ++"object"++

    A chat completion message generated by the model. Can be one of `system`, `user`, or `assistant`.

    ??? child "Show properties"

        `content` ++"string or array"++

        The contents of the message.  

        ---

        `role` ++"string or null"++

        The role of the messages author. Can be one of `system`, `user`, or `assistant`

    ---

    `logprobs` ++"boolean or null"++

    Whether to return log probabilities of the output tokens or not. If true, returns the log probabilities of each output token returned in the `content` of `message`. Defaults to `false`.

    ---

    `finish_reason` ++"string"++

    The reason the model stopped generating tokens. This will be `stop` if the model hit a natural stop point or a provided stop sequence, `length` if the maximum number of tokens specified in the request was reached, `content_filter` if content was omitted due to a flag from our content filters, `tool_calls` if the model called a tool, or `function_call` (_deprecated_) if the model called a function.

    --- 

    `stop_reason` ++"string or null"++

    The reason the model stopped generating text.

---

`usage` ++"object"++

Usage statistics for the completion request.

??? child "Show properties"

    `completion_tokens` ++"integer"++

    Number of tokens in the generated completion.

    ---

    `prompt_tokens` ++"integer"++

    Number of tokens in the prompt.

    ---

    `total_tokens` ++"integer"++

    Total number of tokens used in the request (prompt + completion).

</div>
<div markdown>

```Json title="Chat completion object"
{
    "id": "chat-d187c103e189483485b3bcd3eb899c62",
    "object": "chat.completion",
    "created": 1736136422,
    "model": "klusterai/Meta-Llama-3.1-8B-Instruct-Turbo",
    "choices": [
        {
            "index": 0,
            "message": {
                "role": "assistant",
                "content": "The capital of Argentina is Buenos Aires.",
                "tool_calls": []
            },
            "logprobs": null,
            "finish_reason": "stop",
            "stop_reason": null
        }
    ],
    "usage": {
        "prompt_tokens": 48,
        "total_tokens": 57,
        "completion_tokens": 9
    },
    "prompt_logprobs": null
}
```

</div>
</div>
