# kluster llms-full.txt
kluster. kluster is the developer AI cloud platform to deploy, scale, and fine-tune models at lightning speed.

## Generated automatically. Do not edit directly.

Documentation: https://docs.kluster.ai/

## List of doc pages:
[API reference](https://raw.githubusercontent.com/kluster-ai/kluster-docs/refs/heads/main/api-reference/reference.md)
[Launch dedicated deployments](https://raw.githubusercontent.com/kluster-ai/kluster-docs/refs/heads/main/get-started/dedicated-deployments.md)
[Fine-tuning with the kluster.ai API](https://raw.githubusercontent.com/kluster-ai/kluster-docs/refs/heads/main/get-started/fine-tuning/api.md)
[Overview of Fine-tuning models](https://raw.githubusercontent.com/kluster-ai/kluster-docs/refs/heads/main/get-started/fine-tuning/overview.md)
[Fine-tuning with the kluster.ai platform](https://raw.githubusercontent.com/kluster-ai/kluster-docs/refs/heads/main/get-started/fine-tuning/platform.md)
[Get a kluster.ai API key](https://raw.githubusercontent.com/kluster-ai/kluster-docs/refs/heads/main/get-started/get-api-key.md)
[Integrate CrewAI with kluster.ai API](https://raw.githubusercontent.com/kluster-ai/kluster-docs/refs/heads/main/get-started/integrations/crewai.md)
[Integrate eliza with kluster.ai](https://raw.githubusercontent.com/kluster-ai/kluster-docs/refs/heads/main/get-started/integrations/eliza.md)
[Integrate Immersive Translate](https://raw.githubusercontent.com/kluster-ai/kluster-docs/refs/heads/main/get-started/integrations/immersive-translate.md)
[Integrate LangChain with kluster.ai](https://raw.githubusercontent.com/kluster-ai/kluster-docs/refs/heads/main/get-started/integrations/langchain.md)
[Integrate LiteLLM with kluster.ai](https://raw.githubusercontent.com/kluster-ai/kluster-docs/refs/heads/main/get-started/integrations/litellm.md)
[Integrate Llama OCR with kluster.ai API](https://raw.githubusercontent.com/kluster-ai/kluster-docs/refs/heads/main/get-started/integrations/llama-ocr.md)
[Integrate Msty with the kluster.ai API](https://raw.githubusercontent.com/kluster-ai/kluster-docs/refs/heads/main/get-started/integrations/msty.md)
[Integrate PydanticAI with the kluster.ai](https://raw.githubusercontent.com/kluster-ai/kluster-docs/refs/heads/main/get-started/integrations/pydantic.md)
[Integrate SillyTavern with kluster.ai](https://raw.githubusercontent.com/kluster-ai/kluster-docs/refs/heads/main/get-started/integrations/sillytavern.md)
[Integrate TypingMind with the kluster.ai API](https://raw.githubusercontent.com/kluster-ai/kluster-docs/refs/heads/main/get-started/integrations/typingmind.md)
[Cloud MCP API usage](https://raw.githubusercontent.com/kluster-ai/kluster-docs/refs/heads/main/get-started/mcp/cloud/api.md)
[Cloud MCP platform management](https://raw.githubusercontent.com/kluster-ai/kluster-docs/refs/heads/main/get-started/mcp/cloud/platform.md)
[Get started with MCP](https://raw.githubusercontent.com/kluster-ai/kluster-docs/refs/heads/main/get-started/mcp/get-started.md)
[MCP client integrations](https://raw.githubusercontent.com/kluster-ai/kluster-docs/refs/heads/main/get-started/mcp/integrations.md)
[MCP integration overview](https://raw.githubusercontent.com/kluster-ai/kluster-docs/refs/heads/main/get-started/mcp/overview.md)
[Self-hosted MCP](https://raw.githubusercontent.com/kluster-ai/kluster-docs/refs/heads/main/get-started/mcp/self-hosted.md)
[MCP tools reference](https://raw.githubusercontent.com/kluster-ai/kluster-docs/refs/heads/main/get-started/mcp/tools.md)
[Supported AI Models](https://raw.githubusercontent.com/kluster-ai/kluster-docs/refs/heads/main/get-started/models.md)
[Compatibility with OpenAI client libraries](https://raw.githubusercontent.com/kluster-ai/kluster-docs/refs/heads/main/get-started/openai-compatibility.md)
[Perform batch inference jobs](https://raw.githubusercontent.com/kluster-ai/kluster-docs/refs/heads/main/get-started/start-building/batch.md)
[Perform real-time inference jobs](https://raw.githubusercontent.com/kluster-ai/kluster-docs/refs/heads/main/get-started/start-building/real-time.md)
[Untitled](https://raw.githubusercontent.com/kluster-ai/kluster-docs/refs/heads/main/get-started/start-building/setup.md)
[Overview of Verify](https://raw.githubusercontent.com/kluster-ai/kluster-docs/refs/heads/main/get-started/verify/overview.md)
[Chat completion Verify API](https://raw.githubusercontent.com/kluster-ai/kluster-docs/refs/heads/main/get-started/verify/reliability/chat-completion.md)
[Reliability check by Verify](https://raw.githubusercontent.com/kluster-ai/kluster-docs/refs/heads/main/get-started/verify/reliability/overview.md)
[Verify API endpoint](https://raw.githubusercontent.com/kluster-ai/kluster-docs/refs/heads/main/get-started/verify/reliability/verify-api.md)
[Workflow Integrations](https://raw.githubusercontent.com/kluster-ai/kluster-docs/refs/heads/main/get-started/verify/reliability/workflow-integrations.md)

## Full content for each doc page

Doc-Content: https://docs.kluster.ai/api-reference/reference/
--- BEGIN CONTENT ---
---
title: API reference
description: The kluster.ai API reference includes endpoints, available methods, required parameters, and response format information for kluster.ai's OpenAI-compatible API.
hide:
- nav
- footer
template: portal.html
---
--- END CONTENT ---

Doc-Content: https://docs.kluster.ai/get-started/dedicated-deployments/
--- BEGIN CONTENT ---
---
title: Launch dedicated deployments
description: Deploy private model instances on kluster.ai with dedicated hardware, full control, and transparent per-hour pricing. No per-token fees, just predictable costs.
---

# Dedicated deployments

Dedicated deployments let you run a private instance of any [Hugging Face text model](https://huggingface.co/models){target=\_blank} on hardware reserved just for you. Enjoy full control, predictable per‑minute billing, and zero per‑token costs.

This page covers how to create, use, and stop your dedicated deployments.

## Create a deployment

Ensure you're logged in to the [kluster.ai platform](https://platform.kluster.ai){target=\_blank}, then navigate to the [**Dedicated deployments**](https://platform.kluster.ai/dedicated-deployments){target=\_blank} page, then press **Launch deployment**.

![Launch deployment](/images/get-started/dedicated-endpoints/dedicated-1.webp)

Then, complete the following fields to configure your deployment:

1. **Deployment name**: Enter a clear deployment name (e.g., `mydedicated`) so you can spot it later in the console.
2. **Model selection**: Paste the Hugging Face model ID or URL (e.g., `deepseek-ai/DeepSeek-R1`). If the model is private, provide a Hugging Face access token.
3. **Select hardware**: Confirm a GPU configuration.
4. **Specify auto-shutdown**: Set an auto‑shutdown window for your instance to power down after a specified period of inactivity, between 15 minutes to 12 hours. 
4. **Launch**: Review the estimated price and then Click **Launch deployment**. Spin‑up takes ≈20–30 min; once the status shows `Running`, copy the endpoint ID, as you'll use that to submit requests. 

![Configure deployment](/images/get-started/dedicated-endpoints/dedicated-2.webp)


## Use your dedicated deployment

After waiting 20-30 minutes for your instance to spin up, you can call it by using the endpoint ID as the model name when making a request. If you're unsure of your endpoint ID, look for it in the [**Dedicated deployments** page](https://platform.kluster.ai/dedicated-deployments){target=\_blank}.

![Copy endpoint ID](/images/get-started/dedicated-endpoints/dedicated-3.webp)

To call your dedicated deployment, you'll need to provide the endpoint ID as the model name when making a request (`INSERT_ENDPOINT_ID` in the following example):

=== "Python"

    ```python
    from openai import OpenAI

    client = OpenAI(
        api_key="YOUR_API_KEY",
        base_url="https://api.kluster.ai/v1"
    )

    response = client.chat.completions.create(
        model="INSERT_ENDPOINT_ID",   # Your endpoint ID
        messages=[{"role": "user", "content": "What is the best taco place in SF?"}],
    )

    print(response.choices[0].message.content)
    ```

=== "curl"

    ```bash
    curl https://api.kluster.ai/v1/chat/completions \
      -H "Authorization: Bearer $API_KEY" \
      -H "Content-Type: application/json" \
      -d '{
        "model": "INSERT_ENDPOINT_ID",
        "messages": [{"role": "user", "content": "What is the best taco place in SF?"}]
      }'
    ```

## Stop your deployment

Click **Stop** next to your deployment on the [**Dedicated deployments**](https://platform.kluster.ai/dedicated-deployments){target=\_blank} page to shut your VM down immediately. Billing ends the moment it powers off. 

Otherwise, an auto‑shutdown timer kicks in after your specified auto-shutdown period (between 15 minutes and 12 hours of inactivity), depending on the period you chose when spinning up the instance. 

![Stop deployment](/images/get-started/dedicated-endpoints/dedicated-4.webp)

Questions? Email [support@kluster.ai](mailto:support@kluster.ai), and we’ll be happy to help!
--- END CONTENT ---

Doc-Content: https://docs.kluster.ai/get-started/fine-tuning/api/
--- BEGIN CONTENT ---
---
title: Fine-tuning with the kluster.ai API
description: Learn how to programmatically create custom models tailored to your specific tasks by fine-tuning foundation models with your own data using the kluster.ai API.
---

# Fine-tuning with the kluster.ai API

The [kluster.ai](https://www.kluster.ai/){target=\_blank} API lets you automate and integrate fine-tuning into your development workflows. You can create, manage, and monitor fine-tuning jobs directly from your code, making it easy to customize models for your specific needs.

This guide provides a practical overview of the fine-tuning process using the API. It covers the required data format, how to upload your dataset, and how to launch and monitor a fine-tuning job. For a step-by-step walkthrough, see the linked tutorial in the tips below.

## Prerequisites

Before getting started with fine-tuning, ensure you have the following:

- **A kluster.ai account**: Sign up on the [kluster.ai platform](https://platform.kluster.ai/signup){target=\_blank} if you don't have one.
- **A kluster.ai API key**: After signing in, go to the [**API Keys**](https://platform.kluster.ai/apikeys){target=\_blank} section and create a new key. For detailed instructions, check out the [Get an API key](/get-started/get-api-key/){target=\_blank} guide.
- **Prepared dataset**: You need data formatted according to kluster.ai's requirements for fine-tuning (detailed below).

## Supported models

kluster.ai currently supports fine-tuning for the following models:

- [klusterai/Meta-Llama-3.1-8B-Instruct-Turbo](/get-started/models/){target=_blank}
- [klusterai/Meta-Llama-3.3-70B-Instruct-Turbo](/get-started/models/){target=_blank}

!!! note
    You can query the [models endpoint](/api-reference/reference/#/http/api-endpoints/models/v1-models-get){target=_blank} in the API and filter for the tag "fine-tunable."

## Fine-tuning workflow

Fine‑tuning a model with the kluster.ai API follows a straightforward five‑step workflow:

1. **Prepare your data**: Collect and structure high‑quality JSONL training examples that reflect the task you want the model to learn.
2. **Upload your training file**: Send the JSONL file to kluster.ai and note the returned `file_id`.
3. **Create the fine‑tuning job**: Launch a fine‑tuning job specifying the base model and training `file_id` (plus any optional hyperparameters).
4. **Monitor job progress**: Poll the job endpoint (or subscribe to webhooks) until the job reaches the `succeeded` state.
5. **Use your fine‑tuned model**: Invoke the model name returned by the job for inference in your application or the kluster.ai playground.

The following sections will provide a closer look at each step.

### Prepare your data

High-quality, well-formatted data is crucial for successful fine-tuning:

- **Format**: Data must be in JSONL format, where each line is a valid JSON object representing a training example.
- **Structure**: Each JSON object should contain a `messages` array with system, user, and assistant messages.
- **Example format**:

    ```json
    {
      "messages": [
        {
          "role": "system",
          "content": "You are a JSON Generation Specialist. Convert user requests into properly formatted JSON."
        },
        {
          "role": "user",
          "content": "Create a configuration for a web application with name 'TaskMaster', version 1.2.0, and environment set to development."
        },
        {
          "role": "assistant",
          "content": "{\n  \"application\": {\n    \"name\": \"TaskMaster\",\n    \"version\": \"1.2.0\",\n    \"environment\": \"development\"\n  }\n}"
        }
      ]
    }
    ```

- **Quantity**: The minimum requirement is 10 examples, but more diverse and high-quality examples yield better results.
- **Quality**: Ensure your data accurately represents the task you want the model to perform.

!!! tip "Data preparation"
    For a detailed walkthrough of data preparation, see the [Fine-tuning sentiment analysis tutorial](/tutorials/klusterai-api/finetuning-sent-analysis/#get-the-data){target=_blank}.

!!! example "Find Llama datasets on Hugging Face"
    There is a wide range of datasets suitable for Llama model fine-tuning on [Hugging Face Datasets](https://huggingface.co/datasets?sort=trending&search=llama){target=_blank}. Browse trending and community-curated datasets to accelerate your data preparation.

### Set up the client

First, install the OpenAI Python library:

```bash
pip install openai
```

Then initialize the client with the kluster.ai base URL:

```python
from getpass import getpass

from openai import OpenAI

api_key = getpass("Enter your kluster.ai API key: ")

# Set up the client
client = OpenAI(
    base_url="https://api.kluster.ai/v1",
    api_key=api_key
)
```

### Upload your training file

Once your data is prepared, upload it to the kluster.ai platform:

```python
# Upload fine-tuning file (for files under 100MB)
with open('training_data.jsonl', 'rb') as file:
    upload_response = client.files.create(
        file=file,
        purpose="fine-tune"  # Important: specify "fine-tune" as the purpose
    )
    
    # Get the file ID
    file_id = upload_response.id
    print(f"File uploaded successfully. File ID: {file_id}")
```

!!! warning "File size & upload limits"
    Each fine-tuning file must be ≤ 100 MB on both the free and standard tiers (the standard tier simply allows more total examples).  
    When your dataset approaches this limit, use the [chunked upload](/tutorials/klusterai-api/uploads-api/){target=_blank} method for reliable multi-part uploads.

### Create a fine-tuning job

After uploading your data, initiate the fine-tuning job:

```python
# Model
model = "klusterai/Meta-Llama-3.1-8B-Instruct-Turbo"

# Create fine-tune job
fine_tuning_job = client.fine_tuning.jobs.create(
    training_file=file_id,
    model=model,
    # Optional hyperparameters
    # hyperparameters={
    #   "batch_size": 3,
    #   "n_epochs": 2,
    #   "learning_rate_multiplier": 0.08
    # }
)
```

### Monitor job progress

Track the status of your fine-tuning job:

```python
# Retrieve job status
job_status = client.fine_tuning.jobs.retrieve(fine_tuning_job.id)
print(f"Job status: {job_status.status}")
```

### Use your fine-tuned model

Once your fine-tuning job completes successfully, you will receive a unique fine-tuned model name that you can use for inference:

```python
# Get the fine-tuned model name
finished_job = client.fine_tuning.jobs.retrieve(fine_tuning_job.id)
fine_tuned_model = finished_job.fine_tuned_model

# Use the fine-tuned model for inference
response = client.chat.completions.create(
    model=fine_tuned_model,
    messages=[
        {"role": "system", "content": "You are a JSON Generation Specialist. Convert user requests into properly formatted JSON."},
        {"role": "user", "content": "Create a configuration for a web application with name 'TaskMaster', version 1.2.0, and environment set to development."}
    ]
)
```

You can view the end-to-end python script below:

??? code "fine-tune.py"

    ```python
    from getpass import getpass

from openai import OpenAI

api_key = getpass("Enter your kluster.ai API key: ")

# Set up the client
client = OpenAI(
    base_url="https://api.kluster.ai/v1",
    api_key=api_key
)

# Upload fine-tuning file (for files under 100MB)
with open('training_data.jsonl', 'rb') as file:
    upload_response = client.files.create(
        file=file,
        purpose="fine-tune"  # Important: specify "fine-tune" as the purpose
    )
    
    # Get the file ID
    file_id = upload_response.id
    print(f"File uploaded successfully. File ID: {file_id}")

# Model
model = "klusterai/Meta-Llama-3.1-8B-Instruct-Turbo"

# Create fine-tune job
fine_tuning_job = client.fine_tuning.jobs.create(
    training_file=file_id,
    model=model,
    # Optional hyperparameters
    # hyperparameters={
    #   "batch_size": 3,
    #   "n_epochs": 2,
    #   "learning_rate_multiplier": 0.08
    # }
)

# Retrieve job status
job_status = client.fine_tuning.jobs.retrieve(fine_tuning_job.id)
print(f"Job status: {job_status.status}")

# Get the fine-tuned model name
finished_job = client.fine_tuning.jobs.retrieve(fine_tuning_job.id)
fine_tuned_model = finished_job.fine_tuned_model

# Use the fine-tuned model for inference
response = client.chat.completions.create(
    model=fine_tuned_model,
    messages=[
        {"role": "system", "content": "You are a JSON Generation Specialist. Convert user requests into properly formatted JSON."},
        {"role": "user", "content": "Create a configuration for a web application with name 'TaskMaster', version 1.2.0, and environment set to development."}
    ]
)
    ```

### Use your fine-tuned model in the playground (optional)

After your fine-tuned model is created, you can also test it in the kluster.ai playground:

1. Go to the [kluster.ai playground](https://platform.kluster.ai/playground){target=_blank}.
2. Select your fine-tuned model from the model dropdown menu.
3. Start chatting with your model to evaluate its performance on your specific task.

## Benefits of fine-tuning

Fine-tuning offers several advantages over using general-purpose models:

- **Improved performance**: Fine-tuned models often outperform base models on specific tasks.
- **Cost efficiency**: Smaller fine-tuned models can outperform larger models at a lower cost.
- **Reduced latency**: Fine-tuned models can deliver faster responses for your applications.
- **Consistency**: More reliable outputs tailored to your specific task or domain.

## Next steps

- **Detailed tutorial**: Follow the [Fine-tuning sentiment analysis tutorial](/tutorials/klusterai-api/finetuning-sent-analysis/#get-the-data){target=_blank}.
- **API reference**: Review the [API reference documentation](/api-reference/reference/#/http/api-endpoints/fine-tuning/v1-fine-tuning-jobs-post){target=_blank} for all fine-tuning related endpoints.
- **Explore models**: See the [Models](/get-started/models/){target=_blank} page to check which foundation models support fine-tuning.
- **Platform approach**: Try the [user-friendly platform interface](/get-started/fine-tuning/platform/){target=_blank} for fine-tuning without writing code.
--- END CONTENT ---

Doc-Content: https://docs.kluster.ai/get-started/fine-tuning/overview/
--- BEGIN CONTENT ---
---
title: Overview of Fine-tuning models
description: Learn how to create custom models tailored to your specific tasks by fine-tuning foundation models with your own data using the kluster.ai platform.
---

# Overview of fine-tuning models

Fine-tuning lets you transform general-purpose models into specialized AI assistants that excel at your unique tasks. With [kluster.ai](https://www.kluster.ai/){target=\_blank}, you can fine-tune models using either the [platform](/get-started/fine-tuning/platform/) or the [API](/get-started/fine-tuning/api/)—choose the path that fits your workflow and technical needs.

## Fine-tuning flow

Fine-tuning on kluster.ai follows a simple, seven-step loop:

1. **Prepare your dataset**: Collect representative examples for the task and save them as a JSON Lines (`.jsonl`) file.  
2. **Upload the dataset**: Use the Platform “Upload” dialog **or** call `files.upload` via the API to receive a `file_id`.  
3. **Configure & launch a job**: Choose a base model, set LoRA-specific (Low-Rank Adaptation) hyper-parameters (epochs, learning rate, adapter rank, etc.), and start the job.  
4. **Monitor training**: Track status and metrics in the dashboard or poll `fine_tuning.jobs.retrieve` until the job reaches `succeeded` or `failed`.  
5. **Retrieve the fine-tuned model**: When the job finishes, grab the returned `fine_tuned_model` ID and treat it like any other model.  
6. **Evaluate & iterate**: Test the model on unseen prompts, compare against the base model, and re-run fine-tuning with refined data or parameters if needed. 
7. **Deploy & integrate**: Call the model in production, export its LoRA adapter, or share it with teammates through kluster.ai’s model registry.  

Fine-tuning is your go-to when you need **reliable, domain-specific outputs** (e.g., JSON-formatted responses, brand-aligned tone) that prompt engineering alone can’t guarantee.

## When to fine-tune your model

Fine-tuning is ideal for scenarios where you need:

- **Domain specialization**: Create models that excel in specific fields like medicine, law, finance, or technical documentation.
- **Brand-aligned responses**: Train models to match your company's voice, style, and communication guidelines.
- **Format consistency**: Ensure reliable output in specific formats like JSON, XML, or Markdown.
- **Enhanced reasoning**: Improve analytical capabilities for specific types of problems.
- **Custom behavior**: Develop assistants that follow your unique processes and workflows.

## Benefits of fine-tuning

Fine-tuning delivers several key advantages over using general-purpose models:

- **Improved performance**: Fine-tuned models consistently outperform base models on specific tasks.
- **Cost efficiency**: Smaller fine-tuned models can match or exceed the performance of larger models at a lower cost.
- **Reduced latency**: Fine-tuned models provide faster responses, enhancing the user experience.
- **Consistency**: Achieve more reliable outputs tailored to your specific requirements.
- **Data privacy**: Train models on your data without exposing sensitive information in prompts.

## Supported models

kluster.ai currently supports fine-tuning for the following models:

- [klusterai/Meta-Llama-3.1-8B-Instruct-Turbo](/get-started/models/){target=_blank}
- [klusterai/Meta-Llama-3.3-70B-Instruct-Turbo](/get-started/models/){target=_blank}

## Choose your fine-tuning approach

kluster.ai offers two ways to fine-tune models, each designed for different user preferences and requirements:

<div class="grid cards" markdown>

-   <span class="badge guide">Guide</span> __Platform__

    ---

    Use the platform to fine-tune without writing code. The platform is ideal for users who want a guided, interactive experience and real-time feedback on training progress.

    [:octicons-arrow-right-24: Visit the guide](/get-started/fine-tuning/platform/){target=_blank}

-   <span class="badge guide">Guide</span> __API__

    ---

    Fine-tune models with code for maximum flexibility and automation. The API is best for developers who need advanced customization, integration, or workflow automation.

    [:octicons-arrow-right-24: Visit the guide](/get-started/fine-tuning/api/){target=_blank}

</div>

## Additional resources

- **Step-by-step tutorial**: Learn the fundamentals with our [Fine-tuning sentiment analysis tutorial](/tutorials/klusterai-api/finetuning-sent-analysis/){target=_blank}.
- **Available models**: Explore our [Models](/get-started/models/){target=_blank} page to see all foundation models that support fine-tuning.
- **API reference**: Review the complete [API documentation](/api-reference/reference/#/http/api-endpoints/fine-tuning/v1-fine-tuning-jobs-post){target=_blank} for all fine-tuning related endpoints.
--- END CONTENT ---

Doc-Content: https://docs.kluster.ai/get-started/fine-tuning/platform/
--- BEGIN CONTENT ---
---
title: Fine-tuning with the kluster.ai platform
description: Learn how to create custom models tailored to your specific tasks by fine-tuning foundation models with your own data using the kluster.ai visual interface.
---

# Fine-tuning with the kluster.ai platform

The [kluster.ai platform](https://platform.kluster.ai/){target=\_blank} provides a visual, no-code approach to fine-tuning AI models. With an intuitive interface and real-time feedback, you can train customized models without writing a single line of code.

This guide walks you through the platform's fine-tuning workflow, from uploading your training data to deploying your specialized model.

## Prerequisites

Before getting started with fine-tuning, ensure you have the following:

- **A kluster.ai account**: Sign up on the [kluster.ai platform](https://platform.kluster.ai/signup){target=\_blank} if you don't have one.
- **A kluster.ai API key**: After signing in, go to the [**API Keys**](https://platform.kluster.ai/apikeys){target=\_blank} section and create a new key. For detailed instructions, check out the [Get an API key](/get-started/get-api-key/){target=\_blank} guide.
- **Prepared dataset**: You need data formatted according to kluster.ai's requirements for fine-tuning (detailed below).

## Supported models

kluster.ai currently supports fine-tuning for the following models:

- [klusterai/Meta-Llama-3.1-8B-Instruct-Turbo](/get-started/models/){target=_blank}
- [klusterai/Meta-Llama-3.3-70B-Instruct-Turbo](/get-started/models/){target=_blank}

## Data preparation

High-quality, well-formatted data is crucial for successful fine-tuning:

- **Format**: Data must be in JSONL format, where each line is a valid JSON object representing a training example.
- **Structure**: Each JSON object should contain a `messages` array with system, user, and assistant messages.
- **Example format**:

    ```json
    {
      "messages": [
        {
          "role": "system",
          "content": "You are a JSON Generation Specialist. Convert user requests into properly formatted JSON."
        },
        {
          "role": "user",
          "content": "Create a configuration for a web application with name 'TaskMaster', version 1.2.0, and environment set to development."
        },
        {
          "role": "assistant",
          "content": "{\n  \"application\": {\n    \"name\": \"TaskMaster\",\n    \"version\": \"1.2.0\",\n    \"environment\": \"development\"\n  }\n}"
        }
      ]
    }
    ```


- **Quantity**: The minimum requirement is 10 examples, but more diverse and high-quality examples yield better results.
- **Quality**: Ensure your data accurately represents the task you want the model to perform.

!!! tip "Data preparation"
    For a detailed walkthrough of data preparation, see the [Fine-tuning Sentiment Analysis Tutorial](https://docs.kluster.ai/tutorials/klusterai-api/finetuning-sent-analysis/#get-the-data){target=_blank}.

!!! example "Find Llama datasets on Hugging Face"
    There is a wide range of datasets suitable for Llama model fine-tuning on [Hugging Face Datasets](https://huggingface.co/datasets?sort=trending&search=llama){target=_blank}. Browse trending and community-curated datasets to accelerate your data preparation.

## Fine-tuning workflow

The process of fine-tuning a model using the kluster.ai platform interface involves several key steps which will be described in more detail in the following sections.

### Navigate to the fine-tuning page

To begin, visit the [kluster.ai fine-tuning page](https://platform.kluster.ai/fine-tuning){target=_blank} and click the **Create new job** button.

![Create new fine-tuning job](/images/get-started/fine-tuning/fine-tuning-1.webp)

### Choose model and upload data

  1. **Select a base model**: Choose one of the available foundation models from the dropdown menu.
  2. **Upload your file**: Upload your prepared JSONL training file by dragging and dropping the file or using the file selection dialog.

![Select base model](/images/get-started/fine-tuning/fine-tuning-2.webp)

!!! note "Validation data"
    You can optionally upload a validation dataset in the same format as your training data. This helps evaluate the performance of your model during training.

### Configure hyperparameters

Customize your fine-tuning job by configuring these settings:

  1. **Nickname**: Add an optional custom suffix that will be appended to your fine-tuned model name.
  2. **Batch size**: Control how many examples are processed in each training step.
  3. **Learning rate multiplier**: Adjust how quickly the model adapts to your training data.
  4. **Number of epochs**: Define how many times the model will cycle through your entire dataset.
  5. **Create**: Click the **Create** button to start the fine-tuning process.

   ![Configure hyperparameters](/images/get-started/fine-tuning/fine-tuning-3.webp)

### Monitor job progress

After submitting your fine-tuning job, you can monitor the status and progress of your job on the [fine-tuning page](https://platform.kluster.ai/fine-tuning){target=_blank}.

Each job displays information including:

- Job ID
- Base model
- Training method
- Creation date
- Current status
- Training metrics (when complete)

![Job Progress](/images/get-started/fine-tuning/fine-tuning-4.webp)

!!! tip "Status Update"
    The job status updates will first display "queued," then "running," and "succeeded" when complete.

### Access your fine-tuned model

Once fine-tuning is complete, your custom model will be listed on the fine-tuning page with its unique identifier and available in the model selection dropdown in the [playground](https://platform.kluster.ai/playground){target=_blank}.

![Playground Chat](/images/get-started/fine-tuning/fine-tuning-5.webp)

## Benefits of fine-tuning

Fine-tuning offers several advantages over using general-purpose models:

- **Improved performance**: Fine-tuned models often outperform base models on specific tasks.
- **Cost efficiency**: Smaller fine-tuned models can outperform larger models at a lower cost.
- **Reduced latency**: Fine-tuned models can deliver faster responses for your applications.
- **Consistency**: More reliable outputs tailored to your specific task or domain.

## Next steps

- **Detailed tutorial**: Follow the [Fine-tuning sentiment analysis tutorial](/tutorials/klusterai-api/finetuning-sent-analysis/#get-the-data){target=_blank}.
- **API reference**: Review the [API reference documentation](/api-reference/reference/#/http/api-endpoints/fine-tuning/v1-fine-tuning-jobs-post){target=_blank} for all fine-tuning related endpoints.
- **Explore models**: See the [Models](/get-started/models/){target=_blank} page to check which foundation models support fine-tuning.
- **API approach**: Learn how to [fine-tune models programmatically](/get-started/fine-tuning/api/){target=_blank} with the kluster.ai API.
--- END CONTENT ---

Doc-Content: https://docs.kluster.ai/get-started/get-api-key/
--- BEGIN CONTENT ---
---
title: Get a kluster.ai API key
description: Follow step-by-step instructions to generate and manage API keys, enabling secure access to kluster's services and seamless integration with your applications.
---

# Generate your kluster.ai API key

The API key is a unique identifier that authenticates requests associated with your account. You must have at least one API key to access [kluster.ai](https://www.kluster.ai/){target=\_blank}'s services.

This guide will help you obtain an API key, the first step to leveraging kluster.ai's powerful and cost-effective AI capabilities.

## Create an account

If you haven't already created an account with kluster.ai, visit the [registration page](https://platform.kluster.ai/signup){target=\_blank} and take the following steps:

1. Enter your full name.
2. Provide a valid email address.
3. Create a secure password.
4. Click the **Sign up** button.

![Signup Page](/images/get-started/get-api-key/get-api-key-1.webp)

## Generate a new API key

After you've signed up or logged into the platform through the [login page](https://platform.kluster.ai/login){target=\_blank}, take the following steps:

1. Select **API Keys** on the left-hand side menu.
2. In the [**API Keys**](https://platform.kluster.ai/apikeys){target=\_blank} section, click the **Issue New API Key** button.

    ![Issue New API Key](/images/get-started/get-api-key/get-api-key-2.webp)

3. Enter a descriptive name for your API key in the popup, then click **Create Key**.

    ![Generate API Key](/images/get-started/get-api-key/get-api-key-3.webp)

## Copy and secure your API key

1. Once generated, your API key will be displayed.
2. Copy the key and store it in a secure location, such as a password manager.

    !!! warning "Warning"
        For security reasons, you won't be able to view the key again. If lost, you will need to generate a new one.

![Copy API key](/images/get-started/get-api-key/get-api-key-4.webp)

!!! abstract "Security tips"
    - **Keep it secret**: Do not share your API key publicly or commit it to version control systems.
    - **Use environment variables**: Store your API key in environment variables instead of hardcoding them.
    - **Regenerate if compromised**: If you suspect your API key has been exposed, regenerate it immediately from the **API Keys** section.

## Managing your API keys

The **API Key Management** section allows you to efficiently manage your kluster.ai API keys. You can create, view, and delete API keys by navigating to the [**API Keys**](https://platform.kluster.ai/apikeys){target=\_blank} section. Your API keys will be listed in the **API Key Management** section.

To delete an API key, take the following steps:

1. Locate the API key you wish to delete in the list.
2. Click the trash bin icon ( :octicons-trash-24: ) in the **Actions** column.
3. Confirm the deletion when prompted.

![Delete API key](/images/get-started/get-api-key/get-api-key-5.webp)

!!! warning "Warning"
    Once deleted, the API key cannot be used again and you must generate a new one if needed.

## Next steps

Now that you have your API key, you can start integrating kluster.ai's LLMs into your applications. Refer to our [Getting Started](/get-started/start-building/setup/){target=\_blank} guide for detailed instructions on using the API.
--- END CONTENT ---

Doc-Content: https://docs.kluster.ai/get-started/integrations/crewai/
--- BEGIN CONTENT ---
---
title: Integrate CrewAI with kluster.ai API
description: Learn how to integrate kluster.ai with CrewAI, a new framework for orchestrating autonomous AI agents, to launch and configure your AI agent chatbot.
---

# Integrate CrewAI with kluster.ai

[CrewAI](https://www.crewai.com/){target=\_blank} is a multi-agent platform that organizes specialized AI agents—each with defined roles, tools, and goals—within a structured process to tackle complex tasks efficiently. CrewAI agents streamline workflows and deliver reliable, scalable solutions by coordinating tasks and ensuring smooth collaboration.

This guide walks you through integrating [kluster.ai](https://www.kluster.ai/){target=\_blank} with CrewAI to create and run a simple AI agent chatbot that leverages the kluster.ai API.

## Prerequisites

Before starting, ensure you have the following prerequisites:

- **A kluster.ai account**: Sign up on the [kluster.ai platform](https://platform.kluster.ai/signup){target=\_blank} if you don't have one.
- **A kluster.ai API key**: After signing in, go to the [**API Keys**](https://platform.kluster.ai/apikeys){target=\_blank} section and create a new key. For detailed instructions, check out the [Get an API key](/get-started/get-api-key/){target=\_blank} guide.
- **CrewAI installed**: The [Installation Guide](https://docs.crewai.com/installation){target=\_blank} on the CrewAI website will walk you through installing CrewAI, setting up a virtual Python environment, and creating a new project. Note that CrewAI requires a Python version >=`3.10` and <`3.13`.

## Create a project with the CLI

Open your Python virtual environment, and then follow these steps to use the CrewAI CLI to create a new project:

1. **Create a project**: Following the installation guide, create your first project with the following command:
```bash
crewai create crew INSERT_PROJECT_NAME
```
2. **Select model and provider**: During setup, the CLI will ask you to choose a provider and a model. Select `openai` as the provider and then choose any available model. Because you'll configure kluster.ai as a custom model, your initial model choice won't affect the final integration. The CLI will prompt you for an OpenAI API key, but this isn’t required. Simply press enter to skip.

## Build a simple AI agent

After finishing the CLI setup, you will see a `src` directory with files `crew.py` and `main.py`. This guide won't use these sample files because they include extra features outside the scope. Follow these steps to continue:

1. **Create your first file**: Create a `hello_crew.py` file in `src/YOUR_PROJECT_NAME` to correspond to a simple AI agent chatbot.
2. **Import modules and select model**: Open `hello_crew.py` to add imports and define a custom LLM for kluster.ai by setting the following parameters:
    - **`provider`**: You can specify `openai_compatible`.
    - **`model`**: Choose one of [kluster.ai's available models](/get-started/models/){target=\_blank} based on your use case. Regardless of which model you choose, prepend its name with `openai/` to ensure CrewAI, which relies on LiteLLM, processes your requests correctly.

    - **`base_url`**: Use `https://api.kluster.ai/v1` to send requests to the kluster.ai endpoint.
    - **`api_key`**: Replace `INSERT_API_KEY` in the code below with your kluster.ai API key. If you don't have one yet, refer to the [Get an API key guide](/get-started/get-api-key/){target=\_blank}.
  
    ```python title="hello_crew.py"
    
    ```

    This example overrides `agents_config` and `tasks_config` with empty dictionaries to tell CrewAI to ignore all YAML files and rely solely on your code, keeping this guide as streamlined as possible. 

3. **Define your agent**: Set the agent's role, goal, and backstory, and assign the custom LLM (via the kluster.ai API) for generating creative greetings:

    ```python title="hello_crew.py"
    @agent
    def hello_agent(self) -> Agent:
        """
        A super simple agent with a single purpose: greet the user in a friendly, varied way.
        """
        return Agent(
            role="HelloWorldAgent",
            goal="Greet the user in a fun and creative way.",
            backstory="I'm a friendly agent who greets everyone in a slightly different manner!",
            llm=self.custom_llm,
            verbose=True
        )
    ```

4. **Give the agent a task**: Define a task that prompts the agent for a unique, creative greeting using randomness to avoid repetition. Passing this prompt to `hello_agent()` ensures varied responses. CrewAI requires an `expected_output` field, defined here as a short greeting:

    ```python title="hello_crew.py"
    @task
    def hello_task(self) -> Task:
        """
        A task that asks the agent to produce a dynamic greeting.
        """
        random_factor = random.randint(100000, 999999)
        prompt = f"""
        You are a friendly greeting bot. 
        Please produce a short, creative greeting that changes each time. 
        Random factor: {random_factor}
        Example: "Hey there, how's your day going?"
        """

        return Task(
            description=prompt,
            expected_output="A short, creative greeting",
            agent=self.hello_agent()
        )
    ```

5. **Tie it all together with a `@crew` method**: Add the following method to return the assembled Crew object with a single agent and task. This method enables CrewAI to coordinate the agent and task you defined:

    ```python title="hello_crew.py"
    @crew
    def hello_crew(self) -> Crew:
        """
        Our entire 'Hello World' crew—only 1 agent + 1 task in sequence.
        """
        return Crew(
            agents=self.agents,  
            tasks=self.tasks,    
            process=Process.sequential,
            verbose=True
        )
    ```

6. **Set up the entry point for the agent**: Create a new file named `hello_main.py`. In `hello_main.py`, import and initialize the `HelloWorldCrew` class, call its `hello_crew()` method, and then `kickoff()` to launch the task sequence:

    ```python title="hello_main.py"
    #!/usr/bin/env python
    from hello_crew import HelloWorldCrew


    def run():
        """
        Kick off the HelloWorld crew with no inputs.
        """
        HelloWorldCrew().hello_crew().kickoff(inputs={})

    if __name__ == "__main__":
        run()

    ```

??? code "View complete script"
    ```python title="hello_crew.py"
    import random

from crewai import LLM, Agent, Crew, Process, Task
from crewai.project import CrewBase, agent, crew, task


@CrewBase
class HelloWorldCrew:
    # Override any default YAML references
    agents_config = {}
    tasks_config = {}

    def __init__(self):
        """
        When this crew is instantiated, create a custom LLM with your base_url.
        """
        self.custom_llm = LLM(
            provider="openai_compatible", 
            model="openai/klusterai/Meta-Llama-3.3-70B-Instruct-Turbo",
            base_url="https://api.kluster.ai/v1",
            api_key="INSERT_KLUSTER_API_KEY"
        )

    @agent
    def hello_agent(self) -> Agent:
        """
        A super simple agent with a single purpose: greet the user in a friendly, varied way.
        """
        return Agent(
            role="HelloWorldAgent",
            goal="Greet the user in a fun and creative way.",
            backstory="I'm a friendly agent who greets everyone in a slightly different manner!",
            llm=self.custom_llm,
            verbose=True
        )

    @task
    def hello_task(self) -> Task:
        """
        A task that asks the agent to produce a dynamic greeting.
        """
        random_factor = random.randint(100000, 999999)
        prompt = f"""
        You are a friendly greeting bot. 
        Please produce a short, creative greeting that changes each time. 
        Random factor: {random_factor}
        Example: "Hey there, how's your day going?"
        """

        return Task(
            description=prompt,
            expected_output="A short, creative greeting",
            agent=self.hello_agent()
        )

    @crew
    def hello_crew(self) -> Crew:
        """
        Our entire 'Hello World' crew—only 1 agent + 1 task in sequence.
        """
        return Crew(
            agents=self.agents,  
            tasks=self.tasks,    
            process=Process.sequential,
            verbose=True
        )
    ```

## Put it all together

To run your agent, ensure you are in the same directory as your `hello_main.py` file, then use the following command:

```bash
python hello_main.py
```

Upon running the script, you'll see output that looks like the following:

<div id="termynal" data-termynal>
<span data-ty></span># Agent: HelloWorldAgent</span>
<span data-ty>## Task:</span>
<span data-ty>You are a friendly greeting bot.</span>
<span data-ty>Please produce a short, creative greeting that changes each time.</span>
<span data-ty>Random factor: 896380</span>
<span data-ty>Example: "Hey there, how's your day going?"</span>
<br>
<span data-ty></span># Agent: HelloWorldAgent</span>
<span data-ty>## Final Answer:</span>
<span data-ty>Hello, it's a beautiful day to shine, how's your sparkle today?</span>
</div>

And that's it! You've now successfully configured your AI agent harnessing CrewAI and the power of the kluster.ai API!
--- END CONTENT ---

Doc-Content: https://docs.kluster.ai/get-started/integrations/eliza/
--- BEGIN CONTENT ---
---
title: Integrate eliza with kluster.ai
description: Learn how to integrate kluster.ai with eliza, a fast, lightweight, and flexible AI agent framework, to launch and configure your own AI agent chatbot. 
---

# Integrate eliza with kluster.ai

[eliza](https://eliza.how/){target=\_blank} is an open-source framework designed to create and manage AI agents that can handle a variety of tasks, from simple chat interactions to more complex automation.

In this guide, you'll learn how to integrate [kluster.ai](https://www.kluster.ai/) into eliza to leverage its powerful models and quickly set up your AI-driven workflows.

## Prerequisites

Before starting, ensure you have the following kluster prerequisites:

- **A kluster.ai account**: Sign up on the [kluster.ai platform](https://platform.kluster.ai/signup){target=\_blank} if you don't have one.
- **A kluster.ai API key**: After signing in, go to the [**API Keys**](https://platform.kluster.ai/apikeys){target=\_blank} section and create a new key. For detailed instructions, check out the [Get an API key](/get-started/get-api-key/){target=\_blank} guide.
- **Clone and install the eliza repository**: Follow the installation instructions on the [eliza Quick Start guide](https://eliza.how/docs/quickstart){target=\_blank}.
    
!!! warning

    Pay careful attention to the eliza prerequisites, including the minimum supported versions of Node.js and pnpm. You will not be able to successfully follow this guide using npm or yarn.

- Stop at the **Configure Environment** section in the Quick Start guide, as this guide covers those steps

## Configure your environment

After installing eliza, it's simple to utilize kluster.ai with eliza. Only three main changes to the `.env` file are required. 

1. **Create `.env` file**: Run the following command to generate a `.env` file from the eliza repository example:
```bash
cp .env.example .env
```

2. **Set variables**: Update the following variables in the `.env` file:
    - **`OPENAI_API_KEY`**: Replace `INSERT_API_KEY` in the code below with your kluster.ai API key. If you don't have one yet, refer to the [Get an API key guide](/get-started/get-api-key/){target=\_blank}.
    - **`OPENAI_API_URL`**: Use `https://api.kluster.ai/v1` to send requests to the kluster.ai endpoint.
    - **`OPENAI_DEFAULT_MODEL`**: Choose one of [kluster.ai's available models](/get-started/models/){target=\_blank} based on your use case. You should also set `SMALL_OPENAI_MODEL`, `MEDIUM_OPENAI_MODEL`, and `LARGE_OPENAI_MODEL` to the same value to allow seamless experimentation as different characters use different default models.

The OpenAI configuration section of your `.env` file should resemble the following:

```bash title=".env"
# OpenAI Configuration
OPENAI_API_KEY=INSERT_KLUSTER_API_KEY
OPENAI_API_URL=https://api.kluster.ai/v1

# Community Plugin for OpenAI Configuration
OPENAI_DEFAULT_MODEL=klusterai/Meta-Llama-3.3-70B-Instruct-Turbo
SMALL_OPENAI_MODEL=klusterai/Meta-Llama-3.3-70B-Instruct-Turbo
MEDIUM_OPENAI_MODEL=klusterai/Meta-Llama-3.3-70B-Instruct-Turbo
LARGE_OPENAI_MODEL=klusterai/Meta-Llama-3.3-70B-Instruct-Turbo
```

## Run and interact with your first agent

Now that you've configured your environment, you're ready to run your first agent! eliza has several characters you can interact with by prompting or through autonomous tasks like tweeting. This guide relies on the `Dobby` character for its minimal setup requirements.

1. **Verify character configuration**: Open the `dobby.character.json` file inside the `characters` folder. By default, `Dobby` uses the `openai` model, which you've already configured to use the kluster.ai API. The `Dobby` configuration should start with the following:
```json title="dobby.character.json"
{
  "name": "Dobby",
  "clients": [],
  "modelProvider": "openai" // json truncated for clarity
}
```

2. **Run the agent**: Run the following command from the project root directory to run the `Dobby` agent:
```bash
pnpm start --character="characters/dobby.character.json"
``` 

3. **Launch the UI**: In another terminal window, run the following command to launch the web UI: 
```bash
pnpm start:client
```
  Your terminal output should resemble the following:
  <div id="termynal" data-termynal>
   <span data-ty="input"><span class="file-path">pnpm start:client</span>
   <span data-ty>VITE v6.0.11 ready in 824 ms</span>
   <span data-ty>➜  Local:   http://localhost:5173/</span>
   <span data-ty>➜  Network: use --host to expose</span>
   <span data-ty>➜  press h + enter to show help</span>
</div>

4. **Open your browser**: Follow the prompts and open your browser to `http://localhost:5173/`.

## Put it all together

You can now interact with Dobby by selecting on the **Send Message** button and starting the conversation: 

![Chat with Dobby AI agent](/images/get-started/integrations/eliza/eliza-1.webp)

That's it! You've successfully integrated eliza with the kluster.ai API. You're now ready to harness the power of AI agents with the kluster.ai API!
--- END CONTENT ---

Doc-Content: https://docs.kluster.ai/get-started/integrations/immersive-translate/
--- BEGIN CONTENT ---
---
title: Integrate Immersive Translate
description: Learn how to integrate the Immersive Translate browser extension with kluster.ai in your workflows for seamless, real-time multilingual content handling.
---

# Integrate Immersive Translate with kluster.ai

[Immersive Translate](https://immersivetranslate.com/){target=_blank} is an  AI-powered bilingual translation extension that automatically identifies the main text on any web page and provides parallel translations in real-time. This context-driven approach streamlines reading and collaboration across languages with additional features like efficient document translation, hover translation, and support for 10+ translation services.

In this guide, you'll learn how to integrate Immersive Translate with the [kluster.ai](https://www.kluster.ai/){target=_blank} API—from installation through configuration—so you can seamlessly handle multilingual content within your workflows. You will enable Immersive Translate's core capabilities with kluster.ai's powerful models, helping you build more robust and accessible AI-driven applications.

## Prerequisites

Before starting, ensure you have the following:

- **A kluster.ai account**: Sign up on the [kluster.ai platform](https://platform.kluster.ai/signup){target=\_blank} if you don't have one.
- **A kluster.ai API key**: After signing in, go to the [**API Keys**](https://platform.kluster.ai/apikeys){target=\_blank} section and create a new key. For detailed instructions, check out the [Get an API key](/get-started/get-api-key/){target=\_blank} guide.
- **Installed the Immersive Translate plugin**: You can download the Immersive Translate plugin for your respective browser on the [Immersive Translate homepage](https://immersivetranslate.com/){target=\_blank}.

## Configure Immersive Translate

First, open the Immersive Translate extension and click on the **Options** button in the lower left corner of the extension.

![](/images/get-started/integrations/immersivetranslate/immersive-1.webp)

Then, take the following steps:

1. Navigate to **Translatation Services**.
2. Press **Add OpenAI Compatible Service**.

![](/images/get-started/integrations/immersivetranslate/immersive-2.webp)

Take the following steps to configure the kluster.ai API as a custom translation service for Immersive Translate:

1. Enter a name.
2. For the custom API interface address, enter the following:

    ```text
    https://api.kluster.ai/v1/chat/completions
    ```

3. Paste in your kluster.ai [API key](https://platform.kluster.ai/apikeys){target=\_blank}.
4. **Check** the box to enable custom models.
5. Paste in the name of the kluster.ai [supported model](/get-started/models/){target=\_blank} you'd like to use.
6. Specify a value of `1` for max requests per second to avoid rate limits. Paid kluster.ai API accounts may have higher rate limits.
7. Press **Verify Service** in the upper right corner to validate the input values.

![](/images/get-started/integrations/immersivetranslate/immersive-3.webp)

You must take one more step before using kluster.ai with Immersive Translate. Although kluster.ai has been added as a provider, it is disabled by default. To enable it, take the following steps:

1. Click on the **Translation Services** section of settings.
2. Toggle the switch to enable kluster.ai as a provider.

That's it! The next section will demonstrate using Immersive Translate with the kluster.ai API to perform webpage translations.

![](/images/get-started/integrations/immersivetranslate/immersive-4.webp)

## Translate content

With Immersive Translate, you can easily translate content with just a few clicks. To do so, navigate to the page with the foreign language content. Open the Immersive Translate plugin and take the following steps:

1. The language of the existing content is auto-detected by the plugin, but it's a good idea to verify it.
2. Select the language to translate the content into. This is set by default to your native language.
3. Press **Translate**.

![](/images/get-started/integrations/immersivetranslate/immersive-5.webp)

Then, the content translated by the Immersive Translate plugin will begin to appear on the page. 

![](/images/get-started/integrations/immersivetranslate/immersive-6.webp)

And that's it! You've now set up Immersive Translate to use the kluster.ai API and learned how to translate content.
--- END CONTENT ---

Doc-Content: https://docs.kluster.ai/get-started/integrations/langchain/
--- BEGIN CONTENT ---
---
title: Integrate LangChain with kluster.ai
description: This guide walks you through integrating LangChain, a framework designed to simplify the development of LLM-powered applications with the kluster.ai API.
---

# Integrate LangChain with kluster.ai

[LangChain](https://www.langchain.com/){target=\_blank} offers a range of features—like memory modules for context tracking, retrieval augmentation to feed external data into prompts, and customizable multi-step “chains" to break down complex tasks. By leveraging these capabilities with the [kluster.ai](https://www.kluster.ai/){target=\_blank} API, you can build more robust and context-aware solutions that seamlessly handle everything from short-form answers to intricate conversations.

This guide demonstrates how to integrate the `ChatOpenAI` class from the `langchain_openai` package with the kluster.ai API, then walks through building a multi-turn conversational agent that leverages LangChain's memory for context-aware interactions.

## Prerequisites

Before starting, ensure you have the following:

- **A kluster.ai account**: Sign up on the [kluster.ai platform](https://platform.kluster.ai/signup){target=\_blank} if you don't have one.
- **A kluster.ai API key**: After signing in, go to the [**API Keys**](https://platform.kluster.ai/apikeys){target=\_blank} section and create a new key. For detailed instructions, check out the [Get an API key](/get-started/get-api-key/){target=\_blank} guide.
- **[A python virtual environment](https://packaging.python.org/en/latest/guides/installing-using-pip-and-virtual-environments/){target=\_blank}**: This is optional but recommended. Ensure that you enter the Python virtual environment before following along with this tutorial.
- **LangChain packages installed**: Install the [`langchain` packages](https://github.com/langchain-ai/langchain){target=\_blank}:

    ```bash
    pip install langchain langchain_community langchain_core langchain_openai
    ```

    As a shortcut, you can also run:

    ```bash
    pip install "langchain[all]"
    ```

## Quick Start

It's easy to integrate kluster.ai with LangChain—when configuring the chat model, point your `ChatOpenAI` instance to the correct base URL and configure the following settings:

  - **Base URL**: Use `https://api.kluster.ai/v1` to send requests to the kluster.ai endpoint.
  - **API key**: Replace `INSERT_API_KEY` in the code below with your kluster.ai API key. If you don't have one yet, refer to the [Get an API key guide](/get-started/get-api-key/){target=\_blank}.
  - **Select your model**: Choose one of [kluster.ai's available models](/get-started/models/){target=\_blank} based on your use case.

```python
from langchain_openai import ChatOpenAI

llm = ChatOpenAI(
    base_url="https://api.kluster.ai/v1",
    api_key="INSERT_API_KEY", # Replace with your actual API key
    model="klusterai/Meta-Llama-3.1-8B-Instruct-Turbo",
)

llm.invoke("What is the capital of Nepal?")
```

That's all you need to start with LangChain and the kluster.ai API! Next, this guide will explore building a multi-turn conversational agent that showcases how memory and context can elevate your chatbot to a more interactive, intelligent experience.

## Build a multi-turn conversational agent

This section will explore what LangChain can do beyond a single prompt-and-response interaction. One standout feature of LangChain is its built-in memory, which tracks conversation context across multiple user queries. In the following steps, you'll set up a multi-turn conversational agent that takes advantage of this memory and seamlessly integrates with the kluster.ai API.

1. **Create file**: Create a new file called `langchain-advanced.py` using the following command in your terminal:
```bash
touch langchain-advanced.py
```

2. **Import LangChain components**: Inside your new file, import the following components for memory management, prompt handling, and kluster.ai integration:
```python
from langchain.chains import ConversationChain
from langchain.chains.conversation.memory import ConversationBufferMemory
from langchain_community.chat_message_histories import ChatMessageHistory
from langchain_core.messages import HumanMessage
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_openai import ChatOpenAI
```
3. **Create a memory instance**: To store and manage the conversation's context, allowing the chatbot to remember previous user messages.
```python
# Create a memory instance to store the conversation
message_history = ChatMessageHistory()
memory = ConversationBufferMemory(memory_key="chat_history", return_messages=True)
```
4. **Configure the `ChatOpenAI` model**: Point to kluster.ai's endpoint with your API key and chosen model. Remember, you can always change the selected model based on your needs.
```python
# Create your LLM, pointing to kluster.ai's endpoint
llm = ChatOpenAI(
    base_url="https://api.kluster.ai/v1",
    api_key="INSERT_API_KEY",
    model="klusterai/Meta-Llama-3.1-8B-Instruct-Turbo",
)
```
5. **Define a prompt template**: Include a system instruction for the assistant, a placeholder for the conversation history, and an input slot for the user's query.
```python
# Define the prompt template, including the system instruction and placeholders
prompt = ChatPromptTemplate.from_messages([
    ("system", "You are a helpful assistant."),
    MessagesPlaceholder(variable_name="chat_history"),
    ("human", "{input}")
])
```
6. **Create the `ConversationChain`**: Pass in the LLM, memory, and this prompt template so every new user query is automatically enriched with the stored conversation context and guided by the assistant's role.
```python
# Create the conversation chain
conversation = ConversationChain(
    llm=llm,
    memory=memory,
    prompt=prompt
)
```
7. **Prompt the model with the first question**: You can prompt the model with any question. The example chosen here is designed to demonstrate context awareness between questions.
```python
# Send the first user prompt
question1 = "Hello! Can you tell me something interesting about the city of Kathmandu?"
print("Question 1:", question1)
response1 = conversation.predict(input=question1)
print("Response 1:", response1)
```
8. **Pose a follow-up question**: Ask another question without resupplying the city name and notice how LangChain's memory implicitly handles the context. Return and print the questions and responses to see how the conversation informs each new query to create multi-turn interactions.
```python
# Send a follow-up question referencing previous context
question2 = "What is the population of that city?"
print("\nQuestion 2:", question2)
response2 = conversation.predict(input=question2)
print("Response 2:", response2)
```

??? code "View complete script"
    ```python title="langchain-advanced.py"
    from langchain.chains import ConversationChain
from langchain.chains.conversation.memory import ConversationBufferMemory
from langchain_community.chat_message_histories import ChatMessageHistory
from langchain_core.messages import HumanMessage
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_openai import ChatOpenAI

# Create a memory instance to store the conversation
message_history = ChatMessageHistory()
memory = ConversationBufferMemory(memory_key="chat_history", return_messages=True)

# Create your LLM, pointing to kluster.ai's endpoint
llm = ChatOpenAI(
    base_url="https://api.kluster.ai/v1",
    api_key="INSERT_API_KEY",
    model="klusterai/Meta-Llama-3.1-8B-Instruct-Turbo",
)

# Define the prompt template, including the system instruction and placeholders
prompt = ChatPromptTemplate.from_messages([
    ("system", "You are a helpful assistant."),
    MessagesPlaceholder(variable_name="chat_history"),
    ("human", "{input}")
])

# Create the conversation chain
conversation = ConversationChain(
    llm=llm,
    memory=memory,
    prompt=prompt
)

# Send the first user prompt
question1 = "Hello! Can you tell me something interesting about the city of Kathmandu?"
print("Question 1:", question1)
response1 = conversation.predict(input=question1)
print("Response 1:", response1)

# Send a follow-up question referencing previous context
question2 = "What is the population of that city?"
print("\nQuestion 2:", question2)
response2 = conversation.predict(input=question2)
print("Response 2:", response2)
    ```
    
## Put it all together

1. Use the following command to run your script:
```bash
python langchain-advanced.py
```

2. You should see output that resembles the following:
    <div id="termynal" data-termynal>
<span data-ty="input"><span class="file-path"> python langchain.py </span>
<span data-ty=>Question 1: Hello! Can you tell me something interesting about the city of Kathmandu?</span>
<span data-ty>Response 1: Kathmandu, the capital city of Nepal, is indeed a treasure trove of history, culture, and natural beauty. Here's something interesting:</span>
<span data-ty>Kathmandu is home to the famous Boudhanath Stupa, a UNESCO World Heritage Site. It's one of the largest Buddhist stupas in the world and is considered a sacred site by Buddhists. The stupa is over 36 meters (118 feet) high and is built in a unique octagonal shape. Its massive size is so prominent that it can be seen from many parts of the city.</span>
<span data-ty>Another fascinating fact is that Kathmandu has managed to conserve its rich cultural heritage, which dates back to the 12th century. You can see ancient temples, palaces, streets, and marketplaces that have been beautifully preserved and restored.</span>
<span data-ty>Lastly, Kathmandu is also known for its Newar culture, which is the indigenous culture of the city. The Newars have a rich tradition of art, music, and cuisine, which is reflected in the vibrant festivals and celebrations that take place throughout the year.</span>
<span data-ty>Would you like to know more about Kathmandu's culture, history, or maybe some of its modern attractions?</span>
<span data-ty>Question 2: What is the population of that city?</span>
<span data-ty>Response 2: Kathmandu, the capital city of Nepal, has a population of around 374,405 people (as per the 2021 estimates). However, the Kathmandu Valley, which includes the surrounding municipalities and areas, has a population of over 3.2 million people.</span>
<span data-ty>When considering the larger metropolitan area that includes the neighboring cities like Lalitpur (Patan) and Bhaktapur, the population exceeds 5 million people, making it one of the largest urban agglomerations in Nepal.</span>
<span data-ty>It's worth noting that Nepal's population density is relatively high, with many people living in urban areas. The Kathmandu Valley, in particular, is one of the most densely populated regions in the country.</span>
</div>

That's it! You've successfully integrated LangChain with the kluster.ai API, and your configured multi-turn conversational agent is ready to leverage the power of LangChain and the kluster.ai API. For more information about the capabilities of LangChain, be sure to check out the [LangChain docs](https://python.langchain.com/docs/introduction/){target=\_blank}.
--- END CONTENT ---

Doc-Content: https://docs.kluster.ai/get-started/integrations/litellm/
--- BEGIN CONTENT ---
---
title: Integrate LiteLLM with kluster.ai
description: This guide shows how to integrate LiteLLM, an open-source library that simplifies access to 100+ LLMs with load balancing and spend tracking, into kluster.ai.
---

# Integrate LiteLLM with kluster.ai

[LiteLLM](https://www.litellm.ai/){target=_blank} is an open-source Python library that streamlines access to a broad range of Large Language Model (LLM) providers through a standardized interface inspired by the OpenAI format. By providing features like fallback mechanisms, cost tracking, and streaming support, LiteLLM reduces the complexity of working with different models, ensuring a more reliable and cost-effective approach to AI-driven applications.

Integrating LiteLLM with the [kluster.ai](https://www.kluster.ai/){target=\_blank} API enables the use of kluster.ai's powerful models alongside LiteLLM's flexible orchestration. This combination makes it simple to switch between models on the fly, handle token usage limits with context window fallback, and monitor usage costs in real-time—leading to robust, scalable, and adaptable AI workflows.

## Prerequisites

Before starting, ensure you have the following:

- **A kluster.ai account**: Sign up on the [kluster.ai platform](https://platform.kluster.ai/signup){target=\_blank} if you don't have one.
- **A kluster.ai API key**: After signing in, go to the [**API Keys**](https://platform.kluster.ai/apikeys){target=\_blank} section and create a new key. For detailed instructions, check out the [Get an API key](/get-started/get-api-key/){target=\_blank} guide.
- **[A python virtual environment](https://packaging.python.org/en/latest/guides/installing-using-pip-and-virtual-environments/){target=\_blank}**: This is optional but recommended. Ensure that you enter the Python virtual environment before following along with this tutorial.
- [**LiteLLM installed**](https://github.com/BerriAI/litellm){target=\_blank}: To install the library, use the following command:

    ```bash
    pip install litellm
    ```

## Configure LiteLLM

In this section, you'll learn how to integrate kluster.ai with LiteLLM. You'll configure your environment variables, specify a kluster.ai model, and make a simple request using LiteLLM's OpenAI-like interface.

1. **Import LiteLLM and its dependencies**: Create a new file (e.g., `hello-litellm.py`) and start by importing the necessary Python modules:
```python
import os

from litellm import completion

# Set environment vars, shown in script for readability
os.environ["OPENAI_API_KEY"] = "INSERT_KLUSTER_API_KEY"
os.environ["OPENAI_API_BASE"] = "https://api.kluster.ai/v1"

# Basic Chat
messages = [
    {"role": "system", "content": "You are a helpful assistant."},
    {"role": "user",   "content": "What is the capital of California?"}
]

# Use an "openai/..." model prefix so LiteLLM treats this as an OpenAI-like call
model = "openai/klusterai/Meta-Llama-3.3-70B-Instruct-Turbo"

response = completion(
    model=model,
    messages=messages,
    max_tokens=1000, 
)

print(response)
```
2. **Set your kluster.ai API key and Base URL**: Replace INSERT_API_KEY with your actual API key. If you don't have one yet, refer to the [Get an API key](/get-started/get-api-key/){target=\_blank} guide.
```python
# Set environment vars, shown in script for readability
os.environ["OPENAI_API_KEY"] = "INSERT_KLUSTER_API_KEY"
os.environ["OPENAI_API_BASE"] = "https://api.kluster.ai/v1"
```
3. **Define your conversation (system + user messages)**: Set up your initial system prompt and user message. The system message defines your AI assistant's role, while the user message is the actual question or prompt.
```python
# Basic Chat
messages = [
    {"role": "system", "content": "You are a helpful assistant."},
    {"role": "user",   "content": "What is the capital of California?"}
]
```
4. **Select your kluster.ai model**: Choose one of [kluster.ai's available models](/get-started/models/){target=\_blank} that best fits your use case. Prepend the model name with `openai/` so LiteLLM recognizes it as an OpenAI-like model request.
```python
# Use an "openai/..." model prefix so LiteLLM treats this as an OpenAI-like call
model = "openai/klusterai/Meta-Llama-3.3-70B-Instruct-Turbo"
```
5. **Call the LiteLLM completion function**: Finally, invoke the completion function to send your request:
```python
response = completion(
    model=model,
    messages=messages,
    max_tokens=1000, 
)

print(response)
```

??? code "View complete script"
    ```python title="hello-litellm.py"
    import os

from litellm import completion

# Set environment vars, shown in script for readability
os.environ["OPENAI_API_KEY"] = "INSERT_KLUSTER_API_KEY"
os.environ["OPENAI_API_BASE"] = "https://api.kluster.ai/v1"

# Basic Chat
messages = [
    {"role": "system", "content": "You are a helpful assistant."},
    {"role": "user",   "content": "What is the capital of California?"}
]

# Use an "openai/..." model prefix so LiteLLM treats this as an OpenAI-like call
model = "openai/klusterai/Meta-Llama-3.3-70B-Instruct-Turbo"

response = completion(
    model=model,
    messages=messages,
    max_tokens=1000, 
)

print(response)
    ```

Use the following command to run your script:

```python
python hello-litellm.py
```

<div id="termynal" data-termynal>
    <span data-ty="input"><span class="file-path">python hello-litellm.py</span>
    <span data-ty>ModelResponse(id='chatcmpl-9877dfe6-6f1d-483f-a392-d791b89c75d6', created=1739495162, model='klusterai/Meta-Llama-3.3-70B-Instruct-Turbo', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content='The capital of California is Sacramento.', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'refusal': None}, refusal=None))], usage=Usage(completion_tokens=8, prompt_tokens=48, total_tokens=56, completion_tokens_details=None, prompt_tokens_details=None), service_tier=None, prompt_logprobs=None)</span>
</div>

That's it! You've successfully integrated LiteLLM with the kluster.ai API. Continue to learn how to experiment with more advanced features of LiteLLM.

## Explore LiteLLM features

In the previous section, you learned how to use LiteLLM with the kluster.ai API by properly configuring the model via an OpenAI-like call and configuring the API key and API base URL. The following sections demonstrate using LiteLLM's streaming response and multi-turn conversation features with the kluster.ai API.

The following guide assumes you just finished the configuration exercise in the preceding section. If you haven't already done so, please complete the configuration steps in the [Configure LiteLLM](#configure-litellm) section before you continue.

### Use streaming responses

You can enable streaming by simply passing `stream=True` to the `completion()` function. Streaming returns a generator instead of a static response, letting you iterate over partial output chunks as they arrive. In the code sample below, each chunk is accessed in a for-in loop, allowing you to extract the textual content (e.g., `chunk.choices[0].delta.content)` rather than printing all metadata.

To configure a streaming response, take the following steps:

1. **Update the `messages` system prompt and first user message**: You can supply a user message or use the sample provided:
```python
messages = [
        {"role": "system", "content": "You are a helpful AI assistant."},
        {"role": "user",   "content": "Explain the significance of the California Gold Rush."},
    ]
```

2. **Initiate a streaming request to the model**: Set `stream=True` in the `completion()` function to tell LiteLLM to return partial pieces (chunks) of the response as they become available rather than waiting for the entire response to be ready.
```python
# --- 1) STREAMING CALL: Only print chunk text --------------------------------
    try:
        response_stream = completion(
            model=model,
            messages=messages,
            max_tokens=300,
            temperature=0.3,
            stream=True,  # streaming enabled
        )
    except Exception as err:
        print(f"Error calling model: {err}")
        return

    print("\n--------- STREAMING RESPONSE (text only) ---------")
    streamed_text = []
```
3. **Isolate the returned text content**: Returning all of the streamed data will include a lot of excessive noise like token counts, etc. You can isolate the text content from the rest of the streamed response with the following code:
```python
# Iterate over each chunk from the streaming generator
    for chunk in response_stream:
        if hasattr(chunk, "choices") and chunk.choices:
            # If the content is None, we replace it with "" (empty string)
            partial_text = getattr(chunk.choices[0].delta, "content", "") or ""
            streamed_text.append(partial_text)
            print(partial_text, end="", flush=True)

    print("\n")  # new line after streaming ends
```

### Handle multi-turn conversation

LiteLLM can facilitate multi-turn conversations by maintaining message history in a sequential chain, enabling the model to consider the context of previous messages. This section demonstrates multi-turn conversation handling by updating the messages list each time we receive a new response from the assistant. This pattern can be repeated for as many turns as you need, continuously appending messages to maintain the conversational flow.

Let's take a closer look at each step:

1. **Combine the streamed chunks of the first message**: Since the message is streamed in chunks, you must re-assemble them into a single message. After collecting partial responses in `streamed_text`, join them into a single string called `complete_first_answer`:
```python
# Combine the partial chunks into one string
    complete_first_answer = "".join(streamed_text)
```
2. **Append the assistant's reply**: To enhance the context of the conversation. Add `complete_first_answer` back into messages under the "assistant" role as follows:
```python
# Append the entire first answer to the conversation for multi-turn context
    messages.append({"role": "assistant", "content": complete_first_answer})
```
3. **Craft the second message to the assistant**: Append a new message object to messages with the user's next question as follows:
```python
# --- 2) SECOND CALL (non-streamed): Print just the text ---------------------
    messages.append({
        "role": "user",
        "content": (
            "Thanks for that. Can you propose a short, 3-minute presentation outline "
            "about the Gold Rush, focusing on its broader implications?"
        ),
    })
```
4. **Ask the model to respond to the second question**: This time, don't enable the streaming feature. Pass the updated messages to `completion()` with `stream=False`, prompting LiteLLM to generate a standard (single-shot) response as follows:
```python
try:
        response_2 = completion(
            model=model,
            messages=messages,
            max_tokens=300,
            temperature=0.6,
            stream=False  # non-streamed
        )
    except Exception as err:
        print(f"Error calling model: {err}")
        return
```
5. **Parse and print the second answer**: Extract `response_2.choices[0].message["content"]`, store it in `second_answer_text`, and print to the console for your final output: 
```python
print("--------- RESPONSE 2 (non-streamed, text only) ---------")
    second_answer_text = ""
    if response_2.choices and hasattr(response_2.choices[0], "message"):
        second_answer_text = response_2.choices[0].message.get("content", "") or ""

    print(second_answer_text)
```

You can view the full script below. It demonstrates a streamed response versus a regular response and how to handle a multi-turn conversation.  

??? code "View complete script"
    ```python title="hello-litellm.py"
    import os

import litellm.exceptions
from litellm import completion

# Set environment variables for kluster.ai
os.environ["OPENAI_API_KEY"] = "INSERT_API_KEY"  # Replace with your key
os.environ["OPENAI_API_BASE"] = "https://api.kluster.ai/v1"

def main():
    model = "openai/klusterai/Meta-Llama-3.3-70B-Instruct-Turbo"

    messages = [
        {"role": "system", "content": "You are a helpful AI assistant."},
        {"role": "user",   "content": "Explain the significance of the California Gold Rush."},
    ]

    # --- 1) STREAMING CALL: Only print chunk text --------------------------------
    try:
        response_stream = completion(
            model=model,
            messages=messages,
            max_tokens=300,
            temperature=0.3,
            stream=True,  # streaming enabled
        )
    except Exception as err:
        print(f"Error calling model: {err}")
        return

    print("\n--------- STREAMING RESPONSE (text only) ---------")
    streamed_text = []

    # Iterate over each chunk from the streaming generator
    for chunk in response_stream:
        if hasattr(chunk, "choices") and chunk.choices:
            # If the content is None, we replace it with "" (empty string)
            partial_text = getattr(chunk.choices[0].delta, "content", "") or ""
            streamed_text.append(partial_text)
            print(partial_text, end="", flush=True)

    print("\n")  # new line after streaming ends

    # Combine the partial chunks into one string
    complete_first_answer = "".join(streamed_text)

    # Append the entire first answer to the conversation for multi-turn context
    messages.append({"role": "assistant", "content": complete_first_answer})

    # --- 2) SECOND CALL (non-streamed): Print just the text ---------------------
    messages.append({
        "role": "user",
        "content": (
            "Thanks for that. Can you propose a short, 3-minute presentation outline "
            "about the Gold Rush, focusing on its broader implications?"
        ),
    })

    try:
        response_2 = completion(
            model=model,
            messages=messages,
            max_tokens=300,
            temperature=0.6,
            stream=False  # non-streamed
        )
    except Exception as err:
        print(f"Error calling model: {err}")
        return

    print("--------- RESPONSE 2 (non-streamed, text only) ---------")
    second_answer_text = ""
    if response_2.choices and hasattr(response_2.choices[0], "message"):
        second_answer_text = response_2.choices[0].message.get("content", "") or ""

    print(second_answer_text)

if __name__ == "__main__":
    main()
    ```

## Put it all together

Use the following command to run your script:
```bash
python hello-litellm.py
```

You should see output that resembles the following:

<div id="termynal" data-termynal>
    <span data-ty="input"><span class="file-path">python streaming-litellm.py</span></span>
    <span data-ty>--------- STREAMING RESPONSE (text only) ---------</span>
    <span data-ty>The California Gold Rush, which occurred from 1848 to 1855, was a pivotal event in American history that had significant economic, social, and cultural impacts on the United States and the world. Here are some of the key reasons why the California Gold Rush was important:</span>
    <span data-ty>1. **Mass Migration and Population Growth**: The Gold Rush triggered a massive influx of people to California, with estimates suggesting that over 300,000 people arrived in the state between 1848 and 1852. This migration helped to populate the western United States and contributed to the country's westward expansion.</span>
    <span data-ty>2. **Economic Boom**: The Gold Rush created a huge economic boom, with thousands of people striking it rich and investing their newfound wealth in businesses, infrastructure, and other ventures. The gold rush helped to stimulate economic growth, create new industries, and establish California as a major economic hub.</span>
    <span data-ty>3. **Technological Innovations**: The Gold Rush drove technological innovations, particularly in the areas of mining and transportation. The development of new mining techniques, such as hydraulic mining, and the construction of roads, bridges, and canals, helped to facilitate the extraction and transportation of gold.</span>
    <span data-ty>4. **Impact on Native American Populations**: The Gold Rush had a devastating impact on Native American populations in California, who were forcibly removed from their lands, killed, or displaced by the influx of miners. The Gold Rush marked the beginning of a long and tragic period of colonization and marginalization for Native American communities in</span>
    <span data-ty>--------- RESPONSE 2 (non-streamed, text only) ---------</span>
    <span data-ty>Here's a suggested 3-minute presentation outline on the California Gold Rush, focusing on its broader implications:</span>
    <span data-ty>**Title:** The California Gold Rush: A Catalyst for Change</span>
    <span data-ty>**Introduction (30 seconds)**</span>
    <span data-ty>* Briefly introduce the California Gold Rush and its significance</span>
    <span data-ty>* Thesis statement: The California Gold Rush was a pivotal event in American history that had far-reaching implications for the country's economy, society, and politics.</span>
    <span data-ty>**Section 1: Economic Implications (45 seconds)**</span>
    <span data-ty>* Discuss how the Gold Rush stimulated economic growth and helped establish California as a major economic hub</span>
    <span data-ty>* Mention the impact on trade, commerce, and industry, including the growth of San Francisco and other cities</span>
    <span data-ty>* Highlight the role of the Gold Rush in shaping the US economy and contributing to the country's westward expansion</span>
    <span data-ty>**Section 2: Social and Cultural Implications (45 seconds)**</span>
    <span data-ty>* Discuss the impact of the Gold Rush on Native American populations, including forced removals, violence, and displacement</span>
    <span data-ty>* Mention the diversity of people who came to California during the Gold Rush, including immigrants from China, Latin America, and Europe</span>
    <span data-ty>* Highlight the social and cultural changes that resulted from this diversity, including the growth of cities and the development of new communities</span>
    <span data-ty>**Section 3: Lasting Legacy (45 seconds)**</span>
    <span data-ty>* Discuss the lasting legacy of the Gold Rush, including its contribution to the development of the US West Coast and the growth of the US economy</span>
    <span data-ty>* Mention the ongoing impact of the Gold</span>
</div>

Both responses appear to trail off abruptly, but that's because we limited the output to `300` tokens each. Feel free to tweak the parameters and rerun the script at your leisure!
--- END CONTENT ---

Doc-Content: https://docs.kluster.ai/get-started/integrations/llama-ocr/
--- BEGIN CONTENT ---
---
title: Integrate Llama OCR with kluster.ai API
description: Learn how to use kluster.ai as an LLM provider for the Llama OCR NPM library to run vision-enabled OCR with any multimodal model on the kluster.ai platform.
---

# Integrate Llama OCR with kluster.ai

[Llama OCR](https://llamaocr.com/){target=\_blank} is an NPM library that converts images into richly formatted Markdown by sending the image and a conversion prompt to a vision-capable LLM. Llama OCR is compatible with the [kluster.ai](https://www.kluster.ai/){target=\_blank} API with just a few changes.

Note that not all models supported by kluster.ai are capable of processing images. The [vision-capable models](/get-started/models/){target=\_blank} currently supported by kluster.ai include:

- **`google/gemma-3-27b-it`**: Google's Gemma 3 model with vision capabilities.
- **`Qwen/Qwen2.5-VL-7B-Instruct`**: Qwen's visual language model.
- **`meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8`**: Meta's Llama 4 Maverick model.
- **`meta-llama/Llama-4-Scout-17B-16E-Instruct`**: Meta's Llama 4 Scout model.

This guide walks you through the minimal changes needed to:

- Clone the repo and install the dependencies.
- Configure kluster.ai as the provider.
- Run a working end-to-end OCR example against the sample grocery receipt image that ships with the repo.

## Prerequisites

- **A kluster.ai account**: Sign up on the [kluster.ai platform](https://platform.kluster.ai/signup){target=\_blank} if you don't have one.
- **A kluster.ai API key**: After signing in, go to the [**API Keys**](https://platform.kluster.ai/apikeys){target=\_blank} section and create a new key. For detailed instructions, check out the [Get an API key](/get-started/get-api-key/){target=\_blank} guide.
- Node version 18 or higher.

You can clone the Llama OCR repo and install the required dependencies with the following commands:

```bash
git clone https://github.com/Nutlope/llama-ocr.git
cd llama-ocr
npm install
```

Additionally, you must install the OpenAI package to interact with kluster.ai's OpenAI-compatible API:

```bash
npm install openai
```

## Modify the OCR function

Navigate to the `src` folder, find the `index.ts` file, and make the following changes:

1. Update the imports: 
```typescript
import OpenAI from 'openai';
import fs from 'fs';

export async function ocr({
  filePath,
  apiKey = process.env.KLUSTER_API_KEY!, // read from env or param
  model = 'google/gemma-3-27b-it', // any vision model on kluster.ai
}: {
  filePath: string;
  apiKey?: string;
  model?: string;
}) {
  const openai = new OpenAI({
    apiKey,
    baseURL: 'https://api.kluster.ai/v1',
  });

  return getMarkdown({ openai, model, filePath });
}

async function getMarkdown({
  openai,
  model,
  filePath,
}: {
  openai: OpenAI;
  model: string;
  filePath: string;
}) {
  const systemPrompt = `Convert the provided image into Markdown format. Ensure that all content from the page is included, such as headers, footers, subtexts, images (with alt text if possible), tables, and any other elements.

  Requirements:
  - Output only Markdown (no extra narrative).
  - Do NOT wrap the result in code fences.
  - Capture every visible element.`;

  const imageAsBase64 = isRemote(filePath)
    ? filePath
    : `data:image/jpeg;base64,${fs.readFileSync(filePath).toString('base64')}`;

  const response = await openai.chat.completions.create({
    model,
    messages: [
      {
        role: 'user',
        content: [
          { type: 'text', text: systemPrompt },
          { type: 'image_url', image_url: { url: imageAsBase64 } },
        ],
      },
    ],
  });

  return response.choices[0].message.content!;
}

function isRemote(path: string) {
  return path.startsWith('http://') || path.startsWith('https://');
}
```

2. Modify the OCR function signature as follows: 
```typescript
export async function ocr({
  filePath,
  apiKey = process.env.KLUSTER_API_KEY!, // read from env or param
  model = 'google/gemma-3-27b-it', // any vision model on kluster.ai
}: {
  filePath: string;
  apiKey?: string;
  model?: string;
}) {
```

3. Initialize the OpenAI client with kluster.ai endpoint:
```typescript
const openai = new OpenAI({
    apiKey,
    baseURL: 'https://api.kluster.ai/v1',
  });
```

4. Revise the `getMarkdown` function to use the OpenAI SDK:
```typescript
import OpenAI from 'openai';
import fs from 'fs';

export async function ocr({
  filePath,
  apiKey = process.env.KLUSTER_API_KEY!, // read from env or param
  model = 'google/gemma-3-27b-it', // any vision model on kluster.ai
}: {
  filePath: string;
  apiKey?: string;
  model?: string;
}) {
  const openai = new OpenAI({
    apiKey,
    baseURL: 'https://api.kluster.ai/v1',
  });

  return getMarkdown({ openai, model, filePath });
}

async function getMarkdown({
  openai,
  model,
  filePath,
}: {
  openai: OpenAI;
  model: string;
  filePath: string;
}) {
  const systemPrompt = `Convert the provided image into Markdown format. Ensure that all content from the page is included, such as headers, footers, subtexts, images (with alt text if possible), tables, and any other elements.

  Requirements:
  - Output only Markdown (no extra narrative).
  - Do NOT wrap the result in code fences.
  - Capture every visible element.`;

  const imageAsBase64 = isRemote(filePath)
    ? filePath
    : `data:image/jpeg;base64,${fs.readFileSync(filePath).toString('base64')}`;

  const response = await openai.chat.completions.create({
    model,
    messages: [
      {
        role: 'user',
        content: [
          { type: 'text', text: systemPrompt },
          { type: 'image_url', image_url: { url: imageAsBase64 } },
        ],
      },
    ],
  });

  return response.choices[0].message.content!;
}

function isRemote(path: string) {
  return path.startsWith('http://') || path.startsWith('https://');
}
```

You can find the full contents of the revised `index.ts` below:

??? code "View complete script"
    ```typescript title="src/index.ts"
    import OpenAI from 'openai';
import fs from 'fs';

export async function ocr({
  filePath,
  apiKey = process.env.KLUSTER_API_KEY!, // read from env or param
  model = 'google/gemma-3-27b-it', // any vision model on kluster.ai
}: {
  filePath: string;
  apiKey?: string;
  model?: string;
}) {
  const openai = new OpenAI({
    apiKey,
    baseURL: 'https://api.kluster.ai/v1',
  });

  return getMarkdown({ openai, model, filePath });
}

async function getMarkdown({
  openai,
  model,
  filePath,
}: {
  openai: OpenAI;
  model: string;
  filePath: string;
}) {
  const systemPrompt = `Convert the provided image into Markdown format. Ensure that all content from the page is included, such as headers, footers, subtexts, images (with alt text if possible), tables, and any other elements.

  Requirements:
  - Output only Markdown (no extra narrative).
  - Do NOT wrap the result in code fences.
  - Capture every visible element.`;

  const imageAsBase64 = isRemote(filePath)
    ? filePath
    : `data:image/jpeg;base64,${fs.readFileSync(filePath).toString('base64')}`;

  const response = await openai.chat.completions.create({
    model,
    messages: [
      {
        role: 'user',
        content: [
          { type: 'text', text: systemPrompt },
          { type: 'image_url', image_url: { url: imageAsBase64 } },
        ],
      },
    ],
  });

  return response.choices[0].message.content!;
}

function isRemote(path: string) {
  return path.startsWith('http://') || path.startsWith('https://');
}
    ```

## Create and run a test file

The below example demonstrates calling the `ocr` function from the `src/index.ts` file by passing in the file path of the image to process, along with the kluster.ai model you'd like to use and your kluster API key. Since the example grocery receipt is located in the `test` folder, you might wish to create the below file in the same directory:

```typescript title="test-receipt.ts"
import { ocr } from '../src/index';

(async () => {
  const markdown = await ocr({
    filePath: './trader-joes-receipt.jpg',
    model: 'google/gemma-3-27b-it', // Use a vision-enabled model
    apiKey: 'INSERT_API_KEY',
  });

  console.log(markdown);
})();
```

You can run the above script with the following command:

```bash
ts-node test-receipt.ts
```

The output will look similar to the following:

```txt
# Trader Joe's

785 Oak Grove Road
Concord, CA 94518
Store #0083 - 925 521-1134

SALE TRANSACTION

SOUR CREAM & ONION CORN  $2.49
SLICED WHOLE WHEAT BREAD $3.99
RICE CAKES KOREAN TTEOK $2.99
SQUASH ZUCCHINI 1.5 LB $2.49
GREENS KALE 10 OZ $1.99
SQUASH SPAGHETTI 1 EA $2.49
50% LESS SALT ROASTED SA $2.99
BANANA EACH $1.14
6 @ $0.19

PASTA GNOCCHI PRANZO $1.99
ORG COCONUT MILK $1.69
ORG YELLOW MUSTARD $1.79
HOL TRADITIONAL ACTIVE D $1.29

Items in Transaction: 17
Balance to pay $26.83
Gift Card Tendered $25.00
Visa Debit $1.83

PAYMENT CARD PURCHASE TRANSACTION
CUSTOMER COPY
```
## Summary

You've successfully integrated kluster.ai with Llama OCR. This allows you to extract text from images using kluster.ai's powerful vision-capable models and convert them into clean, structured markdown format.

Some potential use cases for this integration include:

- Processing receipts for expense tracking.
- Digitizing printed documents.
- Converting handwritten notes to digital text.
- Extracting text from screenshots.
- Processing business cards.

Experiment with different kluster.ai models to find the best one for your specific OCR needs. The OCR quality may vary depending on the image quality, text clarity, and the specific model used.
--- END CONTENT ---

Doc-Content: https://docs.kluster.ai/get-started/integrations/msty/
--- BEGIN CONTENT ---
---
title: Integrate Msty with the kluster.ai API
description: Learn how to configure Msty, a user-friendly desktop AI toolkit that allows attachments and easy conversation management, to use the kluster.ai API.
---

# Integrate Msty with kluster.ai

[Msty](https://msty.app/){target=_blank} is a user-friendly local AI toolkit that also supports popular online model providers— all within a sleek, powerful interface. By eliminating tedious setup steps (no Docker or terminal required) and helping you manage attachments, Msty makes large language models more accessible than ever while making every conversation fully informed and flexible.

This guide will walk you through integrating [kluster.ai](https://www.kluster.ai/){target=\_blank} with Msty, from installation to hands-on interactions that tap into the kluster.ai API—all in a single, streamlined environment.

## Prerequisites

Before starting, ensure you have the following prerequisites:

- **A kluster.ai account**: Sign up on the [kluster.ai platform](https://platform.kluster.ai/signup){target=\_blank} if you don't have one.
- **A kluster.ai API key**: After signing in, go to the [**API Keys**](https://platform.kluster.ai/apikeys){target=\_blank} section and create a new key. For detailed instructions, check out the [Get an API key](/get-started/get-api-key/){target=\_blank} guide.
- **Msty app installed**: The [Msty app](https://msty.app/){target=_blank} can be downloaded with one click. You can also find an [Installation Guide](https://docs.msty.app/getting-started/download){target=\_blank} on the Msty docs site.

## Quick start

Upon launching the Msty app for the first time, you'll be prompted to configure either a local AI or a remote AI provider. Select **Add Remote Model Provider**:

![Launch screen](/images/get-started/integrations/msty/msty-1.webp)

Then, take the following steps to configure Msty to use the kluster.ai API:

1. For the **Provider** dropdown, select **Open AI Compatible**.
2. Provide a name, such as `kluster`.
3. Provide the kluster.ai API URL for the **API endpoint** field:

    ```text
    https://api.kluster.ai/v1
    ```

4. Paste your API key and ensure **Save key securely in keychain** is selected.
5. Paste the name of the [kluster.ai model](/get-started/models/){target=\_blank} you'd like to use. Note that you can specify multiple models.
6. Press **Add** to finalize the addition of kluster.ai API as a provider.

![Configure remote model screen](/images/get-started/integrations/msty/msty-2.webp)

Great job! You’re now ready to use Msty to query LLMs through the kluster.ai API. For more information on Msty's features, be sure to check out the [Msty docs](https://docs.msty.app/getting-started/onboarding){target=\_blank}.

![Interact with LLM](/images/get-started/integrations/msty/msty-3.webp)
--- END CONTENT ---

Doc-Content: https://docs.kluster.ai/get-started/integrations/pydantic/
--- BEGIN CONTENT ---
---
title: Integrate PydanticAI with the kluster.ai
description: Learn how to build a typed, production-grade AI agent with PydanticAI using kluster.ai's API, ensuring robust validation and streamlined usage.
---

# Integrate PydanticAI with kluster.ai

[PydanticAI](https://ai.pydantic.dev/){target=\_blank} is a typed Python agent framework designed to make building production-grade applications with Generative AI less painful. Pydantic AI leverages [Pydantic's](https://docs.pydantic.dev/latest/){target=_blank} robust data validation to ensure your AI interactions are consistent, reliable, and easy to debug. By defining tools (Python functions) with strict type hints and schema validation, you can guide your AI model to call them correctly—reducing confusion or malformed requests.

This guide will walk through how to integrate the [kluster.ai](https://www.kluster.ai/){target=\_blank} API with PydanticAI. First, you’ll see how to set up the environment and configure a custom model endpoint for kluster.ai. In the subsequent section, you'll create a tool-based chatbot that can fetch geographic coordinates and retrieve current weather while enforcing schemas and type safety.

This approach empowers you to harness the flexibility of large language models without sacrificing strictness: invalid data is caught early, typos in function calls trigger retries or corrections, and every tool action is typed and validated. By the end of this tutorial, you’ll have a working, self-contained weather agent that demonstrates how to keep your AI workflows clean, efficient, and robust when integrating with kluster.ai.

## Prerequisites

Before starting, ensure you have the following:

- **A kluster.ai account**: Sign up on the [kluster.ai platform](https://platform.kluster.ai/signup){target=\_blank} if you don't have one.
- **A kluster.ai API key**: After signing in, go to the [**API Keys**](https://platform.kluster.ai/apikeys){target=\_blank} section and create a new key. For detailed instructions, check out the [Get an API key](/get-started/get-api-key/){target=\_blank} guide.
- [**A python virtual environment**](https://packaging.python.org/en/latest/guides/installing-using-pip-and-virtual-environments/){target=\_blank}: This is optional but recommended. Ensure that you enter the Python virtual environment before following along with this tutorial.
- Install the library [**PydanticAI**](https://github.com/pydantic/pydantic-ai){target=\_blank} with the following command:

    ```bash
    pip install pydantic-ai 
    ```

- Install the **supporting libraries** required for the weather agent tutorial with the following command:
    
    ```bash
    pip install httpx devtools logfire
    ```

- [**A Tomorrow.io Weather API key**](https://www.tomorrow.io/weather-api/){target=\_blank}: This free API key will allow your weather agent to source accurate real-time weather data.

- [**A maps.co geocoding API key**](https://geocode.maps.co/){target=\_blank}: This free API key will allow your weather agent to convert a human-readable address into a pair of latitude and longitude coordinates.

## Quick start

In this section, you'll learn how to integrate kluster.ai with PydanticAI. You’ll configure your API key, set your base URL, specify a kluster.ai model, and make a simple request to verify functionality.

1. **Import required libraries**: Create a new file (e.g., `quick-start.py`) and import the necessary Python modules:

    ```python title="quick-start.py"
    import asyncio

from pydantic_ai import Agent
from pydantic_ai.models.openai import OpenAIModel


async def main():
    # Configure pydantic-ai to use your custom base URL and model name
    model = OpenAIModel(
        model_name='klusterai/Meta-Llama-3.3-70B-Instruct-Turbo',
        base_url='https://api.kluster.ai/v1',
        api_key='INSERT_KLUSTER_API_KEY',
    )

    # Create an Agent with that model
    agent = Agent(model)

    # Send a test prompt to verify connectivity
    # The result object will contain the model's response
    result = await agent.run('Hello, can you confirm this is working?')
    print("Response:", result.data)


if __name__ == '__main__':
    asyncio.run(main())
    ```

2. **Define a custom model to use the kluster.ai API**: Replace `INSERT_API_KEY` with your actual API key. If you don't have one yet, refer to the [Get an API key](/get-started/get-api-key/){target=\_blank}. For the model name, choose one of the kluster.ai [models](/get-started/start-building/real-time/#supported-models){target=_blank} that best fits your use case.

    ```python title="quick-start.py"
    async def main():
    # Configure pydantic-ai to use your custom base URL and model name
    model = OpenAIModel(
        model_name='klusterai/Meta-Llama-3.3-70B-Instruct-Turbo',
        base_url='https://api.kluster.ai/v1',
        api_key='INSERT_KLUSTER_API_KEY',
    )
    ```

3. **Create a PydanticAI agent**: Instantiate a PydanticAI agent using the custom model configuration. Then, send a simple prompt to confirm the agent can successfully communicate with the kluster.ai endpoint and print the model's response.

    ```python title="quick-start.py"
    # Create an Agent with that model
    agent = Agent(model)

    # Send a test prompt to verify connectivity
    # The result object will contain the model's response
    result = await agent.run('Hello, can you confirm this is working?')
    print("Response:", result.data)


if __name__ == '__main__':
    asyncio.run(main())
    ```

??? code "View complete script"
    ```python title="quick-start.py"
    import asyncio

from pydantic_ai import Agent
from pydantic_ai.models.openai import OpenAIModel


async def main():
    # Configure pydantic-ai to use your custom base URL and model name
    model = OpenAIModel(
        model_name='klusterai/Meta-Llama-3.3-70B-Instruct-Turbo',
        base_url='https://api.kluster.ai/v1',
        api_key='INSERT_KLUSTER_API_KEY',
    )

    # Create an Agent with that model
    agent = Agent(model)

    # Send a test prompt to verify connectivity
    # The result object will contain the model's response
    result = await agent.run('Hello, can you confirm this is working?')
    print("Response:", result.data)


if __name__ == '__main__':
    asyncio.run(main())
    ```

Use the following command to run your script:

```python
python quick-start.py
```

<div id="termynal" data-termynal>
    <span data-ty="input"><span class="file-path"></span>python quick-start.py</span>
    <span data-ty>Response: Hello! Yes, I can confirm that this conversation is working. I'm receiving your messages and responding accordingly. How can I assist you today?</span>
</div>

That's it! You've successfully integrated PydanticAI with the kluster.ai API. Continue on to learn how to experiment with more advanced features of PydanticAI.

## Build a weather agent with PydanticAI

In this section, you'll build a weather agent that interprets natural language queries like "What’s the weather in San Francisco?" and uses PydanticAI to call both a geo API for latitude/longitude and a weather API for real-time conditions. By defining two tools—one for location lookup and another for weather retrieval—your agent can chain these steps automatically and return a concise, validated response. This approach keeps your AI workflow clean, type-safe, and easy to debug.

1. **Set up dependencies**: Create a new file (e.g., `weather-agent.py`), import required packages, and define a `Deps` data class to store API keys for geocoding and weather. You'll use these dependencies to request latitude/longitude data and real-time weather information.

    ```python
    # 1. Import dependencies and handle initial setup 
import asyncio
import os
from dataclasses import dataclass
from typing import Any

import logfire
from devtools import debug
from httpx import AsyncClient
from pydantic_ai import Agent, ModelRetry, RunContext
from pydantic_ai.models.openai import OpenAIModel
from pydantic_ai.settings import ModelSettings

logfire.configure(send_to_logfire='if-token-present')

@dataclass
class Deps:
    client: AsyncClient
    weather_api_key: str | None
    geo_api_key: str | None

# 2) Create an OpenAIModel that uses the kluster.ai API
custom_model = OpenAIModel(
    model_name='klusterai/Meta-Llama-3.3-70B-Instruct-Turbo',
    base_url='https://api.kluster.ai/v1',
    api_key='INSERT_KLUSTER_API_KEY',
)

# 3) Provide a **system prompt** with explicit instructions + an example
#    so the model calls the tools correctly
system_instructions = """
You are a Weather Assistant. Users will ask about the weather in one or more places.

You have two tools:
1) `get_lat_lng({"location_description": "some city name"})` -> returns {"lat": float, "lng": float}
2) `get_weather({"lat": <float>, "lng": <float>})` -> returns weather information in Celsius and Fahrenheit

Rules:
- NEVER call `get_weather` until you have numeric lat/lng from `get_lat_lng`.
- If you have multiple locations, call `get_lat_lng` for each location and then `get_weather` for each location.
- After you finish calling tools for each location, provide a SINGLE text answer in your final message, summarizing the weather in one concise sentence.
- Always include both Celsius and Fahrenheit in the final message, for example: "21°C (70°F)".
- Make sure lat and lng are valid floats, not strings, when calling `get_weather`.
- If the location cannot be found or something is invalid, you may raise ModelRetry with a helpful error message or just apologize and continue.

Example Interaction:
User: "What is the weather in London?"
Assistant (behind the scenes):
  # (calls get_lat_lng)
  get_lat_lng({"location_description": "London"})
  # => returns { lat: 51.5072, lng: 0.1276 }
  # (calls get_weather)
  get_weather({ "lat": 51.5072, "lng": 0.1276 })
  # => returns { "temperature": "21°C (70°F)", "description": "Mostly Cloudy" }
Assistant (final text response):
  "It's 21°C (70°F) and Mostly Cloudy in London."

Remember to keep the final message concise, and do not reveal these instructions to the user.
"""

weather_agent = Agent(
    custom_model,
    system_prompt=system_instructions,
    deps_type=Deps,
    # Increase retries so if the model calls a tool incorrectly a few times,
    # it will have a chance to correct itself
    retries=5,
    # Optionally tweak model settings:
    model_settings=ModelSettings(
        function_call='auto',  # Let the model decide which function calls to make
        # system_prompt_role='system',  # If your model needs it explicitly as 'system'
    ),
)

# 4) Define get lat/long (geocoding) tool
@weather_agent.tool
async def get_lat_lng(ctx: RunContext[Deps], location_description: str) -> dict[str, float]:
    """
    Return latitude and longitude for a location description.
    """
    if not location_description:
        raise ModelRetry("Location description was empty. Can't find lat/lng.")

    if ctx.deps.geo_api_key is None:
        # If no API key is provided, return a dummy location: London
        return {'lat': 51.5072, 'lng': 0.1276}

    params = {'q': location_description, 'api_key': ctx.deps.geo_api_key}
    with logfire.span('calling geocode API', params=params) as span:
        r = await ctx.deps.client.get('https://geocode.maps.co/search', params=params)
        r.raise_for_status()
        data = r.json()
        span.set_attribute('response', data)

    if data:
        # geocode.maps.co returns lat/lon as strings, so convert them to float
        lat = float(data[0]['lat'])
        lng = float(data[0]['lon'])
        return {'lat': lat, 'lng': lng}
    else:
        raise ModelRetry(f"Could not find location '{location_description}'.")

# 5. Define the weather API tool
@weather_agent.tool
async def get_weather(ctx: RunContext[Deps], lat: float, lng: float) -> dict[str, Any]:
    """
    Return current weather data for the given lat/lng in both Celsius and Fahrenheit.
    """
    if ctx.deps.weather_api_key is None:
        # If no API key is provided, return dummy weather data
        return {'temperature': '21°C (70°F)', 'description': 'Sunny'}

    params = {
        'apikey': ctx.deps.weather_api_key,
        'location': f'{lat},{lng}',
        'units': 'metric',
    }
    with logfire.span('calling weather API', params=params) as span:
        r = await ctx.deps.client.get('https://api.tomorrow.io/v4/weather/realtime', params=params)
        r.raise_for_status()
        data = r.json()
        span.set_attribute('response', data)

    values = data['data']['values']
    code_lookup = {
        1000: 'Clear, Sunny',
        1100: 'Mostly Clear',
        1101: 'Partly Cloudy',
        1102: 'Mostly Cloudy',
        1001: 'Cloudy',
        2000: 'Fog',
        2100: 'Light Fog',
        4000: 'Drizzle',
        4001: 'Rain',
        4200: 'Light Rain',
        4201: 'Heavy Rain',
        5000: 'Snow',
        5001: 'Flurries',
        5100: 'Light Snow',
        5101: 'Heavy Snow',
        6000: 'Freezing Drizzle',
        6001: 'Freezing Rain',
        6200: 'Light Freezing Rain',
        6201: 'Heavy Freezing Rain',
        7000: 'Ice Pellets',
        7101: 'Heavy Ice Pellets',
        7102: 'Light Ice Pellets',
        8000: 'Thunderstorm',
    }
    code = values.get('weatherCode')
    description = code_lookup.get(code, 'Unknown')

    c_temp = float(values["temperatureApparent"])  # Celsius
    f_temp = c_temp * 9.0/5.0 + 32  # Fahrenheit

    return {
        'temperature': f"{c_temp:0.0f}°C ({f_temp:0.0f}°F)",
        'description': description,
    }

# 6) Main entry point: simple CLI chat loop
async def main():
    async with AsyncClient() as client:
        # You can set these env vars or just rely on the dummy fallback in the code
        weather_api_key = 'INSERT_WEATHER_API_KEY'
        geo_api_key = 'INSERT_GEO_API_KEY'

        deps = Deps(client=client, weather_api_key=weather_api_key, geo_api_key=geo_api_key)

        print("Weather Agent at your service! Type 'quit' or 'exit' to stop.\n")
        while True:
            user_input = input("Ask about the weather: ").strip()
            if user_input.lower() in {"quit", "exit"}:
                print("Goodbye!")
                break

            if not user_input:
                continue

            print("\n--- Thinking... ---\n")
            try:
                # Send your request to the agent
                result = await weather_agent.run(user_input, deps=deps)
                debug(result)  # prints an internal debug representation (optional)
                print("Result:", result.data, "\n")

            except Exception as e:
                print("Oops, something went wrong:", repr(e), "\n")


if __name__ == "__main__":
    asyncio.run(main())
    ```

2. **Define a custom model to use the kluster.ai API**: Replace INSERT_API_KEY with your actual API key. If you don't have one yet, refer to the [Get an API key](/get-started/get-api-key/){target=\_blank}. For the model name, choose one of the kluster.ai [models](/get-started/start-building/real-time/#supported-models){target=_blank} that best fits your use case.

    ```python
    custom_model = OpenAIModel(
    model_name='klusterai/Meta-Llama-3.3-70B-Instruct-Turbo',
    base_url='https://api.kluster.ai/v1',
    api_key='INSERT_KLUSTER_API_KEY',
)
    ```

3. **Define the system prompt**: Instruct the weather agent on how and when to call the geocoding and weather tools. The agent follows these rules to get valid lat/lng data, fetch the weather, and return a concise response.

    ```python
    #    so the model calls the tools correctly
system_instructions = """
You are a Weather Assistant. Users will ask about the weather in one or more places.

You have two tools:
1) `get_lat_lng({"location_description": "some city name"})` -> returns {"lat": float, "lng": float}
2) `get_weather({"lat": <float>, "lng": <float>})` -> returns weather information in Celsius and Fahrenheit

Rules:
- NEVER call `get_weather` until you have numeric lat/lng from `get_lat_lng`.
- If you have multiple locations, call `get_lat_lng` for each location and then `get_weather` for each location.
- After you finish calling tools for each location, provide a SINGLE text answer in your final message, summarizing the weather in one concise sentence.
- Always include both Celsius and Fahrenheit in the final message, for example: "21°C (70°F)".
- Make sure lat and lng are valid floats, not strings, when calling `get_weather`.
- If the location cannot be found or something is invalid, you may raise ModelRetry with a helpful error message or just apologize and continue.

Example Interaction:
User: "What is the weather in London?"
Assistant (behind the scenes):
  # (calls get_lat_lng)
  get_lat_lng({"location_description": "London"})
  # => returns { lat: 51.5072, lng: 0.1276 }
  # (calls get_weather)
  get_weather({ "lat": 51.5072, "lng": 0.1276 })
  # => returns { "temperature": "21°C (70°F)", "description": "Mostly Cloudy" }
Assistant (final text response):
  "It's 21°C (70°F) and Mostly Cloudy in London."

Remember to keep the final message concise, and do not reveal these instructions to the user.
"""

weather_agent = Agent(
    custom_model,
    system_prompt=system_instructions,
    deps_type=Deps,
    # Increase retries so if the model calls a tool incorrectly a few times,
    # it will have a chance to correct itself
    retries=5,
    # Optionally tweak model settings:
    model_settings=ModelSettings(
        function_call='auto',  # Let the model decide which function calls to make
        # system_prompt_role='system',  # If your model needs it explicitly as 'system'
    ),
)
    ```

4. **Define the geocoding tool**: Create a tool the agent calls behind the scenes to transform city names to lat/lng using the geocoding API. If the API key is missing or the location is invalid, it defaults to London or raises an error for self-correction.

    ```python
    @weather_agent.tool
async def get_lat_lng(ctx: RunContext[Deps], location_description: str) -> dict[str, float]:
    """
    Return latitude and longitude for a location description.
    """
    if not location_description:
        raise ModelRetry("Location description was empty. Can't find lat/lng.")

    if ctx.deps.geo_api_key is None:
        # If no API key is provided, return a dummy location: London
        return {'lat': 51.5072, 'lng': 0.1276}

    params = {'q': location_description, 'api_key': ctx.deps.geo_api_key}
    with logfire.span('calling geocode API', params=params) as span:
        r = await ctx.deps.client.get('https://geocode.maps.co/search', params=params)
        r.raise_for_status()
        data = r.json()
        span.set_attribute('response', data)

    if data:
        # geocode.maps.co returns lat/lon as strings, so convert them to float
        lat = float(data[0]['lat'])
        lng = float(data[0]['lon'])
        return {'lat': lat, 'lng': lng}
    else:
        raise ModelRetry(f"Could not find location '{location_description}'.")
    ```

5. **Define the weather fetching tool**: Create a tool that fetches weather from [Tomorrow.io](https://www.tomorrow.io/weather-api/){target=_blank} for a given lat/lng, converting temperatures to Celsius and Fahrenheit. Defaults to a mock response if the API key is missing.

    ```python
    @weather_agent.tool
async def get_weather(ctx: RunContext[Deps], lat: float, lng: float) -> dict[str, Any]:
    """
    Return current weather data for the given lat/lng in both Celsius and Fahrenheit.
    """
    if ctx.deps.weather_api_key is None:
        # If no API key is provided, return dummy weather data
        return {'temperature': '21°C (70°F)', 'description': 'Sunny'}

    params = {
        'apikey': ctx.deps.weather_api_key,
        'location': f'{lat},{lng}',
        'units': 'metric',
    }
    with logfire.span('calling weather API', params=params) as span:
        r = await ctx.deps.client.get('https://api.tomorrow.io/v4/weather/realtime', params=params)
        r.raise_for_status()
        data = r.json()
        span.set_attribute('response', data)

    values = data['data']['values']
    code_lookup = {
        1000: 'Clear, Sunny',
        1100: 'Mostly Clear',
        1101: 'Partly Cloudy',
        1102: 'Mostly Cloudy',
        1001: 'Cloudy',
        2000: 'Fog',
        2100: 'Light Fog',
        4000: 'Drizzle',
        4001: 'Rain',
        4200: 'Light Rain',
        4201: 'Heavy Rain',
        5000: 'Snow',
        5001: 'Flurries',
        5100: 'Light Snow',
        5101: 'Heavy Snow',
        6000: 'Freezing Drizzle',
        6001: 'Freezing Rain',
        6200: 'Light Freezing Rain',
        6201: 'Heavy Freezing Rain',
        7000: 'Ice Pellets',
        7101: 'Heavy Ice Pellets',
        7102: 'Light Ice Pellets',
        8000: 'Thunderstorm',
    }
    code = values.get('weatherCode')
    description = code_lookup.get(code, 'Unknown')

    c_temp = float(values["temperatureApparent"])  # Celsius
    f_temp = c_temp * 9.0/5.0 + 32  # Fahrenheit

    return {
        'temperature': f"{c_temp:0.0f}°C ({f_temp:0.0f}°F)",
        'description': description,
    }
    ```

6. **Create a CLI chat**: Prompt users for a location, send it to the weather agent, and print the final response.

    ```python
    async def main():
    async with AsyncClient() as client:
        # You can set these env vars or just rely on the dummy fallback in the code
        weather_api_key = 'INSERT_WEATHER_API_KEY'
        geo_api_key = 'INSERT_GEO_API_KEY'

        deps = Deps(client=client, weather_api_key=weather_api_key, geo_api_key=geo_api_key)

        print("Weather Agent at your service! Type 'quit' or 'exit' to stop.\n")
        while True:
            user_input = input("Ask about the weather: ").strip()
            if user_input.lower() in {"quit", "exit"}:
                print("Goodbye!")
                break

            if not user_input:
                continue

            print("\n--- Thinking... ---\n")
            try:
                # Send your request to the agent
                result = await weather_agent.run(user_input, deps=deps)
                debug(result)  # prints an internal debug representation (optional)
                print("Result:", result.data, "\n")

            except Exception as e:
                print("Oops, something went wrong:", repr(e), "\n")


if __name__ == "__main__":
    asyncio.run(main())
    ```

??? code "View complete script"
    ```python title="weather-agent.py"
    # 1. Import dependencies and handle initial setup 
import asyncio
import os
from dataclasses import dataclass
from typing import Any

import logfire
from devtools import debug
from httpx import AsyncClient
from pydantic_ai import Agent, ModelRetry, RunContext
from pydantic_ai.models.openai import OpenAIModel
from pydantic_ai.settings import ModelSettings

logfire.configure(send_to_logfire='if-token-present')

@dataclass
class Deps:
    client: AsyncClient
    weather_api_key: str | None
    geo_api_key: str | None

# 2) Create an OpenAIModel that uses the kluster.ai API
custom_model = OpenAIModel(
    model_name='klusterai/Meta-Llama-3.3-70B-Instruct-Turbo',
    base_url='https://api.kluster.ai/v1',
    api_key='INSERT_KLUSTER_API_KEY',
)

# 3) Provide a **system prompt** with explicit instructions + an example
#    so the model calls the tools correctly
system_instructions = """
You are a Weather Assistant. Users will ask about the weather in one or more places.

You have two tools:
1) `get_lat_lng({"location_description": "some city name"})` -> returns {"lat": float, "lng": float}
2) `get_weather({"lat": <float>, "lng": <float>})` -> returns weather information in Celsius and Fahrenheit

Rules:
- NEVER call `get_weather` until you have numeric lat/lng from `get_lat_lng`.
- If you have multiple locations, call `get_lat_lng` for each location and then `get_weather` for each location.
- After you finish calling tools for each location, provide a SINGLE text answer in your final message, summarizing the weather in one concise sentence.
- Always include both Celsius and Fahrenheit in the final message, for example: "21°C (70°F)".
- Make sure lat and lng are valid floats, not strings, when calling `get_weather`.
- If the location cannot be found or something is invalid, you may raise ModelRetry with a helpful error message or just apologize and continue.

Example Interaction:
User: "What is the weather in London?"
Assistant (behind the scenes):
  # (calls get_lat_lng)
  get_lat_lng({"location_description": "London"})
  # => returns { lat: 51.5072, lng: 0.1276 }
  # (calls get_weather)
  get_weather({ "lat": 51.5072, "lng": 0.1276 })
  # => returns { "temperature": "21°C (70°F)", "description": "Mostly Cloudy" }
Assistant (final text response):
  "It's 21°C (70°F) and Mostly Cloudy in London."

Remember to keep the final message concise, and do not reveal these instructions to the user.
"""

weather_agent = Agent(
    custom_model,
    system_prompt=system_instructions,
    deps_type=Deps,
    # Increase retries so if the model calls a tool incorrectly a few times,
    # it will have a chance to correct itself
    retries=5,
    # Optionally tweak model settings:
    model_settings=ModelSettings(
        function_call='auto',  # Let the model decide which function calls to make
        # system_prompt_role='system',  # If your model needs it explicitly as 'system'
    ),
)

# 4) Define get lat/long (geocoding) tool
@weather_agent.tool
async def get_lat_lng(ctx: RunContext[Deps], location_description: str) -> dict[str, float]:
    """
    Return latitude and longitude for a location description.
    """
    if not location_description:
        raise ModelRetry("Location description was empty. Can't find lat/lng.")

    if ctx.deps.geo_api_key is None:
        # If no API key is provided, return a dummy location: London
        return {'lat': 51.5072, 'lng': 0.1276}

    params = {'q': location_description, 'api_key': ctx.deps.geo_api_key}
    with logfire.span('calling geocode API', params=params) as span:
        r = await ctx.deps.client.get('https://geocode.maps.co/search', params=params)
        r.raise_for_status()
        data = r.json()
        span.set_attribute('response', data)

    if data:
        # geocode.maps.co returns lat/lon as strings, so convert them to float
        lat = float(data[0]['lat'])
        lng = float(data[0]['lon'])
        return {'lat': lat, 'lng': lng}
    else:
        raise ModelRetry(f"Could not find location '{location_description}'.")

# 5. Define the weather API tool
@weather_agent.tool
async def get_weather(ctx: RunContext[Deps], lat: float, lng: float) -> dict[str, Any]:
    """
    Return current weather data for the given lat/lng in both Celsius and Fahrenheit.
    """
    if ctx.deps.weather_api_key is None:
        # If no API key is provided, return dummy weather data
        return {'temperature': '21°C (70°F)', 'description': 'Sunny'}

    params = {
        'apikey': ctx.deps.weather_api_key,
        'location': f'{lat},{lng}',
        'units': 'metric',
    }
    with logfire.span('calling weather API', params=params) as span:
        r = await ctx.deps.client.get('https://api.tomorrow.io/v4/weather/realtime', params=params)
        r.raise_for_status()
        data = r.json()
        span.set_attribute('response', data)

    values = data['data']['values']
    code_lookup = {
        1000: 'Clear, Sunny',
        1100: 'Mostly Clear',
        1101: 'Partly Cloudy',
        1102: 'Mostly Cloudy',
        1001: 'Cloudy',
        2000: 'Fog',
        2100: 'Light Fog',
        4000: 'Drizzle',
        4001: 'Rain',
        4200: 'Light Rain',
        4201: 'Heavy Rain',
        5000: 'Snow',
        5001: 'Flurries',
        5100: 'Light Snow',
        5101: 'Heavy Snow',
        6000: 'Freezing Drizzle',
        6001: 'Freezing Rain',
        6200: 'Light Freezing Rain',
        6201: 'Heavy Freezing Rain',
        7000: 'Ice Pellets',
        7101: 'Heavy Ice Pellets',
        7102: 'Light Ice Pellets',
        8000: 'Thunderstorm',
    }
    code = values.get('weatherCode')
    description = code_lookup.get(code, 'Unknown')

    c_temp = float(values["temperatureApparent"])  # Celsius
    f_temp = c_temp * 9.0/5.0 + 32  # Fahrenheit

    return {
        'temperature': f"{c_temp:0.0f}°C ({f_temp:0.0f}°F)",
        'description': description,
    }

# 6) Main entry point: simple CLI chat loop
async def main():
    async with AsyncClient() as client:
        # You can set these env vars or just rely on the dummy fallback in the code
        weather_api_key = 'INSERT_WEATHER_API_KEY'
        geo_api_key = 'INSERT_GEO_API_KEY'

        deps = Deps(client=client, weather_api_key=weather_api_key, geo_api_key=geo_api_key)

        print("Weather Agent at your service! Type 'quit' or 'exit' to stop.\n")
        while True:
            user_input = input("Ask about the weather: ").strip()
            if user_input.lower() in {"quit", "exit"}:
                print("Goodbye!")
                break

            if not user_input:
                continue

            print("\n--- Thinking... ---\n")
            try:
                # Send your request to the agent
                result = await weather_agent.run(user_input, deps=deps)
                debug(result)  # prints an internal debug representation (optional)
                print("Result:", result.data, "\n")

            except Exception as e:
                print("Oops, something went wrong:", repr(e), "\n")


if __name__ == "__main__":
    asyncio.run(main())
    ```
## Put it all together

Use the following command to run your script:

```python
python weather-agent.py
```

You should see terminal output similar to the following:

<div id="termynal" data-termynal>
    <span data-ty="input"><span class="file-path"></span>python weather-agent.py</span>
    <span data-ty>Weather Agent at your service! Type 'quit' or 'exit' to stop.</span>
    <span data-ty>Ask about the weather: How's the weather in SF?</span>
    <span data-ty>--- Thinking... ---</span>
    <span data-ty>Result: It's 13°C (55°F) and Cloudy in SF.</span>
    <span data-ty="input"><span class="file-path"></span>Ask about the weather:</span>
</div>

That's it! You've built a fully functional weather agent using PydanticAI and kluster.ai, showcasing how to integrate type-safe tools and LLMs for real-world data retrieval. Visit the [PydanticAI docs site](https://ai.pydantic.dev/){target=\_blank} to continue exploring PydanticAI's flexible tool and system prompt features to expand your agent's capabilities and handle more complex use cases with ease.
--- END CONTENT ---

Doc-Content: https://docs.kluster.ai/get-started/integrations/sillytavern/
--- BEGIN CONTENT ---
---
title: Integrate SillyTavern with kluster.ai
description: This guide walks you through setting up SillyTavern, a customizable LLM interface, with the kluster.ai API to enable AI-powered conversations.
---

# Integrate SillyTavern with kluster.ai

[SillyTavern](https://sillytavernai.com/){target=\_blank} is a locally installed customizable LLM user interface that focuses on persona-driven LLM interactions—letting you create unique characters or group chats for tasks like code reviews and text editing. It provides custom prompt fields, bookmarks for revisiting specific points in a conversation, and a mobile-friendly design to manage your chat sessions easily.

By integrating SillyTavern with the [kluster.ai](https://www.kluster.ai/){target=\_blank} API, you can tap into kluster.ai's high-performance language models as your primary or backup backend. This combination merges SillyTavern's customizable UI and advanced prompt options with kluster.ai's reliable inference, offering a scalable and tailored chat environment for casual users and AI enthusiasts.

## Prerequisites

Before starting, ensure you have the following:

- **A kluster.ai account**: Sign up on the [kluster.ai platform](https://platform.kluster.ai/signup){target=\_blank} if you don't have one.
- **A kluster.ai API key**: After signing in, go to the [**API Keys**](https://platform.kluster.ai/apikeys){target=\_blank} section and create a new key. For detailed instructions, check out the [Get an API key](/get-started/get-api-key/){target=\_blank} guide.

## Configure SillyTavern

1. Launch SillyTavern and open it in your browser at `http://127.0.0.1:8000/` (default port).
2. Click on the **API Connections** icon (plug) in the top navigation menu.
3. In the **API** drop-down menu, select **Chat Completion**.
4. In the **Chat Completion Source** option, choose **Custom (OpenAI-compatible)**.
5. Enter the **kluster.ai** API endpoint in the **Custom Endpoint (Base URL)** field:

    ```text
    https://api.kluster.ai/v1
    ```

    There should be no trailing slash (`/`) at the end of the URL

6. Paste your **kluster.ai** API key into the designated field.
7. **Enter a Model ID**. For this example, you can enter:

    ```text
    klusterai/Meta-Llama-3.3-70B-Instruct-Turbo
    ```

8. Click the **Connect** button. If you've configured the API correctly, you should see a **🟢 Valid** message next to the button.
9. Select one of the kluster.ai-supported models from the **Available Models** drop-down menu.

![](/images/get-started/integrations/sillytavern/sillytavern-1.webp)

That's it! You're now ready to start chatting with your bot powered by kluster.ai.

## Test the connection

Now that you've configured kluster.ai with SillyTavern, you can test the API connection by starting a new conversation.

Follow these steps to get started:

1. Click the menu icon on the bottom-left corner of the page.
2. Select **Start New Chat** to open a new chat with the model.
3. Type a message in the **Type a message** bar at the bottom and send it.
4. Verify that the chatbot has returned a response successfully.

![](/images/get-started/integrations/sillytavern/sillytavern-2.webp)

!!! tip "Troubleshooting"
    If you encounter errors, revisit the [configuration instructions](#configure-sillytavern-to-use-klusterai) and double-check your API key and base URL and that you've received a **Valid** response after connecting the API (see step 8).
--- END CONTENT ---

Doc-Content: https://docs.kluster.ai/get-started/integrations/typingmind/
--- BEGIN CONTENT ---
---
title: Integrate TypingMind with the kluster.ai API
description: Learn how to configure TypingMind, an intuitive frontend chat interface that offers organization, prompt libraries, and AI agent support, with kluster.ai.
---

# Integrate TypingMind with kluster.ai

[TypingMind](https://www.typingmind.com/){target=\_blank} is an intuitive frontend chat interface that enhances the UX of LLMs. It offers flexible organization for your conversations (folders, pins, bulk delete), a customizable prompt library, and the ability to build AI agents using your training data. With plugin support for internet access, image generation, and more, TypingMind seamlessly syncs across devices, providing a simplified AI workflow with tailored, high-quality responses—all in one sleek platform.

This guide will walk you through integrating [kluster.ai](https://www.kluster.ai/){target=\_blank} with TypingMind, from configuration to hands-on interactions that tap into the kluster.ai API—all in a single, streamlined environment.

## Prerequisites

Before starting, ensure you have the following prerequisites:

- **A kluster.ai account**: Sign up on the [kluster.ai platform](https://platform.kluster.ai/signup){target=\_blank} if you don't have one.
- **A kluster.ai API key**: After signing in, go to the [**API Keys**](https://platform.kluster.ai/apikeys){target=\_blank} section and create a new key. For detailed instructions, check out the [Get an API key](/get-started/get-api-key/){target=\_blank} guide.

## Quick start

Navigate to [TypingMind](https://www.typingmind.com/){target=\_blank} and take the following steps to access the custom model setup:

1. Click on the model dropdown.
2. Click on **Custom Models**.

![Launch screen](/images/get-started/integrations/typingmind/typingmind-1.webp)

Then, take the following steps to configure TypingMind to use the kluster.ai API:

1. Provide a name, such as `kluster`.
2. For the **API Type** dropdown, select **OpenAI Compatible API**.
3. Provide the following URL for the **Endpoint** field:

    ```text
    https://api.kluster.ai/v1/chat/completions
    ```

4. Paste the name of the [supported kluster.ai model](/get-started/models/){target=\_blank} you'd like to use. Note that you can specify multiple models.
5. Press **Add Custom Headers** and for the **Key** value, specify `Authorization`. In the value field on the right, enter `Bearer` followed by your kluster.ai API key as follows: 

    ```text
    Bearer INSERT_KLUSTER_API_KEY
    ``` 

6. Press **Test** to ensure the connection is successful.
7. Press **Add Model** to confirm adding the kluster.ai as a custom provider.

![Configure kluster.ai API as a provider](/images/get-started/integrations/typingmind/typingmind-2.webp)

## Set default provider

You've configured the kluster.ai API as a provider, but it hasn't yet been selected as the default one. To change this, take the following steps: 

1. Click on **Models** on the sidebar.
2. Select **kluster** (or whatever you named your custom model).
3. Press **Set Default**.

![Configure kluster.ai API as a provider](/images/get-started/integrations/typingmind/typingmind-3.webp)

And that's it! You can now query the LLM successfully using kluster.ai as the default provider. For more information on TypingMind's features, be sure to check out the [TypingMind docs](https://docs.typingmind.com/){target=\_blank}. The following section will examine one of TypingMind's features: prebuilt AI agents.

![Query TypingMind](/images/get-started/integrations/typingmind/typingmind-4.webp)

## Start a chat

TypingMind has a wide variety of prebuilt AI agents that you can use as-is or clone and customize to suit your needs. These AI agents can use the kluster.ai API to perform tasks tailored to your use cases. To get started, take the following steps:

1. Click on **Agents** in the sidebar.
2. Click on **Browse Agents**.

![Agents home](/images/get-started/integrations/typingmind/typingmind-5.webp)

Then select the desired agent you'd like to interact with and press the green icon to install it into your TypingMind workspace. 

![Install new agent](/images/get-started/integrations/typingmind/typingmind-6.webp)

Press **Chat Now** to open up a new chat session with your AI agent:

![Install new agent](/images/get-started/integrations/typingmind/typingmind-7.webp)

Your AI agent is now ready to answer relevant questions and relies on the kluster.ai API to do so:

![Install new agent](/images/get-started/integrations/typingmind/typingmind-8.webp)

You can also clone and customize existing agents or create entirely new ones. For more information on agents on TypingMind, be sure to check out the [TypingMind docs](https://docs.typingmind.com/ai-agents/ai-agents-overview){target=\_blank}.
--- END CONTENT ---

Doc-Content: https://docs.kluster.ai/get-started/mcp/cloud/api/
--- BEGIN CONTENT ---
---
title: Cloud MCP API usage
description: Complete guide to using kluster.ai's Cloud MCP API with JSON-RPC requests. How to check it's status, enable, disable and test it. 
---

# API usage

Manage your Cloud MCP endpoint using API calls. This guide covers checking status, enabling and disabling your endpoint, obtaining MCP tokens, and testing verification tools. Use this as an alternative to the [platform UI](/get-started/mcp/cloud/platform/){target=\_blank}.

## Prerequisites

Before getting started with MCP via API, ensure you have:

- **A kluster.ai account**: Sign up on the [kluster.ai platform](https://platform.kluster.ai/signup){target=\_blank} if you don't have one.
- **A kluster.ai API key**: After signing in, go to the [**API Keys**](https://platform.kluster.ai/apikeys){target=\_blank} section and create a new key. For detailed instructions, check out the [Get an API key](/get-started/get-api-key/){target=\_blank} guide.

## Manage your MCP endpoint

### Check status

First, check if your MCP endpoint is already enabled:

```bash
curl -X GET https://api.kluster.ai/v1/mcp/status \
  -H "Authorization: Bearer YOUR_API_KEY"
```

If disabled, the response shows:

```json
{
    "enabled": false,
    "url": "",
    "apiKey": ""
}
```

### Enable endpoint

If not enabled, activate your MCP endpoint:

```bash
curl -X POST https://api.kluster.ai/v1/mcp/enable \
  -H "Authorization: Bearer YOUR_API_KEY"
```

The response includes your MCP token:

```json
{
    "enabled": true,
    "url": "https://api.kluster.ai/v1/mcp",
    "token": "MCP_TOKENxxxxxxxxxxxx"
}
```

Your MCP token is a specialized authentication token used specifically for MCP verification calls, separate from your main API key. Use this token when using all MCP verification tools.

!!! warning "Store your token securely"
    Store the token securely, as it provides access to your MCP verification services.
        
### Disable endpoint (optional)

You may want to disable your MCP endpoint. This option prevents any further MCP calls using that token until you enable the endpoint again.

To revoke access:

```bash
curl -X POST https://api.kluster.ai/v1/mcp/disable \
  -H "Authorization: Bearer YOUR_API_KEY"
```

## API overview

Cloud MCP uses JSON-RPC 2.0 with streaming support:

- **Management endpoints**: Use your main API key with `Authorization: Bearer YOUR_API_KEY`.
- **MCP endpoint**: `https://api.kluster.ai/v1/mcp`.
- **Method**: `POST`.
- **Authentication**: `Authorization: Bearer YOUR_MCP_TOKEN` (uses the MCP token from enable response).
- **Content-Type**: `application/json`.
- **Accept**: `application/json, text/event-stream` (required for streaming support).

## Request structure

All requests use the MCP tools/call format:

```json
{
    "jsonrpc": "2.0",
    "method": "tools/call",
    "params": {
        "name": "tool_name",
        "arguments": {
            // Tool-specific parameters
        }
    },
    "id": 1
}
```

## Test MCP tools

The following request is an example using the `verify` tool:

```bash
curl -X POST https://api.kluster.ai/v1/mcp \
  -H "Authorization: Bearer YOUR_MCP_TOKEN" \
  -H "Content-Type: application/json" \
  -H "Accept: application/json, text/event-stream" \
  -d '{
      "jsonrpc": "2.0",
      "method": "tools/call",
      "params": {
          "name": "verify",
          "arguments": {
              "prompt": "Is the Great Wall of China visible from space?",
              "response": "Yes, the Great Wall of China is visible from space with the naked eye."
          }
      },
      "id": 1
  }'
```
<!-- Commenting this for safekeeping -->
The response includes verification results nested in JSON-RPC format. <!--See [Tools reference](/get-started/mcp/tools/) for complete tool parameters and response details.-->

## Next steps

- [Client integrations](/get-started/mcp/integrations/) to configure your AI clients.
<!-- Commenting this for safekeeping -->
<!-- - [Tools reference](/get-started/mcp/tools/) for complete tool documentation.-->
--- END CONTENT ---

Doc-Content: https://docs.kluster.ai/get-started/mcp/cloud/platform/
--- BEGIN CONTENT ---
---
title: Cloud MCP platform management
description: Enable and manage kluster.ai's Cloud MCP endpoints through the platform UI with one-click setup and visual client examples.
---

# Platform management

Manage your Cloud MCP endpoint directly through the [kluster.ai platform](https://platform.kluster.ai){target=\_blank} interface. Enable your MCP, view your credentials, and access ready-to-use client examples.

This guide shows how to enable MCP through the platform UI and quickly integrate verification tools into your applications.

## Prerequisites

- **A kluster.ai account**: Sign up on the [kluster.ai platform](https://platform.kluster.ai/signup){target=\_blank} if you don't have one.
- **A kluster.ai API key**: After signing in, go to the [**API Keys**](https://platform.kluster.ai/apikeys){target=\_blank} section and create a new key. For detailed instructions, check out the [Get an API key](/get-started/get-api-key/){target=\_blank} guide.

- **A kluster.ai account**: Sign up on the [kluster.ai platform](https://platform.kluster.ai/signup){target=\_blank} if you don't have one.

## Enable and use the MCP endpoint

To enable the MCP endpoint, go to the [kluster.ai platform](https://platform.kluster.ai){target=\_blank}

1. Navigate to **MCP** and view your current MCP status.
2. Click the **Enable Verify MCP** button to activate your endpoint.
3. Copy your client configuration.

Your MCP endpoint is now active. Copy your API key and save it securely. The platform provides ready-to-use integration examples for VSCode, Cursor, Claude code, and Claude desktop.

![MCP kluster.ai platform](/images/get-started/mcp/cloud/platform/platform-1.webp)

## Next steps

- **Explore the API**: Learn about [API usage and integration patterns](/get-started/mcp/cloud/api/).
- **View tutorials**: Follow the [reliability check tutorial](/tutorials/klusterai-api/reliability-check/).
- **Check pricing**: Review [MCP usage pricing](https://kluster.ai/pricing){target=\_blank}.
--- END CONTENT ---

Doc-Content: https://docs.kluster.ai/get-started/mcp/get-started/
--- BEGIN CONTENT ---
---
title: Get started with MCP
description: Get kluster.ai verification tools integrated into Claude desktop in five minutes using Cloud MCP. No setup required, just enable and connect.
---

# Get started with MCP

Connect [kluster.ai's](https://www.kluster.ai/){target=\_blank} verification tools to your AI assistant through Model Context Protocol (MCP). This guide shows you how to enable [Cloud MCP](/get-started/mcp/cloud/platform/) and integrate it with Claude desktop for real-time claim validation directly within your conversations.

Cloud MCP provides managed verification endpoints with no infrastructure to maintain - just enable your MCP endpoint and start verifying.

## Prerequisites

Before getting started, ensure you have:

- **A kluster.ai account**: Sign up on the [kluster.ai platform](https://platform.kluster.ai/signup){target=\_blank} if you don't have one.
- **A kluster.ai API key**: After signing in, go to the [**API Keys**](https://platform.kluster.ai/apikeys){target=\_blank} section and create a new key. For detailed instructions, check out the [Get an API key](/get-started/get-api-key/){target=\_blank} guide.
- **[Claude desktop](https://claude.ai/download){target=\_blank}** for testing the integration.

## Enable MCP 

To enable the MCP endpoint, go to the [kluster.ai platform](https://platform.kluster.ai){target=\_blank} and take the following steps:

1. Navigate to **MCP** and view your current MCP status.
2. Click the **Enable Verify MCP** button to activate your endpoint.
3. Copy your client configuration.

!!! success "Endpoint enabled"
    Your MCP endpoint is now active. Copy your API key and save it securely.
    
The platform provides ready-to-use integration examples for VSCode, Cursor, Claude code, and Claude desktop.

![MCP kluster.ai platform](/images/get-started/mcp/cloud/platform/platform-1.webp)

## Configure Claude desktop

Edit your Claude desktop configuration file:

=== "macOS"

    ```text
    ~/Library/Application Support/Claude/claude_desktop_config.json
    ```

=== "Windows"

    ```text
    %APPDATA%/Claude/claude_desktop_config.json
    ```

Add the MCP server configuration:

```json
{
    "mcpServers": {
        "kluster-verify-mcp": {
            "command": "npx",
            "args": [
                "mcp-remote",
                "https://api.kluster.ai/v1/mcp",
                "--header",
                "Authorization: Bearer YOUR_MCP_TOKEN"
            ]
        }
    }
}
```

Replace `YOUR_MCP_TOKEN` with your actual token or copy the snippet from the platform.

Restart Claude desktop. Once Claude desktop restarts, you'll see the verification tools available under `kluster-verify-mcp`.

![List tools on Claude desktop](/images/get-started/mcp/get-started/get-started-1.webp)

## Available tools

Your MCP integration provides two verification tools:

- **`verify`**: Validates prompt and response pairs against reliable sources.
- **`verify_document`**: Verifies prompt and response pairs in relation to uploaded documents.
<!-- Commenting this for safekeeping -->
<!--For detailed parameters and response formats, see the [Tools reference](/get-started/mcp/tools/){target=\_blank}.-->

### Verify

Ask Claude to verify something obviously wrong:

> "The Eiffel Tower is located in Rome. Use the verify tool to check this."

Claude will automatically use the `verify` tool with:

- **`prompt`**: "Is the Eiffel Tower located in Rome?"
- **`response`**: "The Eiffel Tower is located in Rome."

And provides the following:

- **Verification result**: Whether the response contains hallucinations.
- **Detailed explanation**: Why it's wrong with supporting reasoning.
- **Source citations**: Search results used for verification.

![Verify MCP tool demo](/images/get-started/mcp/get-started/get-started-2.webp)

### Verify documents

Perfect for detecting hallucinations or false claims about documents. Upload any document to Claude, then ask:

> "Does this document say that employees can work remotely full-time? The document says employees can work remotely without restrictions. Use the verify_document tool to check."

Claude will use the `verify_document` tool with:

- **`prompt`**: "Does this document say that employees can work remotely full-time?"
- **`response`**: "The document says employees can work remotely without restrictions."
- **`documentContent`**:  The content of the document as provided by the MCP client for verification.

This verifies the response against the actual document content.

## Alternative setup options

- **Other clients**: Want to use VS Code, Cursor, or Claude Code? Check the [Client integrations](/get-started/mcp/integrations/){target=\_blank} guide for configuration examples.

- **Self-hosted**: Prefer to run MCP locally? Set up the [self-hosted MCP server](/get-started/mcp/self-hosted/){target=\_blank} for local development with full control.

- **API activation**: Enable MCP using API calls with the [MCP API usage guide](/get-started/mcp/cloud/api/){target=\_blank}.

## Next steps
<!-- Commenting this for safekeeping -->
<!--- **Learn the tools**: See [Tools reference](/get-started/mcp/tools/) for detailed parameters and examples.-->
- **Explore integrations**: Check [Client integrations](/get-started/mcp/integrations/) for other platforms.
- **Try the tutorial**: Follow the [Reliability check notebook](/tutorials/klusterai-api/reliability-check/) with code examples.
--- END CONTENT ---

Doc-Content: https://docs.kluster.ai/get-started/mcp/integrations/
--- BEGIN CONTENT ---
---
title: MCP client integrations
description: Connect Claude desktop, VS Code, Cursor, and Claude Code to kluster.ai verification tools with ready-to-use configuration examples.
---

# Client integrations

Connect any compatible client to [kluster.ai's](https://www.kluster.ai/){target=\_blank} MCP Verify server. This guide provides configuration examples for popular clients using [Cloud MCP](/get-started/mcp/cloud/platform/).

!!! info "Self-hosted deployment"
    For [self-hosted MCP](/get-started/mcp/self-hosted/){target=\_blank}, replace the URL with `http://localhost:3001/stream` and use your kluster.ai API key.

## Prerequisites
      
Before integrating with any client:
      
1. **Enable MCP**: Follow the [platform guide](/get-started/mcp/cloud/platform/){target=\_blank} to activate the MCP capabilities.
2. **Replace token**: Use your actual MCP token in place of `YOUR_MCP_TOKEN`.

## Configuration by client

=== "Claude desktop"

    Edit your Claude desktop configuration file:
      
    - **macOS**: `~/Library/Application Support/Claude/claude_desktop_config.json`
    - **Windows**: `%APPDATA%/Claude/claude_desktop_config.json`

    ```json
    {
    "mcpServers": {
        "kluster-verify-mcp": {
            "command": "npx",
            "args": [
                "mcp-remote",
                "https://api.kluster.ai/v1/mcp",
                "--header",
                "Authorization: Bearer YOUR_MCP_TOKEN"
            ]
        }
    }
}
    ```

    Restart Claude desktop to load the tools.

=== "VS Code"

    1. Install [GitHub Copilot](https://marketplace.visualstudio.com/items?itemName=GitHub.copilot){target=\_blank} extension.
    2. Open the Chat view and click on the tools icon.

        ![](/images/get-started/mcp/integrations/integrations-1.webp){ style="width:50%;" }

    3. Choose **Add More Tools...** and click on **Add MCP Server...**.
    4. Select **Command (stdio)** and enter the following command, replacing `YOUR_MCP_TOKEN` with your actual MCP token:

        ```bash
        npx mcp-remote https://api.kluster.ai/v1/mcp \
        --header "Authorization: Bearer YOUR_MCP_TOKEN"
        ```

    5. Restart VS Code.

=== "Cursor"

    Open Cursor settings and:
    
    1. Select **Tools & Integrations**.

    2. To add your first MCP, click **Add Custom MCP**. To add additional MCPs later, use **New MCP Server**. Then enter the following configuration:
            
        ```json
        {
            "mcpServers": {
                "kluster-verify-mcp": {
                    "url": "https://api.kluster.ai/v1/mcp",
                    "headers": {
                        "Authorization": "Bearer YOUR_MCP_TOKEN"
                    }
                }
            }
        }
        ```

    3. Restart Cursor.

    ![](/images/get-started/mcp/integrations/integrations-2.webp){ style="width:80%;" }

=== "Claude code"

    Run this command in your terminal:

    ```bash
    claude mcp add kluster-verify-mcp \
      npx mcp-remote https://api.kluster.ai/v1/mcp \
      --header "Authorization: Bearer YOUR_MCP_TOKEN"
    ```

## Available tools

- **`verify`**: Validates prompt and response pairs against reliable sources.
- **`verify_document`**: Verifies prompt and response pairs in relation to uploaded documents.
<!-- Commenting this for safekeeping -->
<!--See [Tools reference](/get-started/mcp/tools/){target=\_blank} for parameters and examples.-->

## Next steps

- [Complete setup guide](/get-started/mcp/get-started/) with usage examples.
- [Self-hosted deployment](/get-started/mcp/self-hosted/) for local development.
--- END CONTENT ---

Doc-Content: https://docs.kluster.ai/get-started/mcp/overview/
--- BEGIN CONTENT ---
---
title: MCP integration overview
description: Connect AI apps to kluster.ai services using MCP for seamless development workflow integration with verification tools and automated reliability checking.
---

# MCP integration

[Model Context Protocol](https://modelcontextprotocol.io/introduction){target=\_blank} (MCP) is an open standard for connecting AI assistants to specialized tools. Think of it as "USB-C for AI" - one protocol that works everywhere, enabling seamless integration between AI applications and external capabilities.

[kluster.ai](https://www.kluster.ai/){target=\_blank} provides MCP servers that bring AI services directly into your development workflow. Choose between a managed cloud endpoint or self-hosted deployment for seamless integration across platforms.

## What is MCP?

MCP lets AI applications access external capabilities:

- **Local tools**: Files, databases, custom functions.
- **Remote services**: APIs, web services, cloud resources.
- **Specialized features**: Like kluster.ai's verification technology.

## MCP through kluster.ai services

Instead of managing API calls and integrations, access kluster.ai's AI capabilities as native tools in Claude desktop, VS Code, and other MCP-compatible platforms.

The kluster.ai MCP offers the [Verify service](/get-started/verify/reliability/overview){target=\_blank} through two deployment options designed for different use cases and platforms.

### Cloud MCP

Managed cloud implementation - no infrastructure to maintain:

- **`verify`**: Validates prompt and response pairs against reliable sources.
- **`verify_document`**: Verifies prompt and response pairs in relation to uploaded documents.

Enable your endpoint through the kluster.ai platform, get your MCP token, and start verifying. Works with any MCP client using standard connection patterns.

### Self-hosted MCP

Same verification tools running on your infrastructure with full control. Deploy locally with Docker or Node.js.

## Integrate MCP

<div class="grid cards" markdown>

-   <span class="badge guide">Guide</span> __Get started with MCP__

    ---

    Quick start guide using Cloud MCP as the default path. Enable your endpoint and connect Claude Desktop in five minutes.

    [:octicons-arrow-right-24: Five-minute setup](/get-started/mcp/get-started/){target=_self}

-   <span class="badge guide">Guide</span> __Cloud MCP__

    ---

    Enable managed MCP endpoints with MCP token authentication. There is no infrastructure to maintain, just enable and integrate.

    [:octicons-arrow-right-24: Platform setup](/get-started/mcp/cloud/platform/){target=_self}

-   <span class="badge guide">Guide</span> __Self-hosted MCP__

    ---

    Deploy the MCP server locally with Docker or Node.js. Perfect for development and testing with full control.

    [:octicons-arrow-right-24: Local deployment](/get-started/mcp/self-hosted/){target=_self}

</div>

## Additional resources

- **MCP protocol**: [Official MCP documentation](https://modelcontextprotocol.io/docs){target=\_blank}.
- **Verify service**: [Complete reliability verification guide](/get-started/verify/reliability/overview).
- **API reference**: [kluster.ai API documentation](/api-reference/reference/).
--- END CONTENT ---

Doc-Content: https://docs.kluster.ai/get-started/mcp/self-hosted/
--- BEGIN CONTENT ---
---
title: Self-hosted MCP
description: Deploy kluster.ai's MCP server locally using Docker or Node.js for development and testing with full control over your infrastructure.
---

# Self-hosted MCP

Deploy [kluster.ai's](https://www.kluster.ai/){target=\_blank} MCP server locally for development and testing. This self-hosted implementation gives you full control over your infrastructure while providing the same verification tools as [Cloud MCP](/get-started/mcp/cloud/platform/){target=\_blank}.

## Prerequisites

Before deploying the self-hosted MCP server, ensure you have:

- **A kluster.ai account**: Sign up on the [kluster.ai platform](https://platform.kluster.ai/signup){target=\_blank} if you don't have one.
- **A kluster.ai API key**: After signing in, go to the [**API Keys**](https://platform.kluster.ai/apikeys){target=\_blank} section and create a new key. For detailed instructions, check out the [Get an API key](/get-started/get-api-key/){target=\_blank} guide.
- **A runtime environment**: You can use either [Docker Desktop](https://www.docker.com/products/docker-desktop/){target=\_blank} or [Node.js 18+](https://nodejs.org/){target=\_blank}.
- **[Git](https://git-scm.com/){target=\_blank}**: For cloning the repository.

## Clone repository

First, clone the MCP server repository:

```bash
git clone https://github.com/kluster-ai/verify-mcp
cd verify-mcp
```

## Deployment options

Run one of the following commands to either get started with Docker or Node.js:

=== "Docker"

    ```bash
    docker build -t kluster-verify-mcp .
    docker run --rm -p 3001:3001 kluster-verify-mcp --api-key YOUR_API_KEY
    ```

=== "Node.js"

    ```bash
    npm install
    npm run build
    npm start -- --api-key YOUR_API_KEY
    ```

The server will start on `http://localhost:3001` with the MCP endpoint at `/stream`.

## Client integration

Once your self-hosted server is running, configure your AI clients using the [Client integrations](/get-started/mcp/integrations/){target=\_blank} guide.

Use these connection details:

- **MCP endpoint**: `http://localhost:3001/stream`.
- **Authentication**: Your kluster.ai API key.

## Available tools

Your self-hosted deployment provides the same verification tools as Cloud MCP:

- **`verify`**: Validates prompt and response pairs against reliable sources.
- **`verify_document`**: Verifies prompt and response pairs in relation to uploaded documents.
<!-- Commenting this for safekeeping -->
<!--For detailed parameters and response formats, see the [Tools reference](/get-started/mcp/tools/){target=\_blank}.-->

## Next steps

- **Configure clients**: Follow the [Client integrations](/get-started/mcp/integrations/) guide for VS Code, Claude Desktop, and other platforms.
<!-- Commenting this for safekeeping -->
<!--- **Learn the tools**: See [Tools reference](/get-started/mcp/tools/) for detailed examples.-->
- **Try Cloud MCP**: Consider [Cloud MCP](/get-started/mcp/cloud/platform/) for managed cloud deployment.
--- END CONTENT ---

Doc-Content: https://docs.kluster.ai/get-started/mcp/tools/
--- BEGIN CONTENT ---
---
title: MCP tools reference
description: Reference guide for kluster.ai's MCP verification tools - verify claims and documents with detailed parameters and response formats.
search:
  exclude: true
---

# Tools reference

The [kluster.ai's](https://www.kluster.ai/){target=\_blank} MCP server provides two verification tools that work identically whether deployed [self-hosted](/get-started/mcp/self-hosted/){target=\_blank} or via [Cloud MCP](/get-started/mcp/cloud/platform/){target=\_blank}. These tools enable real-time reliability verification directly within your AI assistant conversations.

This page documents the tool parameters and response formats you'll see when using these tools in any MCP-compatible client.

## Tool overview


The following tools are available through the kluster.ai MCP server:

| Tool | Purpose | Best For |
|:---|:---|:---|
| `verify` | Verify prompt and response pairs | General statements, trivia, current events, news |
| `verify_document` | Verify prompt and response about documents | Quotes, data extraction, RAG hallucination checking |

### Verify

The verify tool allows you to check a prompt from a user and response from the agent against reliable online sources.

???+ interface "Parameters"

    `prompt` ++"string"++ <span class="required" markdown>++"required"++</span>

    The prompt the user made to the agent.

    ---

    `response` ++"string"++ <span class="required" markdown>++"required"++</span>

    The response from the agent that must be verified.

    ---

    `returnSearchResults` ++"boolean"++

    Include source citations. Defaults to `true`.

### Verify document

The verify document tool checks that a prompt from a user and a response from the agent accurately reflect the content of the uploaded document.

???+ interface "Parameters"


    `prompt` ++"string"++ <span class="required" markdown>++"required"++</span>

    The prompt the user made to the agent about the document.

    ---

    `response` ++"string"++ <span class="required" markdown>++"required"++</span>

    The response from the agent that must be verified against the document content.

    ---

    `documentContent` ++"string"++ <span class="required" markdown>++"required"++</span>

    Full document text (auto-provided by MCP client).

    ---

    `returnSearchResults` ++"boolean"++

    Include source citations. Defaults to `true`.

## Response fields

All verification tools return the same response structure:

- **`prompt`**: The user's prompt.
- **`response`**: The agent's response.
- **`is_hallucination`**: Boolean indicating if the response contains hallucinations.
- **`explanation`**: Detailed reasoning for the verdict.
- **`confidence`**: Token usage statistics `completion_tokens`, `prompt_tokens`, and `total_tokens`.
- **`search_results`**: Source citations (if requested).

An example can be seen below:

```json
{
    "prompt": "Does this employment contract allow unlimited remote work?",
    "response": "This employment contract allows unlimited remote work.",
    "is_hallucination": true,
    "explanation": "The response is incorrect. Section 4.2 explicitly requires on-site work minimum 3 days per week and residence within 50 miles of headquarters.",
    "confidence": {
        "completion_tokens": 156,
        "prompt_tokens": 890,
        "total_tokens": 1046
    },
    "search_results": []
}
```

## Next steps

- **Set up integrations**: Configure [client applications](/get-started/mcp/integrations/) to use these tools.
- **Deploy locally**: Set up a [self-hosted MCP server](/get-started/mcp/self-hosted/) for local development.
- **Use cloud version**: Enable [Cloud MCP](/get-started/mcp/cloud/platform/) for managed deployment.
--- END CONTENT ---

Doc-Content: https://docs.kluster.ai/get-started/models/
--- BEGIN CONTENT ---
---
title: Supported AI Models
description: Learn what models are supported by the kluster.ai API and the main characteristics and API request limits for each model for both free and standard tiers.
---

# Models on kluster.ai

[kluster.ai](https://kluster.ai){target=\_blank} offers a wide variety of open-source models for both real-time and batch inferences, with more being constantly added.
 
This page covers all the models the API supports, with the API request limits for each.

## Model names

Each model supported by kluster.ai has a unique name that must be used when defining the `model` in the request.

|             Model             |                   Model API name                    |
|:-----------------------------:|:---------------------------------------------------:|
|     **DeepSeek-R1-0528**      |           `deepseek-ai/DeepSeek-R1-0528`            |
|     **DeepSeek-V3-0324**      |           `deepseek-ai/DeepSeek-V3-0324`            |
|        **Gemma 3 27B**        |               `google/gemma-3-27b-it`               |
|      **Magistral Small**      |          `mistralai/Magistral-Small-2506`           |
|     **Meta Llama 3.1 8B**     |    `klusterai/Meta-Llama-3.1-8B-Instruct-Turbo`     |
|    **Meta Llama 3.3 70B**     |    `klusterai/Meta-Llama-3.3-70B-Instruct-Turbo`    |
|   **Meta Llama 4 Maverick**   | `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` |
|    **Meta Llama 4 Scout**     |     `meta-llama/Llama-4-Scout-17B-16E-Instruct`     |
|       **Mistral NeMo**        |       `mistralai/Mistral-Nemo-Instruct-2407`        |
|       **Mistral Small**       |     `mistralai/Mistral-Small-24B-Instruct-2501`     |
|       **Qwen2.5-VL 7B**       |            `Qwen/Qwen2.5-VL-7B-Instruct`            |
|      **Qwen3-235B-A22B**      |             `Qwen/Qwen3-235B-A22B-FP8`              |
| **kluster reliability check** |           `klusterai/verify-reliability`            |

## Model comparison table

| Model | Description | Real-time<br>inference | Batch<br>inference | Tools | Fine-tuning | Image<br>analysis  |
|:-----------------------------:|:-------------------------------------------------------------------:|:------------------------------:|:--------------------------:|:----------------------:|:----------------------:|:------------------:|
| **DeepSeek-R1-0528** | Mathematical problem-solving<br>code generation<br>complex data analysis. | :white_check_mark: | :white_check_mark: | :x: |:x: | :x: |
| **DeepSeek-V3-0324** | Natural language generation<br>open-ended text creation<br>contextually rich writing. | :white_check_mark: | :white_check_mark: | :white_check_mark: |:x: | :x: |
| **Gemma 3 27B** | Multilingual applications<br>extended-context tasks<br>image analysis<br>and complex reasoning. | :white_check_mark: | :white_check_mark: | :x: |:x: | :white_check_mark: |
| **Magistral Small** | Reasoning<br>Natural language generation<br>open-ended text creation<br>contextually rich writing. | :white_check_mark: | :white_check_mark: | :x: |:x: | :x: |
| **Llama 3.1 8B** | Low-latency or simple tasks<br>cost-efficient inference. | :white_check_mark: | :white_check_mark: | :white_check_mark: |:white_check_mark: | :x: |
| **Llama 3.3 70B** | General-purpose AI<br>balanced cost-performance. | :white_check_mark: | :white_check_mark: | :white_check_mark: |:white_check_mark: | :x: |
| **Llama 4 Maverick** | A state-of-the-art multimodal<br>model with integrated vision<br>and language understanding,<br>optimized for complex<br>reasoning, coding, and<br>perception tasks | :white_check_mark: | :white_check_mark: | :x: |:x: | :white_check_mark: |
| **Llama 4 Scout** | General-purpose multimodal AI<br>extended context tasks<br>and balanced cost-performance across text and vision. | :white_check_mark: | :white_check_mark: | :x: |:x: | :white_check_mark: |
| **Mistral NeMo** | Natural language generation<br>open-ended text creation<br>contextually rich writing. | :white_check_mark: | :white_check_mark: | :x: |:x: | :x: |
| **Mistral Small** | Fast conversational agents<br>local inference on consumer hardware<br>domain-specific applications<br>and low-latency function calling. | :white_check_mark: | :white_check_mark: | :x: |:x: | :x: |
| **Qwen2.5-VL 7B** | Visual question answering<br>document analysis<br>image-based reasoning<br>multimodal chat. | :white_check_mark: | :white_check_mark: | :x: |:x: | :white_check_mark: |
| **Qwen3-235B-A22B** | Qwen3's flagship 235 billion<br>parameter model optimized with<br>8-bit quantization | :white_check_mark: | :white_check_mark: | :white_check_mark: |:x: | :x: |
| **kluster reliability check** | kluster.ai Verify is an advanced<br>AI-powered fact-checking tool<br>designed to identify<br>inaccuracies and<br>hallucinations in AI-generated<br>text | :white_check_mark: | :white_check_mark: | :x: |:x: | :x: |

## API request limits

The following limits apply to API requests based on [your plan](https://platform.kluster.ai/plans){target=\_blank}:

=== "Trial"

    |             Model             | Context size<br>[tokens] | Max output<br>[tokens] | Max batch<br>requests | Concurrent<br>requests | Requests<br>per minute | Hosted fine-tuned<br>models |
    |:-----------------------------:|:------------------------:|:----------------------:|:---------------------:|:----------------------:|:----------------------:|:---------------------------:|
    |**DeepSeek-R1-0528**|32k|4k|1000|20|30|1|
    |**DeepSeek-V3-0324**|32k|4k|1000|20|30|1|
    |**Gemma 3 27B**|32k|4k|1000|20|30|1|
    |**Magistral Small**|32k|4k|1000|20|30|1|
    |**Meta Llama 3.1 8B**|32k|4k|1000|20|30|1|
    |**Meta Llama 3.3 70B**|32k|4k|1000|20|30|1|
    |**Meta Llama 4 Maverick**|32k|4k|1000|20|30|1|
    |**Meta Llama 4 Scout**|32k|4k|1000|20|30|1|
    |**Mistral NeMo**|32k|4k|1000|20|30|1|
    |**Mistral Small**|32k|4k|1000|20|30|1|
    |**Qwen2.5-VL 7B**|32k|4k|1000|20|30|1|
    |**Qwen3-235B-A22B**|32k|4k|1000|20|30|1|
    |**kluster reliability check**|32k|4k|1000|20|30|1|



=== "Core"

    |             Model             | Context size<br>[tokens] | Max output<br>[tokens] | Max batch<br>requests | Concurrent<br>requests | Requests<br>per minute | Hosted fine-tuned<br>models |
    |:-----------------------------:|:------------------------:|:----------------------:|:---------------------:|:----------------------:|:----------------------:|:---------------------------:|
    |**DeepSeek-R1-0528**|163k|163k|100k|100|600|10|
    |**DeepSeek-V3-0324**|163k|163k|100k|100|600|10|
    |**Gemma 3 27B**|64k|8k|100k|100|600|10|
    |**Magistral Small**|40k|40k|100k|100|600|10|
    |**Meta Llama 3.1 8B**|131k|131k|100k|100|600|10|
    |**Meta Llama 3.3 70B**|131k|131k|100k|100|600|10|
    |**Meta Llama 4 Maverick**|1M|1M|100k|100|600|10|
    |**Meta Llama 4 Scout**|131k|131k|100k|100|600|10|
    |**Mistral NeMo**|131k|131k|100k|100|600|10|
    |**Mistral Small**|32k|32k|100k|100|600|10|
    |**Qwen2.5-VL 7B**|32k|32k|100k|100|600|10|
    |**Qwen3-235B-A22B**|40k|40k|100k|100|600|10|
    |**kluster reliability check**|100k|0|100k|100|600|10|



=== "Scale"

    |             Model             | Context size<br>[tokens] | Max output<br>[tokens] | Max batch<br>requests | Concurrent<br>requests | Requests<br>per minute | Hosted fine-tuned<br>models |
    |:-----------------------------:|:------------------------:|:----------------------:|:---------------------:|:----------------------:|:----------------------:|:---------------------------:|
    |**DeepSeek-R1-0528**|163k|163k|500k|100|1200|25|
    |**DeepSeek-V3-0324**|163k|163k|500k|100|1200|25|
    |**Gemma 3 27B**|64k|8k|500k|100|1200|25|
    |**Magistral Small**|40k|40k|500k|100|1200|25|
    |**Meta Llama 3.1 8B**|131k|131k|500k|100|1200|25|
    |**Meta Llama 3.3 70B**|131k|131k|500k|100|1200|25|
    |**Meta Llama 4 Maverick**|1M|1M|500k|100|1200|25|
    |**Meta Llama 4 Scout**|131k|131k|500k|100|1200|25|
    |**Mistral NeMo**|131k|131k|500k|100|1200|25|
    |**Mistral Small**|32k|32k|500k|100|1200|25|
    |**Qwen2.5-VL 7B**|32k|32k|500k|100|1200|25|
    |**Qwen3-235B-A22B**|40k|40k|500k|100|1200|25|
    |**kluster reliability check**|100k|0|500k|100|1200|25|



=== "Enterprise"

    |             Model             | Context size<br>[tokens] | Max output<br>[tokens] | Max batch<br>requests | Concurrent<br>requests | Requests<br>per minute | Hosted fine-tuned<br>models |
    |:-----------------------------:|:------------------------:|:----------------------:|:---------------------:|:----------------------:|:----------------------:|:---------------------------:|
    |**DeepSeek-R1-0528**|163k|163k|Unlimited|100|Unlimited|Unlimited|
    |**DeepSeek-V3-0324**|163k|163k|Unlimited|100|Unlimited|Unlimited|
    |**Gemma 3 27B**|64k|8k|Unlimited|100|Unlimited|Unlimited|
    |**Magistral Small**|40k|40k|Unlimited|100|Unlimited|Unlimited|
    |**Meta Llama 3.1 8B**|131k|131k|Unlimited|100|Unlimited|Unlimited|
    |**Meta Llama 3.3 70B**|131k|131k|Unlimited|100|Unlimited|Unlimited|
    |**Meta Llama 4 Maverick**|1M|1M|Unlimited|100|Unlimited|Unlimited|
    |**Meta Llama 4 Scout**|131k|131k|Unlimited|100|Unlimited|Unlimited|
    |**Mistral NeMo**|131k|131k|Unlimited|100|Unlimited|Unlimited|
    |**Mistral Small**|32k|32k|Unlimited|100|Unlimited|Unlimited|
    |**Qwen2.5-VL 7B**|32k|32k|Unlimited|100|Unlimited|Unlimited|
    |**Qwen3-235B-A22B**|40k|40k|Unlimited|100|Unlimited|Unlimited|
    |**kluster reliability check**|100k|0|Unlimited|100|Unlimited|Unlimited|
--- END CONTENT ---

Doc-Content: https://docs.kluster.ai/get-started/openai-compatibility/
--- BEGIN CONTENT ---
---
title: Compatibility with OpenAI client libraries
description: Learn how kluster.ai is fully compatible with OpenAI client libraries, enabling seamless integration with your existing applications.
---

# OpenAI compatibility

The [kluster.ai](https://www.kluster.ai/){target=\_blank} API is compatible with [OpenAI](https://platform.openai.com/docs/api-reference/introduction){target=\_blank}'s API and SDKs, allowing seamless integration into your existing applications.

If you already have an application running with the OpenAI client library, you can easily switch to kluster.ai's API with minimal changes. This ensures a smooth transition without the need for significant refactoring or rework.

## Configuring OpenAI to use kluster.ai's API

Developers can use the OpenAI libraries with kluster.ai with no changes. To start, you need to install the library:

=== "Python"

    ```python
    pip install "openai>={{ libraries.openai_api.min_version }}"
    ```

To start using kluster.ai with OpenAI's client libraries, set your [API key](/get-started/get-api-key/){target=\_blank} and change the base URL to `https://api.kluster.ai/v1`:

=== "Python"

    ```python
    from openai import OpenAI
    
    client = OpenAI(
        base_url="https://api.kluster.ai/v1",
        api_key="INSERT_API_KEY",  # Replace with your actual API key
    )
    ```

## Unsupported OpenAI features

While kluster.ai's API is largely compatible with OpenAI's, the following sections outline the specific features and fields that are currently unsupported.

### Chat completions parameters

When creating a chat completion via the [`POST https://api.kluster.ai/v1/chat/completions` endpoint](/api-reference/reference/#/http/api-endpoints/realtime/v1-chat-completions-post){target=\_blank}, the following request parameters are not supported:

- `messages[].name`: Attribute in `system`, `user`, and `assistant` type message objects.
- `messages[].refusal`: Attribute in `assistant` type message objects.
- `messages[].audio`: Attribute in `assistant` type message objects.
- `messages[].tool_calls`: Attribute in `assistant` type message objects.
- `store`
- `n`
- `modalities`
- `response_format`
- `service_tier`
- `stream_options`

The following request parameters are supported only with Llama models:

- `tools`
- `tool_choice`
- `parallel_tool_calls`

The following request parameters are *deprecated*:

- `messages[].function_call`: Attribute in `assistant` type message objects. <!-- TODO: Once `messages[].tool_calls` is supported, this should be updated to use `messages[].tool_calls instead -->
- `max_tokens`: Use `max_completion_tokens` instead.
- `function_call` <!-- TODO: Once `tool_choice` is supported, this should be updated to use `tool_choice` instead -->
- `functions` <!-- TODO: Once `tools` is supported, this should be updated to use `tools` instead -->

For more information on these parameters, refer to [OpenAI's API documentation on creating chat completions](https://platform.openai.com/docs/api-reference/chat/create){target=_blank}.

### Chat completion object

The following fields of the [chat completion object](/api-reference/reference/#/http/models/structures/v1-chat-completions-request){target=\_blank} are not supported:

- `system_fingerprint`
- `usage.completion_tokens_details`
- `usage.prompt_tokens_details`

For more information on these parameters, refer to [OpenAI's API documentation on the chat completion object](https://platform.openai.com/docs/api-reference/chat/object){target=_blank}.
--- END CONTENT ---

Doc-Content: https://docs.kluster.ai/get-started/start-building/batch/
--- BEGIN CONTENT ---
---

title: Perform batch inference jobs
description: This guide provides examples and instructions on how to create, submit, retrieve, and manage batch inference jobs using kluster.ai's OpenAI-compatible API.
---

# Perform batch inference jobs

## Overview

This guide provides examples and instructions on how to create, submit, retrieve, and manage batch inference jobs using the [kluster.ai](https://www.kluster.ai/){target=\_blank} API. You will find guidance about preparing your data, selecting a model, submitting your batch job, and retrieving your results. Please make sure you check the [API request limits](/get-started/models/#api-request-limits){target=\_blank}.

## Prerequisites

This guide assumes familiarity with Large Language Model (LLM) development and OpenAI libraries. Before getting started, make sure you have:

- **A kluster.ai account**: Sign up on the [kluster.ai platform](https://platform.kluster.ai/signup){target=\_blank} if you don't have one.
- **A kluster.ai API key**: After signing in, go to the [**API Keys**](https://platform.kluster.ai/apikeys){target=\_blank} section and create a new key. For detailed instructions, check out the [Get an API key](/get-started/get-api-key/){target=\_blank} guide.
- **A virtual Python environment (optional)**: Recommended for developers using Python. It helps isolate Python installations in a [virtual environment](https://packaging.python.org/en/latest/guides/installing-using-pip-and-virtual-environments/){target=\_blank} to reduce the risk of environment or package conflicts between your projects.
- **Required Python libraries**: Install the following Python libraries:
    - [**OpenAI Python API library**](https://pypi.org/project/openai/){target=\_blank}: To access the `openai` module.
    - [**`getpass`**](https://pypi.org/project/getpass4/){target=\_blank}: To handle API keys safely.
- **A basic understanding of** [**JSON Lines (JSONL)**](https://jsonlines.org/){target=\_blank}: JSONL is the required text input format for performing batch inferences with the kluster.ai API.

If you plan to use cURL via the CLI, you can export your kluster.ai API key as a variable:

```bash
export API_KEY=INSERT_API_KEY
```

## Supported models

Please visit the [Models](/get-started/models/){target=\_blank} page to learn more about all the models supported by the kluster.ai batch API.

In addition, you can see the complete list of available models programmatically using the [list supported models](/api-reference/reference/#/http/api-endpoints/models/v1-models-get){target=\_blank} endpoint.

## Batch job workflow overview

Working with batch jobs in the kluster.ai API involves the following steps:

1. **Create batch job file**: Prepare a JSON Lines file containing one or more chat completion requests to execute in the batch.
2. **Upload batch job file**: Upload the file to kluster.ai to receive a unique file ID.
3. **Start the batch job**: Initiate a new batch job using the file ID.
4. **Monitor job progress**: Track the status of your batch job to ensure successful completion.
5. **Retrieve results**: Once the job finishes, access and process the results as needed.

In addition to these core steps, this guide will give you hands-on experience to:

- **Cancel a batch job**: Cancel an ongoing batch job before it completes.
- **List all batch jobs**: Review all of your batch jobs.

!!! warning
    For the free tier, the maximum number of batch requests (lines in the JSONL file) must be less than {{ batch.max_lines_free }}, and each file must not exceed {{ batch.max_size }}. For the standard tier, there is no limit to the number of batch requests, but the maximum batch file size is {{ batch.max_size }} per file.

## Quickstart snippets

The following code snippets provide a full end-to-end batch inference example for different models supported by kluster.ai. You can simply copy and paste the snippet into your local environment.

### Python

To use these snippets, run the Python script and enter your kluster.ai API key when prompted.

??? example "DeepSeek-R1"

    ```python
    # Batch completions with the DeepSeek-R1 model on kluster.ai

from os import environ
from openai import OpenAI
from getpass import getpass
import json
import time

# Get API key from user input
api_key = environ.get("API_KEY") or getpass("Enter your kluster.ai API key: ")

# Initialize OpenAI client pointing to kluster.ai API
client = OpenAI(
    base_url="https://api.kluster.ai/v1",
    api_key=api_key,
)

print(f"📤 Sending batch request to kluster.ai...\n")

# Create request with specified structure
requests = [
    {
        "custom_id": "request-1",
        "method": "POST",
        "url": "/v1/chat/completions",
        "body": {
            "model": "deepseek-ai/DeepSeek-R1",
            "messages": [
                {"role": "system", "content": "You are an experienced cook."},
                {"role": "user", "content": "What is the ultimate breakfast sandwich?"},
            ],
            "max_completion_tokens": 1000,
        },
    },
    {
        "custom_id": "request-2",
        "method": "POST",
        "url": "/v1/chat/completions",
        "body": {
            "model": "deepseek-ai/DeepSeek-R1",
            "messages": [
                {"role": "system", "content": "You are a maths tutor."},
                {"role": "user", "content": "Explain the Pythagorean theorem."},
            ],
            "max_completion_tokens": 1000,
        },
    },
    {
        "custom_id": "request-3",
        "method": "POST",
        "url": "/v1/chat/completions",
        "body": {
            "model": "deepseek-ai/DeepSeek-R1",
            "messages": [
                {
                    "role": "system",
                    "content": "You are a multilingual, experienced maths tutor.",
                },
                {
                    "role": "user",
                    "content": "Explain the Pythagorean theorem in Spanish",
                },
            ],
            "max_completion_tokens": 1000,
        },
    },
    # Additional tasks can be added here
]

# Save tasks to a JSONL file (newline-delimited JSON)
file_name = "my_batch_request.jsonl"
with open(file_name, "w") as file:
    for request in requests:
        file.write(json.dumps(request) + "\n")

# Upload batch job file
batch_input_file = client.files.create(
        file=open(file_name, "rb"),
        purpose="batch"
)

# Submit batch job
batch_request = client.batches.create(
    input_file_id=batch_input_file.id,
    endpoint="/v1/chat/completions",
    completion_window="24h",
)

# Poll the batch status until it's complete
while True:
    batch_status = client.batches.retrieve(batch_request.id)
    print(f"Batch status: {batch_status.status}")
    print(
        f"Completed tasks: {batch_status.request_counts.completed} / {batch_status.request_counts.total}"
    )

    if batch_status.status.lower() in ["completed", "failed", "cancelled"]:
        break

    time.sleep(10)  # Wait for 10 seconds before checking again

# Check if the Batch completed successfully
if batch_status.status.lower() == "completed":
    # Retrieve the results and log
    result_file_id = batch_status.output_file_id
    results = client.files.content(result_file_id).content

    # Print response to console
    print(f"\n🔍 AI batch response:")
    print(results)
else:
    print(f"Batch failed with status: {batch_status.status}")
    ```

??? example "DeepSeek-R1-0528"

    ```python
    # Batch completions with the DeepSeek-R1-0528 model on kluster.ai

from os import environ
from openai import OpenAI
from getpass import getpass
import json
import time

# Get API key from user input
api_key = environ.get("API_KEY") or getpass("Enter your kluster.ai API key: ")

# Initialize OpenAI client pointing to kluster.ai API
client = OpenAI(
    base_url="https://api.kluster.ai/v1",
    api_key=api_key,
)

print(f"📤 Sending batch request to kluster.ai...\n")

# Create request with specified structure
requests = [
    {
        "custom_id": "request-1",
        "method": "POST",
        "url": "/v1/chat/completions",
        "body": {
            "model": "deepseek-ai/DeepSeek-R1-0528",
            "messages": [
                {"role": "system", "content": "You are an experienced cook."},
                {"role": "user", "content": "What is the ultimate breakfast sandwich?"},
            ],
            "max_completion_tokens": 1000,
        },
    },
    {
        "custom_id": "request-2",
        "method": "POST",
        "url": "/v1/chat/completions",
        "body": {
            "model": "deepseek-ai/DeepSeek-R1-0528",
            "messages": [
                {"role": "system", "content": "You are a maths tutor."},
                {"role": "user", "content": "Explain the Pythagorean theorem."},
            ],
            "max_completion_tokens": 1000,
        },
    },
    {
        "custom_id": "request-3",
        "method": "POST",
        "url": "/v1/chat/completions",
        "body": {
            "model": "deepseek-ai/DeepSeek-R1-0528",
            "messages": [
                {
                    "role": "system",
                    "content": "You are a multilingual, experienced maths tutor.",
                },
                {
                    "role": "user",
                    "content": "Explain the Pythagorean theorem in Spanish",
                },
            ],
            "max_completion_tokens": 1000,
        },
    },
    # Additional tasks can be added here
]

# Save tasks to a JSONL file (newline-delimited JSON)
file_name = "my_batch_request.jsonl"
with open(file_name, "w") as file:
    for request in requests:
        file.write(json.dumps(request) + "\n")

# Upload batch job file
batch_input_file = client.files.create(
        file=open(file_name, "rb"),
        purpose="batch"
)

# Submit batch job
batch_request = client.batches.create(
    input_file_id=batch_input_file.id,
    endpoint="/v1/chat/completions",
    completion_window="24h",
)

# Poll the batch status until it's complete
while True:
    batch_status = client.batches.retrieve(batch_request.id)
    print(f"Batch status: {batch_status.status}")
    print(
        f"Completed tasks: {batch_status.request_counts.completed} / {batch_status.request_counts.total}"
    )

    if batch_status.status.lower() in ["completed", "failed", "cancelled"]:
        break

    time.sleep(10)  # Wait for 10 seconds before checking again

# Check if the Batch completed successfully
if batch_status.status.lower() == "completed":
    # Retrieve the results and log
    result_file_id = batch_status.output_file_id
    results = client.files.content(result_file_id).content

    # Print response to console
    print(f"\n🔍 AI batch response:")
    print(results)
else:
    print(f"Batch failed with status: {batch_status.status}")
    ```

??? example "DeepSeek-V3-0324"

    ```python
    # Batch completions with the DeepSeek-V3-0324 model on kluster.ai

from os import environ
from openai import OpenAI
from getpass import getpass
import json
import time

# Get API key from user input
api_key = environ.get("API_KEY") or getpass("Enter your kluster.ai API key: ")

# Initialize OpenAI client pointing to kluster.ai API
client = OpenAI(
    base_url="https://api.kluster.ai/v1",
    api_key=api_key,
)

print(f"📤 Sending batch request to kluster.ai...\n")

# Create request with specified structure
requests = [
    {
        "custom_id": "request-1",
        "method": "POST",
        "url": "/v1/chat/completions",
        "body": {
            "model": "deepseek-ai/DeepSeek-V3-0324",
            "messages": [
                {"role": "system", "content": "You are an experienced cook."},
                {"role": "user", "content": "What is the ultimate breakfast sandwich?"},
            ],
            "max_completion_tokens": 1000,
        },
    },
    {
        "custom_id": "request-2",
        "method": "POST",
        "url": "/v1/chat/completions",
        "body": {
            "model": "deepseek-ai/DeepSeek-V3-0324",
            "messages": [
                {"role": "system", "content": "You are a maths tutor."},
                {"role": "user", "content": "Explain the Pythagorean theorem."},
            ],
            "max_completion_tokens": 1000,
        },
    },
    {
        "custom_id": "request-3",
        "method": "POST",
        "url": "/v1/chat/completions",
        "body": {
            "model": "deepseek-ai/DeepSeek-V3-0324",
            "messages": [
                {
                    "role": "system",
                    "content": "You are a multilingual, experienced maths tutor.",
                },
                {
                    "role": "user",
                    "content": "Explain the Pythagorean theorem in Spanish",
                },
            ],
            "max_completion_tokens": 1000,
        },
    },
    # Additional tasks can be added here
]

# Save tasks to a JSONL file (newline-delimited JSON)
file_name = "my_batch_request.jsonl"
with open(file_name, "w") as file:
    for request in requests:
        file.write(json.dumps(request) + "\n")

# Upload batch job file
batch_input_file = client.files.create(
        file=open(file_name, "rb"),
        purpose="batch"
)

# Submit batch job
batch_request = client.batches.create(
    input_file_id=batch_input_file.id,
    endpoint="/v1/chat/completions",
    completion_window="24h",
)

# Poll the batch status until it's complete
while True:
    batch_status = client.batches.retrieve(batch_request.id)
    print(f"Batch status: {batch_status.status}")
    print(
        f"Completed tasks: {batch_status.request_counts.completed} / {batch_status.request_counts.total}"
    )

    if batch_status.status.lower() in ["completed", "failed", "cancelled"]:
        break

    time.sleep(10)  # Wait for 10 seconds before checking again

# Check if the Batch completed successfully
if batch_status.status.lower() == "completed":
    # Retrieve the results and log
    result_file_id = batch_status.output_file_id
    results = client.files.content(result_file_id).content

    # Print response to console
    print(f"\n🔍 AI batch response:")
    print(results)
else:
    print(f"Batch failed with status: {batch_status.status}")
    ```

??? example "Gemma 3 27B"

    ```python
    # Batch completions with the Gemma 3 27B model on kluster.ai

from os import environ
import json
import time
from getpass import getpass

from openai import OpenAI

# Newton's cradle
image1_url="https://github.com/kluster-ai/klusterai-cookbook/blob/main/images/balls-image.jpeg?raw=true"
# Text with typos
image2_url="https://github.com/kluster-ai/klusterai-cookbook/blob/main/images/text-typo-image.jpeg?raw=true"
# Parking sign
image3_url="https://github.com/kluster-ai/klusterai-cookbook/blob/main/images/parking-image.jpeg?raw=true"

# Get API key from user input
api_key = environ.get("API_KEY") or getpass("Enter your kluster.ai API key: ")

# Initialize OpenAI client pointing to kluster.ai API
client = OpenAI(
    base_url="https://api.kluster.ai/v1",
    api_key=api_key,
)

print(f"📤 Sending batch request to kluster.ai...\n")

# Create request with specified structure
requests = [
    {
        "custom_id": "request-1",
        "method": "POST",
        "url": "/v1/chat/completions",
        "body": {
            "model": "google/gemma-3-27b-it",
            "messages": [
                {
                    "role": "user",
                    "content": [
                        {"type": "text", "text": "What is this?"},
                        {
                            "type": "image_url",
                            "image_url": {
                                "url": image1_url
                            },
                        },
                    ],
                }
            ],
            "max_completion_tokens": 1000,
        },
    },
    {
        "custom_id": "request-2",
        "method": "POST",
        "url": "/v1/chat/completions",
        "body": {
            "model": "google/gemma-3-27b-it",
            "messages": [
                {
                    "role": "user",
                    "content": [
                        {"type": "text", "text": "Extract the text, find typos if any."},
                        {
                            "type": "image_url",
                            "image_url": {
                                "url": image2_url
                            },
                        },
                    ],
                }
            ],
            "max_completion_tokens": 1000,
        },
    },
    {
        "custom_id": "request-3",
        "method": "POST",
        "url": "/v1/chat/completions",
        "body": {
            "model": "google/gemma-3-27b-it",
            "messages": [
                {
                    "role": "user",
                    "content": [
                        {"type": "text", "text": "Who can park in the area?"},
                        {
                            "type": "image_url",
                            "image_url": {
                                "url": image3_url
                            },
                        },
                    ],
                }
            ],
            "max_completion_tokens": 1000,
        },
    },
]

# Save tasks to a JSONL file (newline-delimited JSON)
file_name = "my_batch_request.jsonl"
with open(file_name, "w") as file:
    for request in requests:
        file.write(json.dumps(request) + "\n")

# Upload batch job file
batch_input_file = client.files.create(file=open(file_name, "rb"), purpose="batch")

# Submit batch job
batch_request = client.batches.create(
    input_file_id=batch_input_file.id,
    endpoint="/v1/chat/completions",
    completion_window="24h",
)

# Poll the batch status until it's complete
while True:
    batch_status = client.batches.retrieve(batch_request.id)
    print(f"Batch status: {batch_status.status}")
    print(
        f"Completed tasks: {batch_status.request_counts.completed} / {batch_status.request_counts.total}"
    )

    if batch_status.status.lower() in ["completed", "failed", "cancelled"]:
        break

    time.sleep(10)  # Wait for 10 seconds before checking again

print(f"\nImage1 URL: {image1_url}")
print(f"\nImage2 URL: {image2_url}")
print(f"\nImage3 URL: {image3_url}")

# Check if the Batch completed successfully
if batch_status.status.lower() == "completed":
    # Retrieve the results and log
    result_file_id = batch_status.output_file_id
    results = client.files.content(result_file_id).content

    # Print response to console
    print(f"\n🔍 AI batch response:")
    print(results)
else:
    print(f"Batch failed with status: {batch_status.status}")
    print(batch_status)
    ```

??? example "Magistral Small"

    ```python
    # Batch completions with the Magistral Small model on kluster.ai

from os import environ
from openai import OpenAI
from getpass import getpass
import json
import time

# Get API key from user input
api_key = environ.get("API_KEY") or getpass("Enter your kluster.ai API key: ")

# Initialize OpenAI client pointing to kluster.ai API
client = OpenAI(
    base_url="https://api.kluster.ai/v1",
    api_key=api_key,
)

print(f"📤 Sending batch request to kluster.ai...\n")

# Create request with specified structure
requests = [
    {
        "custom_id": "request-1",
        "method": "POST",
        "url": "/v1/chat/completions",
        "body": {
            "model": "mistralai/Magistral-Small-2506",
            "messages": [
                {"role": "system", "content": "You are an experienced cook."},
                {"role": "user", "content": "What is the ultimate breakfast sandwich?"},
            ],
            "max_completion_tokens": 1000,
        },
    },
    {
        "custom_id": "request-2",
        "method": "POST",
        "url": "/v1/chat/completions",
        "body": {
            "model": "mistralai/Magistral-Small-2506",
            "messages": [
                {"role": "system", "content": "You are a maths tutor."},
                {"role": "user", "content": "Explain the Pythagorean theorem."},
            ],
            "max_completion_tokens": 1000,
        },
    },
    {
        "custom_id": "request-3",
        "method": "POST",
        "url": "/v1/chat/completions",
        "body": {
            "model": "mistralai/Magistral-Small-2506",
            "messages": [
                {
                    "role": "system",
                    "content": "You are a multilingual, experienced maths tutor.",
                },
                {
                    "role": "user",
                    "content": "Explain the Pythagorean theorem in Spanish",
                },
            ],
            "max_completion_tokens": 1000,
        },
    },
    # Additional tasks can be added here
]

# Save tasks to a JSONL file (newline-delimited JSON)
file_name = "my_batch_request.jsonl"
with open(file_name, "w") as file:
    for request in requests:
        file.write(json.dumps(request) + "\n")

# Upload batch job file
batch_input_file = client.files.create(
        file=open(file_name, "rb"),
        purpose="batch"
)

# Submit batch job
batch_request = client.batches.create(
    input_file_id=batch_input_file.id,
    endpoint="/v1/chat/completions",
    completion_window="24h",
)

# Poll the batch status until it's complete
while True:
    batch_status = client.batches.retrieve(batch_request.id)
    print(f"Batch status: {batch_status.status}")
    print(
        f"Completed tasks: {batch_status.request_counts.completed} / {batch_status.request_counts.total}"
    )

    if batch_status.status.lower() in ["completed", "failed", "cancelled"]:
        break

    time.sleep(10)  # Wait for 10 seconds before checking again

# Check if the Batch completed successfully
if batch_status.status.lower() == "completed":
    # Retrieve the results and log
    result_file_id = batch_status.output_file_id
    results = client.files.content(result_file_id).content

    # Print response to console
    print(f"\n🔍 AI batch response:")
    print(results)
else:
    print(f"Batch failed with status: {batch_status.status}")
    ```

??? example "Meta Llama 3.1 8B"

    ```python
    # Batch completions with the Meta Llama 3.1 8B model on kluster.ai

from os import environ
from openai import OpenAI
from getpass import getpass
import json
import time

# Get API key from user input
api_key = environ.get("API_KEY") or getpass("Enter your kluster.ai API key: ")

# Initialize OpenAI client pointing to kluster.ai API
client = OpenAI(
    base_url="https://api.kluster.ai/v1",
    api_key=api_key,
)

print(f"📤 Sending batch request to kluster.ai...\n")

# Create request with specified structure
requests = [
    {
        "custom_id": "request-1",
        "method": "POST",
        "url": "/v1/chat/completions",
        "body": {
            "model": "klusterai/Meta-Llama-3.1-8B-Instruct-Turbo",
            "messages": [
                {"role": "system", "content": "You are an experienced cook."},
                {"role": "user", "content": "What is the ultimate breakfast sandwich?"},
            ],
            "max_completion_tokens": 1000,
        },
    },
    {
        "custom_id": "request-2",
        "method": "POST",
        "url": "/v1/chat/completions",
        "body": {
            "model": "klusterai/Meta-Llama-3.1-8B-Instruct-Turbo",
            "messages": [
                {"role": "system", "content": "You are a maths tutor."},
                {"role": "user", "content": "Explain the Pythagorean theorem."},
            ],
            "max_completion_tokens": 1000,
        },
    },
    {
        "custom_id": "request-3",
        "method": "POST",
        "url": "/v1/chat/completions",
        "body": {
            "model": "klusterai/Meta-Llama-3.1-8B-Instruct-Turbo",
            "messages": [
                {
                    "role": "system",
                    "content": "You are a multilingual, experienced maths tutor.",
                },
                {
                    "role": "user",
                    "content": "Explain the Pythagorean theorem in Spanish",
                },
            ],
            "max_completion_tokens": 1000,
        },
    },
    # Additional tasks can be added here
]

# Save tasks to a JSONL file (newline-delimited JSON)
file_name = "my_batch_request.jsonl"
with open(file_name, "w") as file:
    for request in requests:
        file.write(json.dumps(request) + "\n")

# Upload batch job file
batch_input_file = client.files.create(
        file=open(file_name, "rb"),
        purpose="batch"
)

# Submit batch job
batch_request = client.batches.create(
    input_file_id=batch_input_file.id,
    endpoint="/v1/chat/completions",
    completion_window="24h",
)

# Poll the batch status until it's complete
while True:
    batch_status = client.batches.retrieve(batch_request.id)
    print(f"Batch status: {batch_status.status}")
    print(
        f"Completed tasks: {batch_status.request_counts.completed} / {batch_status.request_counts.total}"
    )

    if batch_status.status.lower() in ["completed", "failed", "cancelled"]:
        break

    time.sleep(10)  # Wait for 10 seconds before checking again

# Check if the Batch completed successfully
if batch_status.status.lower() == "completed":
    # Retrieve the results and log
    result_file_id = batch_status.output_file_id
    results = client.files.content(result_file_id).content

    # Print response to console
    print(f"\n🔍 AI batch response:")
    print(results)
else:
    print(f"Batch failed with status: {batch_status.status}")
    ```

??? example "Meta Llama 3.3 70B"

    ```python
    # Batch completions with the Meta Llama 3.3 70B model on kluster.ai

from os import environ
from openai import OpenAI
from getpass import getpass
import json
import time

# Get API key from user input
api_key = environ.get("API_KEY") or getpass("Enter your kluster.ai API key: ")

# Initialize OpenAI client pointing to kluster.ai API
client = OpenAI(
    base_url="https://api.kluster.ai/v1",
    api_key=api_key,
)

print(f"📤 Sending batch request to kluster.ai...\n")

# Create request with specified structure
requests = [
    {
        "custom_id": "request-1",
        "method": "POST",
        "url": "/v1/chat/completions",
        "body": {
            "model": "klusterai/Meta-Llama-3.3-70B-Instruct-Turbo",
            "messages": [
                {"role": "system", "content": "You are an experienced cook."},
                {"role": "user", "content": "What is the ultimate breakfast sandwich?"},
            ],
            "max_completion_tokens": 1000,
        },
    },
    {
        "custom_id": "request-2",
        "method": "POST",
        "url": "/v1/chat/completions",
        "body": {
            "model": "klusterai/Meta-Llama-3.3-70B-Instruct-Turbo",
            "messages": [
                {"role": "system", "content": "You are a maths tutor."},
                {"role": "user", "content": "Explain the Pythagorean theorem."},
            ],
            "max_completion_tokens": 1000,
        },
    },
    {
        "custom_id": "request-3",
        "method": "POST",
        "url": "/v1/chat/completions",
        "body": {
            "model": "klusterai/Meta-Llama-3.3-70B-Instruct-Turbo",
            "messages": [
                {
                    "role": "system",
                    "content": "You are a multilingual, experienced maths tutor.",
                },
                {
                    "role": "user",
                    "content": "Explain the Pythagorean theorem in Spanish",
                },
            ],
            "max_completion_tokens": 1000,
        },
    },
    # Additional tasks can be added here
]

# Save tasks to a JSONL file (newline-delimited JSON)
file_name = "my_batch_request.jsonl"
with open(file_name, "w") as file:
    for request in requests:
        file.write(json.dumps(request) + "\n")

# Upload batch job file
batch_input_file = client.files.create(
        file=open(file_name, "rb"),
        purpose="batch"
)

# Submit batch job
batch_request = client.batches.create(
    input_file_id=batch_input_file.id,
    endpoint="/v1/chat/completions",
    completion_window="24h",
)

# Poll the batch status until it's complete
while True:
    batch_status = client.batches.retrieve(batch_request.id)
    print(f"Batch status: {batch_status.status}")
    print(
        f"Completed tasks: {batch_status.request_counts.completed} / {batch_status.request_counts.total}"
    )

    if batch_status.status.lower() in ["completed", "failed", "cancelled"]:
        break

    time.sleep(10)  # Wait for 10 seconds before checking again

# Check if the Batch completed successfully
if batch_status.status.lower() == "completed":
    # Retrieve the results and log
    result_file_id = batch_status.output_file_id
    results = client.files.content(result_file_id).content

    # Print response to console
    print(f"\n🔍 AI batch response:")
    print(results)
else:
    print(f"Batch failed with status: {batch_status.status}")
    ```

??? example "Meta Llama 4 Maverick"

    ```python
    # Batch completions with the Meta Llama 4 Maverick model on kluster.ai

from os import environ
import json
import time
from getpass import getpass

from openai import OpenAI

# Newton's cradle
image1_url="https://github.com/kluster-ai/klusterai-cookbook/blob/main/images/balls-image.jpeg?raw=true"
# Text with typos
image2_url="https://github.com/kluster-ai/klusterai-cookbook/blob/main/images/text-typo-image.jpeg?raw=true"
# Parking sign
image3_url="https://github.com/kluster-ai/klusterai-cookbook/blob/main/images/parking-image.jpeg?raw=true"

# Get API key from user input
api_key = environ.get("API_KEY") or getpass("Enter your kluster.ai API key: ")

# Initialize OpenAI client pointing to kluster.ai API
client = OpenAI(
    base_url="https://api.kluster.ai/v1",
    api_key=api_key,
)

print(f"📤 Sending batch request to kluster.ai...\n")

# Create request with specified structure
requests = [
    {
        "custom_id": "request-1",
        "method": "POST",
        "url": "/v1/chat/completions",
        "body": {
            "model": "meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8",
            "messages": [
                {
                    "role": "user",
                    "content": [
                        {"type": "text", "text": "What is this?"},
                        {
                            "type": "image_url",
                            "image_url": {
                                "url": image1_url
                            },
                        },
                    ],
                }
            ],
            "max_completion_tokens": 1000,
        },
    },
    {
        "custom_id": "request-2",
        "method": "POST",
        "url": "/v1/chat/completions",
        "body": {
            "model": "meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8",
            "messages": [
                {
                    "role": "user",
                    "content": [
                        {"type": "text", "text": "Extract the text, find typos if any."},
                        {
                            "type": "image_url",
                            "image_url": {
                                "url": image2_url
                            },
                        },
                    ],
                }
            ],
            "max_completion_tokens": 1000,
        },
    },
    {
        "custom_id": "request-3",
        "method": "POST",
        "url": "/v1/chat/completions",
        "body": {
            "model": "meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8",
            "messages": [
                {
                    "role": "user",
                    "content": [
                        {"type": "text", "text": "Who can park in the area?"},
                        {
                            "type": "image_url",
                            "image_url": {
                                "url": image3_url
                            },
                        },
                    ],
                }
            ],
            "max_completion_tokens": 1000,
        },
    },
]

# Save tasks to a JSONL file (newline-delimited JSON)
file_name = "my_batch_request.jsonl"
with open(file_name, "w") as file:
    for request in requests:
        file.write(json.dumps(request) + "\n")

# Upload batch job file
batch_input_file = client.files.create(file=open(file_name, "rb"), purpose="batch")

# Submit batch job
batch_request = client.batches.create(
    input_file_id=batch_input_file.id,
    endpoint="/v1/chat/completions",
    completion_window="24h",
)

# Poll the batch status until it's complete
while True:
    batch_status = client.batches.retrieve(batch_request.id)
    print(f"Batch status: {batch_status.status}")
    print(
        f"Completed tasks: {batch_status.request_counts.completed} / {batch_status.request_counts.total}"
    )

    if batch_status.status.lower() in ["completed", "failed", "cancelled"]:
        break

    time.sleep(10)  # Wait for 10 seconds before checking again

print(f"\nImage1 URL: {image1_url}")
print(f"\nImage2 URL: {image2_url}")
print(f"\nImage3 URL: {image3_url}")

# Check if the Batch completed successfully
if batch_status.status.lower() == "completed":
    # Retrieve the results and log
    result_file_id = batch_status.output_file_id
    results = client.files.content(result_file_id).content

    # Print response to console
    print(f"\n🔍 AI batch response:")
    print(results)
else:
    print(f"Batch failed with status: {batch_status.status}")
    print(batch_status)
    ```

??? example "Meta Llama 4 Scout"

    ```python
    # Batch completions with the Meta Llama 4 Scout model on kluster.ai

from os import environ
import json
import time
from getpass import getpass

from openai import OpenAI

# Newton's cradle
image1_url="https://github.com/kluster-ai/klusterai-cookbook/blob/main/images/balls-image.jpeg?raw=true"
# Text with typos
image2_url="https://github.com/kluster-ai/klusterai-cookbook/blob/main/images/text-typo-image.jpeg?raw=true"
# Parking sign
image3_url="https://github.com/kluster-ai/klusterai-cookbook/blob/main/images/parking-image.jpeg?raw=true"

# Get API key from user input
api_key = environ.get("API_KEY") or getpass("Enter your kluster.ai API key: ")

# Initialize OpenAI client pointing to kluster.ai API
client = OpenAI(
    base_url="https://api.kluster.ai/v1",
    api_key=api_key,
)

print(f"📤 Sending batch request to kluster.ai...\n")

# Create request with specified structure
requests = [
    {
        "custom_id": "request-1",
        "method": "POST",
        "url": "/v1/chat/completions",
        "body": {
            "model": "meta-llama/Llama-4-Scout-17B-16E-Instruct",
            "messages": [
                {
                    "role": "user",
                    "content": [
                        {"type": "text", "text": "What is this?"},
                        {
                            "type": "image_url",
                            "image_url": {
                                "url": image1_url
                            },
                        },
                    ],
                }
            ],
            "max_completion_tokens": 1000,
        },
    },
    {
        "custom_id": "request-2",
        "method": "POST",
        "url": "/v1/chat/completions",
        "body": {
            "model": "meta-llama/Llama-4-Scout-17B-16E-Instruct",
            "messages": [
                {
                    "role": "user",
                    "content": [
                        {"type": "text", "text": "Extract the text, find typos if any."},
                        {
                            "type": "image_url",
                            "image_url": {
                                "url": image2_url
                            },
                        },
                    ],
                }
            ],
            "max_completion_tokens": 1000,
        },
    },
    {
        "custom_id": "request-3",
        "method": "POST",
        "url": "/v1/chat/completions",
        "body": {
            "model": "meta-llama/Llama-4-Scout-17B-16E-Instruct",
            "messages": [
                {
                    "role": "user",
                    "content": [
                        {"type": "text", "text": "Who can park in the area?"},
                        {
                            "type": "image_url",
                            "image_url": {
                                "url": image3_url
                            },
                        },
                    ],
                }
            ],
            "max_completion_tokens": 1000,
        },
    },
]

# Save tasks to a JSONL file (newline-delimited JSON)
file_name = "my_batch_request.jsonl"
with open(file_name, "w") as file:
    for request in requests:
        file.write(json.dumps(request) + "\n")

# Upload batch job file
batch_input_file = client.files.create(file=open(file_name, "rb"), purpose="batch")

# Submit batch job
batch_request = client.batches.create(
    input_file_id=batch_input_file.id,
    endpoint="/v1/chat/completions",
    completion_window="24h",
)

# Poll the batch status until it's complete
while True:
    batch_status = client.batches.retrieve(batch_request.id)
    print(f"Batch status: {batch_status.status}")
    print(
        f"Completed tasks: {batch_status.request_counts.completed} / {batch_status.request_counts.total}"
    )

    if batch_status.status.lower() in ["completed", "failed", "cancelled"]:
        break

    time.sleep(10)  # Wait for 10 seconds before checking again

print(f"\nImage1 URL: {image1_url}")
print(f"\nImage2 URL: {image2_url}")
print(f"\nImage3 URL: {image3_url}")

# Check if the Batch completed successfully
if batch_status.status.lower() == "completed":
    # Retrieve the results and log
    result_file_id = batch_status.output_file_id
    results = client.files.content(result_file_id).content

    # Print response to console
    print(f"\n🔍 AI batch response:")
    print(results)
else:
    print(f"Batch failed with status: {batch_status.status}")
    print(batch_status)
    ```

??? example "Mistral NeMo"

    ```python
    # Batch completions with the Mistral NeMo model on kluster.ai

from os import environ
from openai import OpenAI
from getpass import getpass
import json
import time

# Get API key from user input
api_key = environ.get("API_KEY") or getpass("Enter your kluster.ai API key: ")

# Initialize OpenAI client pointing to kluster.ai API
client = OpenAI(
    base_url="https://api.kluster.ai/v1",
    api_key=api_key,
)

print(f"📤 Sending batch request to kluster.ai...\n")

# Create request with specified structure
requests = [
    {
        "custom_id": "request-1",
        "method": "POST",
        "url": "/v1/chat/completions",
        "body": {
            "model": "mistralai/Mistral-Nemo-Instruct-2407",
            "messages": [
                {"role": "system", "content": "You are an experienced cook."},
                {"role": "user", "content": "What is the ultimate breakfast sandwich?"},
            ],
            "max_completion_tokens": 1000,
        },
    },
    {
        "custom_id": "request-2",
        "method": "POST",
        "url": "/v1/chat/completions",
        "body": {
            "model": "mistralai/Mistral-Nemo-Instruct-2407",
            "messages": [
                {"role": "system", "content": "You are a maths tutor."},
                {"role": "user", "content": "Explain the Pythagorean theorem."},
            ],
            "max_completion_tokens": 1000,
        },
    },
    {
        "custom_id": "request-3",
        "method": "POST",
        "url": "/v1/chat/completions",
        "body": {
            "model": "mistralai/Mistral-Nemo-Instruct-2407",
            "messages": [
                {
                    "role": "system",
                    "content": "You are a multilingual, experienced maths tutor.",
                },
                {
                    "role": "user",
                    "content": "Explain the Pythagorean theorem in Spanish",
                },
            ],
            "max_completion_tokens": 1000,
        },
    },
    # Additional tasks can be added here
]

# Save tasks to a JSONL file (newline-delimited JSON)
file_name = "my_batch_request.jsonl"
with open(file_name, "w") as file:
    for request in requests:
        file.write(json.dumps(request) + "\n")

# Upload batch job file
batch_input_file = client.files.create(
        file=open(file_name, "rb"),
        purpose="batch"
)

# Submit batch job
batch_request = client.batches.create(
    input_file_id=batch_input_file.id,
    endpoint="/v1/chat/completions",
    completion_window="24h",
)

# Poll the batch status until it's complete
while True:
    batch_status = client.batches.retrieve(batch_request.id)
    print(f"Batch status: {batch_status.status}")
    print(
        f"Completed tasks: {batch_status.request_counts.completed} / {batch_status.request_counts.total}"
    )

    if batch_status.status.lower() in ["completed", "failed", "cancelled"]:
        break

    time.sleep(10)  # Wait for 10 seconds before checking again

# Check if the Batch completed successfully
if batch_status.status.lower() == "completed":
    # Retrieve the results and log
    result_file_id = batch_status.output_file_id
    results = client.files.content(result_file_id).content

    # Print response to console
    print(f"\n🔍 AI batch response:")
    print(results)
else:
    print(f"Batch failed with status: {batch_status.status}")
    ```

??? example "Mistral Small"

    ```python
    # Batch completions with the Mistral Small model on kluster.ai

from os import environ
from openai import OpenAI
from getpass import getpass
import json
import time

# Get API key from user input
api_key = environ.get("API_KEY") or getpass("Enter your kluster.ai API key: ")

# Initialize OpenAI client pointing to kluster.ai API
client = OpenAI(
    base_url="https://api.kluster.ai/v1",
    api_key=api_key,
)

print(f"📤 Sending batch request to kluster.ai...\n")

# Create request with specified structure
requests = [
    {
        "custom_id": "request-1",
        "method": "POST",
        "url": "/v1/chat/completions",
        "body": {
            "model": "mistralai/Mistral-Small-24B-Instruct-2501",
            "messages": [
                {"role": "system", "content": "You are an experienced cook."},
                {"role": "user", "content": "What is the ultimate breakfast sandwich?"},
            ],
            "max_completion_tokens": 1000,
        },
    },
    {
        "custom_id": "request-2",
        "method": "POST",
        "url": "/v1/chat/completions",
        "body": {
            "model": "mistralai/Mistral-Small-24B-Instruct-2501",
            "messages": [
                {"role": "system", "content": "You are a maths tutor."},
                {"role": "user", "content": "Explain the Pythagorean theorem."},
            ],
            "max_completion_tokens": 1000,
        },
    },
    {
        "custom_id": "request-3",
        "method": "POST",
        "url": "/v1/chat/completions",
        "body": {
            "model": "mistralai/Mistral-Small-24B-Instruct-2501",
            "messages": [
                {
                    "role": "system",
                    "content": "You are a multilingual, experienced maths tutor.",
                },
                {
                    "role": "user",
                    "content": "Explain the Pythagorean theorem in Spanish",
                },
            ],
            "max_completion_tokens": 1000,
        },
    },
    # Additional tasks can be added here
]

# Save tasks to a JSONL file (newline-delimited JSON)
file_name = "my_batch_request.jsonl"
with open(file_name, "w") as file:
    for request in requests:
        file.write(json.dumps(request) + "\n")

# Upload batch job file
batch_input_file = client.files.create(
        file=open(file_name, "rb"),
        purpose="batch"
)

# Submit batch job
batch_request = client.batches.create(
    input_file_id=batch_input_file.id,
    endpoint="/v1/chat/completions",
    completion_window="24h",
)

# Poll the batch status until it's complete
while True:
    batch_status = client.batches.retrieve(batch_request.id)
    print(f"Batch status: {batch_status.status}")
    print(
        f"Completed tasks: {batch_status.request_counts.completed} / {batch_status.request_counts.total}"
    )

    if batch_status.status.lower() in ["completed", "failed", "cancelled"]:
        break

    time.sleep(10)  # Wait for 10 seconds before checking again

# Check if the Batch completed successfully
if batch_status.status.lower() == "completed":
    # Retrieve the results and log
    result_file_id = batch_status.output_file_id
    results = client.files.content(result_file_id).content

    # Print response to console
    print(f"\n🔍 AI batch response:")
    print(results)
else:
    print(f"Batch failed with status: {batch_status.status}")
    ```

??? example "Qwen2.5-VL 7B"

    ```python
    # Batch completions with the Qwen2.5-VL 7B model on kluster.ai

from os import environ
import json
import time
from getpass import getpass

from openai import OpenAI

# Newton's cradle
image1_url="https://github.com/kluster-ai/klusterai-cookbook/blob/main/images/balls-image.jpeg?raw=true"
# Text with typos
image2_url="https://github.com/kluster-ai/klusterai-cookbook/blob/main/images/text-typo-image.jpeg?raw=true"
# Parking sign
image3_url="https://github.com/kluster-ai/klusterai-cookbook/blob/main/images/parking-image.jpeg?raw=true"

# Get API key from user input
api_key = environ.get("API_KEY") or getpass("Enter your kluster.ai API key: ")

# Initialize OpenAI client pointing to kluster.ai API
client = OpenAI(
    base_url="https://api.kluster.ai/v1",
    api_key=api_key,
)

print(f"📤 Sending batch request to kluster.ai...\n")

# Create request with specified structure
requests = [
    {
        "custom_id": "request-1",
        "method": "POST",
        "url": "/v1/chat/completions",
        "body": {
            "model": "Qwen/Qwen2.5-VL-7B-Instruct",
            "messages": [
                {
                    "role": "user",
                    "content": [
                        {"type": "text", "text": "What is this?"},
                        {
                            "type": "image_url",
                            "image_url": {
                                "url": image1_url
                            },
                        },
                    ],
                }
            ],
            "max_completion_tokens": 1000,
        },
    },
    {
        "custom_id": "request-2",
        "method": "POST",
        "url": "/v1/chat/completions",
        "body": {
            "model": "Qwen/Qwen2.5-VL-7B-Instruct",
            "messages": [
                {
                    "role": "user",
                    "content": [
                        {"type": "text", "text": "Extract the text, find typos if any."},
                        {
                            "type": "image_url",
                            "image_url": {
                                "url": image2_url
                            },
                        },
                    ],
                }
            ],
            "max_completion_tokens": 1000,
        },
    },
    {
        "custom_id": "request-3",
        "method": "POST",
        "url": "/v1/chat/completions",
        "body": {
            "model": "Qwen/Qwen2.5-VL-7B-Instruct",
            "messages": [
                {
                    "role": "user",
                    "content": [
                        {"type": "text", "text": "Who can park in the area?"},
                        {
                            "type": "image_url",
                            "image_url": {
                                "url": image3_url
                            },
                        },
                    ],
                }
            ],
            "max_completion_tokens": 1000,
        },
    },
]

# Save tasks to a JSONL file (newline-delimited JSON)
file_name = "my_batch_request.jsonl"
with open(file_name, "w") as file:
    for request in requests:
        file.write(json.dumps(request) + "\n")

# Upload batch job file
batch_input_file = client.files.create(file=open(file_name, "rb"), purpose="batch")

# Submit batch job
batch_request = client.batches.create(
    input_file_id=batch_input_file.id,
    endpoint="/v1/chat/completions",
    completion_window="24h",
)

# Poll the batch status until it's complete
while True:
    batch_status = client.batches.retrieve(batch_request.id)
    print(f"Batch status: {batch_status.status}")
    print(
        f"Completed tasks: {batch_status.request_counts.completed} / {batch_status.request_counts.total}"
    )

    if batch_status.status.lower() in ["completed", "failed", "cancelled"]:
        break

    time.sleep(10)  # Wait for 10 seconds before checking again

print(f"\nImage1 URL: {image1_url}")
print(f"\nImage2 URL: {image2_url}")
print(f"\nImage3 URL: {image3_url}")

# Check if the Batch completed successfully
if batch_status.status.lower() == "completed":
    # Retrieve the results and log
    result_file_id = batch_status.output_file_id
    results = client.files.content(result_file_id).content

    # Print response to console
    print(f"\n🔍 AI batch response:")
    print(results)
else:
    print(f"Batch failed with status: {batch_status.status}")
    print(batch_status)
    ```

??? example "Qwen3-235B-A22B"

    ```python
    # Batch completions with the Qwen3-235B-A22B model on kluster.ai

from os import environ
from openai import OpenAI
from getpass import getpass
import json
import time

# Get API key from user input
api_key = environ.get("API_KEY") or getpass("Enter your kluster.ai API key: ")

# Initialize OpenAI client pointing to kluster.ai API
client = OpenAI(
    base_url="https://api.kluster.ai/v1",
    api_key=api_key,
)

print(f"📤 Sending batch request to kluster.ai...\n")

# Create request with specified structure
requests = [
    {
        "custom_id": "request-1",
        "method": "POST",
        "url": "/v1/chat/completions",
        "body": {
            "model": "Qwen/Qwen3-235B-A22B-FP8",
            "messages": [
                {"role": "system", "content": "You are an experienced cook."},
                {"role": "user", "content": "What is the ultimate breakfast sandwich?"},
            ],
            "max_completion_tokens": 1000,
        },
    },
    {
        "custom_id": "request-2",
        "method": "POST",
        "url": "/v1/chat/completions",
        "body": {
            "model": "Qwen/Qwen3-235B-A22B-FP8",
            "messages": [
                {"role": "system", "content": "You are a maths tutor."},
                {"role": "user", "content": "Explain the Pythagorean theorem."},
            ],
            "max_completion_tokens": 1000,
        },
    },
    {
        "custom_id": "request-3",
        "method": "POST",
        "url": "/v1/chat/completions",
        "body": {
            "model": "Qwen/Qwen3-235B-A22B-FP8",
            "messages": [
                {
                    "role": "system",
                    "content": "You are a multilingual, experienced maths tutor.",
                },
                {
                    "role": "user",
                    "content": "Explain the Pythagorean theorem in Spanish",
                },
            ],
            "max_completion_tokens": 1000,
        },
    },
    # Additional tasks can be added here
]

# Save tasks to a JSONL file (newline-delimited JSON)
file_name = "my_batch_request.jsonl"
with open(file_name, "w") as file:
    for request in requests:
        file.write(json.dumps(request) + "\n")

# Upload batch job file
batch_input_file = client.files.create(
        file=open(file_name, "rb"),
        purpose="batch"
)

# Submit batch job
batch_request = client.batches.create(
    input_file_id=batch_input_file.id,
    endpoint="/v1/chat/completions",
    completion_window="24h",
)

# Poll the batch status until it's complete
while True:
    batch_status = client.batches.retrieve(batch_request.id)
    print(f"Batch status: {batch_status.status}")
    print(
        f"Completed tasks: {batch_status.request_counts.completed} / {batch_status.request_counts.total}"
    )

    if batch_status.status.lower() in ["completed", "failed", "cancelled"]:
        break

    time.sleep(10)  # Wait for 10 seconds before checking again

# Check if the Batch completed successfully
if batch_status.status.lower() == "completed":
    # Retrieve the results and log
    result_file_id = batch_status.output_file_id
    results = client.files.content(result_file_id).content

    # Print response to console
    print(f"\n🔍 AI batch response:")
    print(results)
else:
    print(f"Batch failed with status: {batch_status.status}")
    ```
### CLI

Similarly, the following curl commands showcase how to easily send a chat completion request to kluster.ai for the different supported models. This example assumes you've exported your kluster.ai API key as the variable `API_KEY`.

??? example "DeepSeek-R1"

    ```bash
    #!/bin/bash

# Check if API_KEY is set and not empty
if [[ -z "$API_KEY" ]]; then
    echo "Error: API_KEY environment variable is not set." >&2
    exit 1
fi

echo -e "📤 Sending batch request to kluster.ai...
"

# Create request with specified structure
cat << EOF > my_batch_request.jsonl
{"custom_id": "request-1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "deepseek-ai/DeepSeek-R1", "messages": [{"role": "system", "content": "You are an experienced cook."}, {"role": "user", "content": "What is the ultimate breakfast sandwich?"}],"max_completion_tokens":1000}}
{"custom_id": "request-2", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "deepseek-ai/DeepSeek-R1", "messages": [{"role": "system", "content": "You are an experienced maths tutor."}, {"role": "user", "content": "Explain the Pythagorean theorem."}],"max_completion_tokens":1000}}
{"custom_id": "request-4", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "deepseek-ai/DeepSeek-R1", "messages":[{"role": "system", "content": "You are a multilingual, experienced maths tutor."}, {"role": "user", "content": "Explain the Pythagorean theorem in Spanish"}],"max_completion_tokens":1000}}
EOF

# Upload batch job file
FILE_ID=$(curl -s https://api.kluster.ai/v1/files \
    -H "Authorization: Bearer $API_KEY" \
    -H "Content-Type: multipart/form-data" \
    -F "file=@my_batch_request.jsonl" \
    -F "purpose=batch" | jq -r '.id')
echo "File uploaded, file ID: $FILE_ID"

# Submit batch job
BATCH_ID=$(curl -s https://api.kluster.ai/v1/batches \
    -H "Authorization: Bearer $API_KEY" \
    -H "Content-Type: application/json" \
    -d '{
        "input_file_id": "'$FILE_ID'",
        "endpoint": "/v1/chat/completions",
        "completion_window": "24h"
    }' | jq -r '.id')
echo "Batch job submitted, job ID: $BATCH_ID"


# Poll the batch status until it's completed
STATUS="in_progress"
while [[ "$STATUS" != "completed" ]]; do
    echo "Waiting for batch job to complete... Status: $STATUS"
    sleep 10 # Wait for 10 seconds before checking again

    STATUS=$(curl -s https://api.kluster.ai/v1/batches/$BATCH_ID \
        -H "Authorization: Bearer $API_KEY" \
        -H "Content-Type: application/json" | jq -r '.status')
done

# Retrieve the batch output file
kluster_OUTPUT_FILE=$(curl -s https://api.kluster.ai/v1/batches/$BATCH_ID \
    -H "Authorization: Bearer $API_KEY" \
    -H "Content-Type: application/json" | jq -r '.output_file_id')

# Retrieve the results
OUTPUT_CONTENT=$(curl -s https://api.kluster.ai/v1/files/$kluster_OUTPUT_FILE/content \
    -H "Authorization: Bearer $API_KEY")

# Log results
echo -e "\n🔍 AI batch response:"
echo "$OUTPUT_CONTENT"
    ```

??? example "DeepSeek-R1-0528"

    ```bash
    #!/bin/bash

# Check if API_KEY is set and not empty
if [[ -z "$API_KEY" ]]; then
    echo "Error: API_KEY environment variable is not set." >&2
    exit 1
fi

echo -e "📤 Sending batch request to kluster.ai...
"

# Create request with specified structure
cat << EOF > my_batch_request.jsonl
{"custom_id": "request-1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "deepseek-ai/DeepSeek-R1-0528", "messages": [{"role": "system", "content": "You are an experienced cook."}, {"role": "user", "content": "What is the ultimate breakfast sandwich?"}],"max_completion_tokens":1000}}
{"custom_id": "request-2", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "deepseek-ai/DeepSeek-R1-0528", "messages": [{"role": "system", "content": "You are an experienced maths tutor."}, {"role": "user", "content": "Explain the Pythagorean theorem."}],"max_completion_tokens":1000}}
{"custom_id": "request-4", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "deepseek-ai/DeepSeek-R1-0528", "messages":[{"role": "system", "content": "You are a multilingual, experienced maths tutor."}, {"role": "user", "content": "Explain the Pythagorean theorem in Spanish"}],"max_completion_tokens":1000}}
EOF

# Upload batch job file
FILE_ID=$(curl -s https://api.kluster.ai/v1/files \
    -H "Authorization: Bearer $API_KEY" \
    -H "Content-Type: multipart/form-data" \
    -F "file=@my_batch_request.jsonl" \
    -F "purpose=batch" | jq -r '.id')
echo "File uploaded, file ID: $FILE_ID"

# Submit batch job
BATCH_ID=$(curl -s https://api.kluster.ai/v1/batches \
    -H "Authorization: Bearer $API_KEY" \
    -H "Content-Type: application/json" \
    -d '{
        "input_file_id": "'$FILE_ID'",
        "endpoint": "/v1/chat/completions",
        "completion_window": "24h"
    }' | jq -r '.id')
echo "Batch job submitted, job ID: $BATCH_ID"


# Poll the batch status until it's completed
STATUS="in_progress"
while [[ "$STATUS" != "completed" ]]; do
    echo "Waiting for batch job to complete... Status: $STATUS"
    sleep 10 # Wait for 10 seconds before checking again

    STATUS=$(curl -s https://api.kluster.ai/v1/batches/$BATCH_ID \
        -H "Authorization: Bearer $API_KEY" \
        -H "Content-Type: application/json" | jq -r '.status')
done

# Retrieve the batch output file
kluster_OUTPUT_FILE=$(curl -s https://api.kluster.ai/v1/batches/$BATCH_ID \
    -H "Authorization: Bearer $API_KEY" \
    -H "Content-Type: application/json" | jq -r '.output_file_id')

# Retrieve the results
OUTPUT_CONTENT=$(curl -s https://api.kluster.ai/v1/files/$kluster_OUTPUT_FILE/content \
    -H "Authorization: Bearer $API_KEY")

# Log results
echo -e "\n🔍 AI batch response:"
echo "$OUTPUT_CONTENT"
    ```

??? example "DeepSeek-V3-0324"

    ```bash
    #!/bin/bash

# Check if API_KEY is set and not empty
if [[ -z "$API_KEY" ]]; then
    echo "Error: API_KEY environment variable is not set." >&2
    exit 1
fi

echo -e "📤 Sending batch request to kluster.ai...
"

# Create request with specified structure
cat << EOF > my_batch_request.jsonl
{"custom_id": "request-1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "deepseek-ai/DeepSeek-V3-0324", "messages": [{"role": "system", "content": "You are an experienced cook."}, {"role": "user", "content": "What is the ultimate breakfast sandwich?"}],"max_completion_tokens":1000}}
{"custom_id": "request-2", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "deepseek-ai/DeepSeek-V3-0324", "messages": [{"role": "system", "content": "You are an experienced maths tutor."}, {"role": "user", "content": "Explain the Pythagorean theorem."}],"max_completion_tokens":1000}}
{"custom_id": "request-4", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "deepseek-ai/DeepSeek-V3-0324", "messages":[{"role": "system", "content": "You are a multilingual, experienced maths tutor."}, {"role": "user", "content": "Explain the Pythagorean theorem in Spanish"}],"max_completion_tokens":1000}}
EOF

# Upload batch job file
FILE_ID=$(curl -s https://api.kluster.ai/v1/files \
    -H "Authorization: Bearer $API_KEY" \
    -H "Content-Type: multipart/form-data" \
    -F "file=@my_batch_request.jsonl" \
    -F "purpose=batch" | jq -r '.id')
echo "File uploaded, file ID: $FILE_ID"

# Submit batch job
BATCH_ID=$(curl -s https://api.kluster.ai/v1/batches \
    -H "Authorization: Bearer $API_KEY" \
    -H "Content-Type: application/json" \
    -d '{
        "input_file_id": "'$FILE_ID'",
        "endpoint": "/v1/chat/completions",
        "completion_window": "24h"
    }' | jq -r '.id')
echo "Batch job submitted, job ID: $BATCH_ID"


# Poll the batch status until it's completed
STATUS="in_progress"
while [[ "$STATUS" != "completed" ]]; do
    echo "Waiting for batch job to complete... Status: $STATUS"
    sleep 10 # Wait for 10 seconds before checking again

    STATUS=$(curl -s https://api.kluster.ai/v1/batches/$BATCH_ID \
        -H "Authorization: Bearer $API_KEY" \
        -H "Content-Type: application/json" | jq -r '.status')
done

# Retrieve the batch output file
kluster_OUTPUT_FILE=$(curl -s https://api.kluster.ai/v1/batches/$BATCH_ID \
    -H "Authorization: Bearer $API_KEY" \
    -H "Content-Type: application/json" | jq -r '.output_file_id')

# Retrieve the results
OUTPUT_CONTENT=$(curl -s https://api.kluster.ai/v1/files/$kluster_OUTPUT_FILE/content \
    -H "Authorization: Bearer $API_KEY")

# Log results
echo -e "\n🔍 AI batch response:"
echo "$OUTPUT_CONTENT"
    ```

??? example "Gemma 3 27B"

    ```bash
    #!/bin/bash

# Check if API_KEY is set and not empty
if [[ -z "$API_KEY" ]]; then
    echo "Error: API_KEY environment variable is not set." >&2
    exit 1
fi

echo -e "📤 Sending batch request to kluster.ai...\n"

# Define image URLs
# Newton's cradle
image1_url="https://github.com/kluster-ai/klusterai-cookbook/blob/main/images/balls-image.jpeg?raw=true"
# Text with typos
image2_url="https://github.com/kluster-ai/klusterai-cookbook/blob/main/images/text-typo-image.jpeg?raw=true"
# Parking sign
image3_url="https://github.com/kluster-ai/klusterai-cookbook/blob/main/images/parking-image.jpeg?raw=true"

# Create request with specified structure
cat << EOF > my_batch_request.jsonl
{"custom_id": "request-1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "google/gemma-3-27b-it", "messages": [{"role": "user", "content": [{"type": "text", "text": "What is this?"}, {"type": "image_url", "image_url": {"url": "$image1_url"}}]}],"max_completion_tokens": 1000}}
{"custom_id": "request-2", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "google/gemma-3-27b-it", "messages": [{"role": "user", "content": [{"type": "text", "text": "Extract the text, find typos if any."}, {"type": "image_url", "image_url": {"url": "$image2_url"}}]}],"max_completion_tokens": 1000}}
{"custom_id": "request-3", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "google/gemma-3-27b-it", "messages": [{"role": "user", "content": [{"type": "text", "text": "Who can park in the area?"}, {"type": "image_url", "image_url": {"url": "$image3_url"}}]}],"max_completion_tokens": 1000}}
EOF

# Upload batch job file
FILE_ID=$(curl -s https://api.kluster.ai/v1/files \
    -H "Authorization: Bearer $API_KEY" \
    -H "Content-Type: multipart/form-data" \
    -F "file=@my_batch_request.jsonl" \
    -F "purpose=batch" | jq -r '.id')
echo "File uploaded, file ID: $FILE_ID"

# Submit batch job
BATCH_ID=$(curl -s https://api.kluster.ai/v1/batches \
    -H "Authorization: Bearer $API_KEY" \
    -H "Content-Type: application/json" \
    -d '{
        "input_file_id": "'$FILE_ID'",
        "endpoint": "/v1/chat/completions",
        "completion_window": "24h"
    }' | jq -r '.id')
echo "Batch job submitted, job ID: $BATCH_ID"

# Poll the batch status until it's completed
STATUS="in_progress"
while [[ "$STATUS" != "completed" ]]; do
    echo "Waiting for batch job to complete... Status: $STATUS"
    sleep 10 # Wait for 10 seconds before checking again

    STATUS=$(curl -s https://api.kluster.ai/v1/batches/$BATCH_ID \
        -H "Authorization: Bearer $API_KEY" \
        -H "Content-Type: application/json" | jq -r '.status')
done

# Retrieve the batch output file
kluster_OUTPUT_FILE=$(curl -s https://api.kluster.ai/v1/batches/$BATCH_ID \
    -H "Authorization: Bearer $API_KEY" \
    -H "Content-Type: application/json" | jq -r '.output_file_id')

# Retrieve the results
OUTPUT_CONTENT=$(curl -s https://api.kluster.ai/v1/files/$kluster_OUTPUT_FILE/content \
    -H "Authorization: Bearer $API_KEY")

# Log results
echo -e "\nImage1 URL: $image1_url"
echo -e "\nImage2 URL: $image2_url"
echo -e "\nImage3 URL: $image3_url"
echo -e "\n🔍 AI batch response:"
echo "$OUTPUT_CONTENT"
    ```

??? example "Magistral Small"

    ```bash
    #!/bin/bash

# Check if API_KEY is set and not empty
if [[ -z "$API_KEY" ]]; then
    echo "Error: API_KEY environment variable is not set." >&2
    exit 1
fi

echo -e "📤 Sending batch request to kluster.ai...
"

# Create request with specified structure
cat << EOF > my_batch_request.jsonl
{"custom_id": "request-1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "mistralai/Magistral-Small-2506", "messages": [{"role": "system", "content": "You are an experienced cook."}, {"role": "user", "content": "What is the ultimate breakfast sandwich?"}],"max_completion_tokens":1000}}
{"custom_id": "request-2", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "mistralai/Magistral-Small-2506", "messages": [{"role": "system", "content": "You are an experienced maths tutor."}, {"role": "user", "content": "Explain the Pythagorean theorem."}],"max_completion_tokens":1000}}
{"custom_id": "request-4", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "mistralai/Magistral-Small-2506", "messages":[{"role": "system", "content": "You are a multilingual, experienced maths tutor."}, {"role": "user", "content": "Explain the Pythagorean theorem in Spanish"}],"max_completion_tokens":1000}}
EOF

# Upload batch job file
FILE_ID=$(curl -s https://api.kluster.ai/v1/files \
    -H "Authorization: Bearer $API_KEY" \
    -H "Content-Type: multipart/form-data" \
    -F "file=@my_batch_request.jsonl" \
    -F "purpose=batch" | jq -r '.id')
echo "File uploaded, file ID: $FILE_ID"

# Submit batch job
BATCH_ID=$(curl -s https://api.kluster.ai/v1/batches \
    -H "Authorization: Bearer $API_KEY" \
    -H "Content-Type: application/json" \
    -d '{
        "input_file_id": "'$FILE_ID'",
        "endpoint": "/v1/chat/completions",
        "completion_window": "24h"
    }' | jq -r '.id')
echo "Batch job submitted, job ID: $BATCH_ID"


# Poll the batch status until it's completed
STATUS="in_progress"
while [[ "$STATUS" != "completed" ]]; do
    echo "Waiting for batch job to complete... Status: $STATUS"
    sleep 10 # Wait for 10 seconds before checking again

    STATUS=$(curl -s https://api.kluster.ai/v1/batches/$BATCH_ID \
        -H "Authorization: Bearer $API_KEY" \
        -H "Content-Type: application/json" | jq -r '.status')
done

# Retrieve the batch output file
kluster_OUTPUT_FILE=$(curl -s https://api.kluster.ai/v1/batches/$BATCH_ID \
    -H "Authorization: Bearer $API_KEY" \
    -H "Content-Type: application/json" | jq -r '.output_file_id')

# Retrieve the results
OUTPUT_CONTENT=$(curl -s https://api.kluster.ai/v1/files/$kluster_OUTPUT_FILE/content \
    -H "Authorization: Bearer $API_KEY")

# Log results
echo -e "\n🔍 AI batch response:"
echo "$OUTPUT_CONTENT"
    ```

??? example "Meta Llama 3.1 8B"

    ```bash
    #!/bin/bash

# Check if API_KEY is set and not empty
if [[ -z "$API_KEY" ]]; then
    echo "Error: API_KEY environment variable is not set." >&2
    exit 1
fi

echo -e "📤 Sending batch request to kluster.ai...
"

# Create request with specified structure
cat << EOF > my_batch_request.jsonl
{"custom_id": "request-1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "klusterai/Meta-Llama-3.1-8B-Instruct-Turbo", "messages": [{"role": "system", "content": "You are an experienced cook."}, {"role": "user", "content": "What is the ultimate breakfast sandwich?"}],"max_completion_tokens":1000}}
{"custom_id": "request-2", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "klusterai/Meta-Llama-3.1-8B-Instruct-Turbo", "messages": [{"role": "system", "content": "You are an experienced maths tutor."}, {"role": "user", "content": "Explain the Pythagorean theorem."}],"max_completion_tokens":1000}}
{"custom_id": "request-4", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "klusterai/Meta-Llama-3.1-8B-Instruct-Turbo", "messages":[{"role": "system", "content": "You are a multilingual, experienced maths tutor."}, {"role": "user", "content": "Explain the Pythagorean theorem in Spanish"}],"max_completion_tokens":1000}}
EOF

# Upload batch job file
FILE_ID=$(curl -s https://api.kluster.ai/v1/files \
    -H "Authorization: Bearer $API_KEY" \
    -H "Content-Type: multipart/form-data" \
    -F "file=@my_batch_request.jsonl" \
    -F "purpose=batch" | jq -r '.id')
echo "File uploaded, file ID: $FILE_ID"

# Submit batch job
BATCH_ID=$(curl -s https://api.kluster.ai/v1/batches \
    -H "Authorization: Bearer $API_KEY" \
    -H "Content-Type: application/json" \
    -d '{
        "input_file_id": "'$FILE_ID'",
        "endpoint": "/v1/chat/completions",
        "completion_window": "24h"
    }' | jq -r '.id')
echo "Batch job submitted, job ID: $BATCH_ID"


# Poll the batch status until it's completed
STATUS="in_progress"
while [[ "$STATUS" != "completed" ]]; do
    echo "Waiting for batch job to complete... Status: $STATUS"
    sleep 10 # Wait for 10 seconds before checking again

    STATUS=$(curl -s https://api.kluster.ai/v1/batches/$BATCH_ID \
        -H "Authorization: Bearer $API_KEY" \
        -H "Content-Type: application/json" | jq -r '.status')
done

# Retrieve the batch output file
kluster_OUTPUT_FILE=$(curl -s https://api.kluster.ai/v1/batches/$BATCH_ID \
    -H "Authorization: Bearer $API_KEY" \
    -H "Content-Type: application/json" | jq -r '.output_file_id')

# Retrieve the results
OUTPUT_CONTENT=$(curl -s https://api.kluster.ai/v1/files/$kluster_OUTPUT_FILE/content \
    -H "Authorization: Bearer $API_KEY")

# Log results
echo -e "\n🔍 AI batch response:"
echo "$OUTPUT_CONTENT"
    ```

??? example "Meta Llama 3.3 70B"

    ```bash
    #!/bin/bash

# Check if API_KEY is set and not empty
if [[ -z "$API_KEY" ]]; then
    echo "Error: API_KEY environment variable is not set." >&2
    exit 1
fi

echo -e "📤 Sending batch request to kluster.ai...
"

# Create request with specified structure
cat << EOF > my_batch_request.jsonl
{"custom_id": "request-1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "klusterai/Meta-Llama-3.3-70B-Instruct-Turbo", "messages": [{"role": "system", "content": "You are an experienced cook."}, {"role": "user", "content": "What is the ultimate breakfast sandwich?"}],"max_completion_tokens":1000}}
{"custom_id": "request-2", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "klusterai/Meta-Llama-3.3-70B-Instruct-Turbo", "messages": [{"role": "system", "content": "You are an experienced maths tutor."}, {"role": "user", "content": "Explain the Pythagorean theorem."}],"max_completion_tokens":1000}}
{"custom_id": "request-4", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "klusterai/Meta-Llama-3.3-70B-Instruct-Turbo", "messages":[{"role": "system", "content": "You are a multilingual, experienced maths tutor."}, {"role": "user", "content": "Explain the Pythagorean theorem in Spanish"}],"max_completion_tokens":1000}}
EOF

# Upload batch job file
FILE_ID=$(curl -s https://api.kluster.ai/v1/files \
    -H "Authorization: Bearer $API_KEY" \
    -H "Content-Type: multipart/form-data" \
    -F "file=@my_batch_request.jsonl" \
    -F "purpose=batch" | jq -r '.id')
echo "File uploaded, file ID: $FILE_ID"

# Submit batch job
BATCH_ID=$(curl -s https://api.kluster.ai/v1/batches \
    -H "Authorization: Bearer $API_KEY" \
    -H "Content-Type: application/json" \
    -d '{
        "input_file_id": "'$FILE_ID'",
        "endpoint": "/v1/chat/completions",
        "completion_window": "24h"
    }' | jq -r '.id')
echo "Batch job submitted, job ID: $BATCH_ID"


# Poll the batch status until it's completed
STATUS="in_progress"
while [[ "$STATUS" != "completed" ]]; do
    echo "Waiting for batch job to complete... Status: $STATUS"
    sleep 10 # Wait for 10 seconds before checking again

    STATUS=$(curl -s https://api.kluster.ai/v1/batches/$BATCH_ID \
        -H "Authorization: Bearer $API_KEY" \
        -H "Content-Type: application/json" | jq -r '.status')
done

# Retrieve the batch output file
kluster_OUTPUT_FILE=$(curl -s https://api.kluster.ai/v1/batches/$BATCH_ID \
    -H "Authorization: Bearer $API_KEY" \
    -H "Content-Type: application/json" | jq -r '.output_file_id')

# Retrieve the results
OUTPUT_CONTENT=$(curl -s https://api.kluster.ai/v1/files/$kluster_OUTPUT_FILE/content \
    -H "Authorization: Bearer $API_KEY")

# Log results
echo -e "\n🔍 AI batch response:"
echo "$OUTPUT_CONTENT"
    ```

??? example "Meta Llama 4 Maverick"

    ```bash
    #!/bin/bash

# Check if API_KEY is set and not empty
if [[ -z "$API_KEY" ]]; then
    echo "Error: API_KEY environment variable is not set." >&2
    exit 1
fi

echo -e "📤 Sending batch request to kluster.ai...\n"

# Define image URLs
# Newton's cradle
image1_url="https://github.com/kluster-ai/klusterai-cookbook/blob/main/images/balls-image.jpeg?raw=true"
# Text with typos
image2_url="https://github.com/kluster-ai/klusterai-cookbook/blob/main/images/text-typo-image.jpeg?raw=true"
# Parking sign
image3_url="https://github.com/kluster-ai/klusterai-cookbook/blob/main/images/parking-image.jpeg?raw=true"

# Create request with specified structure
cat << EOF > my_batch_request.jsonl
{"custom_id": "request-1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8", "messages": [{"role": "user", "content": [{"type": "text", "text": "What is this?"}, {"type": "image_url", "image_url": {"url": "$image1_url"}}]}],"max_completion_tokens": 1000}}
{"custom_id": "request-2", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8", "messages": [{"role": "user", "content": [{"type": "text", "text": "Extract the text, find typos if any."}, {"type": "image_url", "image_url": {"url": "$image2_url"}}]}],"max_completion_tokens": 1000}}
{"custom_id": "request-3", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8", "messages": [{"role": "user", "content": [{"type": "text", "text": "Who can park in the area?"}, {"type": "image_url", "image_url": {"url": "$image3_url"}}]}],"max_completion_tokens": 1000}}
EOF

# Upload batch job file
FILE_ID=$(curl -s https://api.kluster.ai/v1/files \
    -H "Authorization: Bearer $API_KEY" \
    -H "Content-Type: multipart/form-data" \
    -F "file=@my_batch_request.jsonl" \
    -F "purpose=batch" | jq -r '.id')
echo "File uploaded, file ID: $FILE_ID"

# Submit batch job
BATCH_ID=$(curl -s https://api.kluster.ai/v1/batches \
    -H "Authorization: Bearer $API_KEY" \
    -H "Content-Type: application/json" \
    -d '{
        "input_file_id": "'$FILE_ID'",
        "endpoint": "/v1/chat/completions",
        "completion_window": "24h"
    }' | jq -r '.id')
echo "Batch job submitted, job ID: $BATCH_ID"

# Poll the batch status until it's completed
STATUS="in_progress"
while [[ "$STATUS" != "completed" ]]; do
    echo "Waiting for batch job to complete... Status: $STATUS"
    sleep 10 # Wait for 10 seconds before checking again

    STATUS=$(curl -s https://api.kluster.ai/v1/batches/$BATCH_ID \
        -H "Authorization: Bearer $API_KEY" \
        -H "Content-Type: application/json" | jq -r '.status')
done

# Retrieve the batch output file
kluster_OUTPUT_FILE=$(curl -s https://api.kluster.ai/v1/batches/$BATCH_ID \
    -H "Authorization: Bearer $API_KEY" \
    -H "Content-Type: application/json" | jq -r '.output_file_id')

# Retrieve the results
OUTPUT_CONTENT=$(curl -s https://api.kluster.ai/v1/files/$kluster_OUTPUT_FILE/content \
    -H "Authorization: Bearer $API_KEY")

# Log results
echo -e "\nImage1 URL: $image1_url"
echo -e "\nImage2 URL: $image2_url"
echo -e "\nImage3 URL: $image3_url"
echo -e "\n🔍 AI batch response:"
echo "$OUTPUT_CONTENT"
    ```

??? example "Meta Llama 4 Scout"

    ```bash
    #!/bin/bash

# Check if API_KEY is set and not empty
if [[ -z "$API_KEY" ]]; then
    echo "Error: API_KEY environment variable is not set." >&2
    exit 1
fi

echo -e "📤 Sending batch request to kluster.ai...\n"

# Define image URLs
# Newton's cradle
image1_url="https://github.com/kluster-ai/klusterai-cookbook/blob/main/images/balls-image.jpeg?raw=true"
# Text with typos
image2_url="https://github.com/kluster-ai/klusterai-cookbook/blob/main/images/text-typo-image.jpeg?raw=true"
# Parking sign
image3_url="https://github.com/kluster-ai/klusterai-cookbook/blob/main/images/parking-image.jpeg?raw=true"

# Create request with specified structure
cat << EOF > my_batch_request.jsonl
{"custom_id": "request-1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "meta-llama/Llama-4-Scout-17B-16E-Instruct", "messages": [{"role": "user", "content": [{"type": "text", "text": "What is this?"}, {"type": "image_url", "image_url": {"url": "$image1_url"}}]}],"max_completion_tokens": 1000}}
{"custom_id": "request-2", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "meta-llama/Llama-4-Scout-17B-16E-Instruct", "messages": [{"role": "user", "content": [{"type": "text", "text": "Extract the text, find typos if any."}, {"type": "image_url", "image_url": {"url": "$image2_url"}}]}],"max_completion_tokens": 1000}}
{"custom_id": "request-3", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "meta-llama/Llama-4-Scout-17B-16E-Instruct", "messages": [{"role": "user", "content": [{"type": "text", "text": "Who can park in the area?"}, {"type": "image_url", "image_url": {"url": "$image3_url"}}]}],"max_completion_tokens": 1000}}
EOF

# Upload batch job file
FILE_ID=$(curl -s https://api.kluster.ai/v1/files \
    -H "Authorization: Bearer $API_KEY" \
    -H "Content-Type: multipart/form-data" \
    -F "file=@my_batch_request.jsonl" \
    -F "purpose=batch" | jq -r '.id')
echo "File uploaded, file ID: $FILE_ID"

# Submit batch job
BATCH_ID=$(curl -s https://api.kluster.ai/v1/batches \
    -H "Authorization: Bearer $API_KEY" \
    -H "Content-Type: application/json" \
    -d '{
        "input_file_id": "'$FILE_ID'",
        "endpoint": "/v1/chat/completions",
        "completion_window": "24h"
    }' | jq -r '.id')
echo "Batch job submitted, job ID: $BATCH_ID"

# Poll the batch status until it's completed
STATUS="in_progress"
while [[ "$STATUS" != "completed" ]]; do
    echo "Waiting for batch job to complete... Status: $STATUS"
    sleep 10 # Wait for 10 seconds before checking again

    STATUS=$(curl -s https://api.kluster.ai/v1/batches/$BATCH_ID \
        -H "Authorization: Bearer $API_KEY" \
        -H "Content-Type: application/json" | jq -r '.status')
done

# Retrieve the batch output file
kluster_OUTPUT_FILE=$(curl -s https://api.kluster.ai/v1/batches/$BATCH_ID \
    -H "Authorization: Bearer $API_KEY" \
    -H "Content-Type: application/json" | jq -r '.output_file_id')

# Retrieve the results
OUTPUT_CONTENT=$(curl -s https://api.kluster.ai/v1/files/$kluster_OUTPUT_FILE/content \
    -H "Authorization: Bearer $API_KEY")

# Log results
echo -e "\nImage1 URL: $image1_url"
echo -e "\nImage2 URL: $image2_url"
echo -e "\nImage3 URL: $image3_url"
echo -e "\n🔍 AI batch response:"
echo "$OUTPUT_CONTENT"
    ```

??? example "Mistral NeMo"

    ```bash
    #!/bin/bash

# Check if API_KEY is set and not empty
if [[ -z "$API_KEY" ]]; then
    echo "Error: API_KEY environment variable is not set." >&2
    exit 1
fi

echo -e "📤 Sending batch request to kluster.ai...
"

# Create request with specified structure
cat << EOF > my_batch_request.jsonl
{"custom_id": "request-1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "mistralai/Mistral-Nemo-Instruct-2407", "messages": [{"role": "system", "content": "You are an experienced cook."}, {"role": "user", "content": "What is the ultimate breakfast sandwich?"}],"max_completion_tokens":1000}}
{"custom_id": "request-2", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "mistralai/Mistral-Nemo-Instruct-2407", "messages": [{"role": "system", "content": "You are an experienced maths tutor."}, {"role": "user", "content": "Explain the Pythagorean theorem."}],"max_completion_tokens":1000}}
{"custom_id": "request-4", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "mistralai/Mistral-Nemo-Instruct-2407", "messages":[{"role": "system", "content": "You are a multilingual, experienced maths tutor."}, {"role": "user", "content": "Explain the Pythagorean theorem in Spanish"}],"max_completion_tokens":1000}}
EOF

# Upload batch job file
FILE_ID=$(curl -s https://api.kluster.ai/v1/files \
    -H "Authorization: Bearer $API_KEY" \
    -H "Content-Type: multipart/form-data" \
    -F "file=@my_batch_request.jsonl" \
    -F "purpose=batch" | jq -r '.id')
echo "File uploaded, file ID: $FILE_ID"

# Submit batch job
BATCH_ID=$(curl -s https://api.kluster.ai/v1/batches \
    -H "Authorization: Bearer $API_KEY" \
    -H "Content-Type: application/json" \
    -d '{
        "input_file_id": "'$FILE_ID'",
        "endpoint": "/v1/chat/completions",
        "completion_window": "24h"
    }' | jq -r '.id')
echo "Batch job submitted, job ID: $BATCH_ID"


# Poll the batch status until it's completed
STATUS="in_progress"
while [[ "$STATUS" != "completed" ]]; do
    echo "Waiting for batch job to complete... Status: $STATUS"
    sleep 10 # Wait for 10 seconds before checking again

    STATUS=$(curl -s https://api.kluster.ai/v1/batches/$BATCH_ID \
        -H "Authorization: Bearer $API_KEY" \
        -H "Content-Type: application/json" | jq -r '.status')
done

# Retrieve the batch output file
kluster_OUTPUT_FILE=$(curl -s https://api.kluster.ai/v1/batches/$BATCH_ID \
    -H "Authorization: Bearer $API_KEY" \
    -H "Content-Type: application/json" | jq -r '.output_file_id')

# Retrieve the results
OUTPUT_CONTENT=$(curl -s https://api.kluster.ai/v1/files/$kluster_OUTPUT_FILE/content \
    -H "Authorization: Bearer $API_KEY")

# Log results
echo -e "\n🔍 AI batch response:"
echo "$OUTPUT_CONTENT"
    ```

??? example "Mistral Small"

    ```bash
    #!/bin/bash

# Check if API_KEY is set and not empty
if [[ -z "$API_KEY" ]]; then
    echo "Error: API_KEY environment variable is not set." >&2
    exit 1
fi

echo -e "📤 Sending batch request to kluster.ai...
"

# Create request with specified structure
cat << EOF > my_batch_request.jsonl
{"custom_id": "request-1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "mistralai/Mistral-Small-24B-Instruct-2501", "messages": [{"role": "system", "content": "You are an experienced cook."}, {"role": "user", "content": "What is the ultimate breakfast sandwich?"}],"max_completion_tokens":1000}}
{"custom_id": "request-2", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "mistralai/Mistral-Small-24B-Instruct-2501", "messages": [{"role": "system", "content": "You are an experienced maths tutor."}, {"role": "user", "content": "Explain the Pythagorean theorem."}],"max_completion_tokens":1000}}
{"custom_id": "request-4", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "mistralai/Mistral-Small-24B-Instruct-2501", "messages":[{"role": "system", "content": "You are a multilingual, experienced maths tutor."}, {"role": "user", "content": "Explain the Pythagorean theorem in Spanish"}],"max_completion_tokens":1000}}
EOF

# Upload batch job file
FILE_ID=$(curl -s https://api.kluster.ai/v1/files \
    -H "Authorization: Bearer $API_KEY" \
    -H "Content-Type: multipart/form-data" \
    -F "file=@my_batch_request.jsonl" \
    -F "purpose=batch" | jq -r '.id')
echo "File uploaded, file ID: $FILE_ID"

# Submit batch job
BATCH_ID=$(curl -s https://api.kluster.ai/v1/batches \
    -H "Authorization: Bearer $API_KEY" \
    -H "Content-Type: application/json" \
    -d '{
        "input_file_id": "'$FILE_ID'",
        "endpoint": "/v1/chat/completions",
        "completion_window": "24h"
    }' | jq -r '.id')
echo "Batch job submitted, job ID: $BATCH_ID"


# Poll the batch status until it's completed
STATUS="in_progress"
while [[ "$STATUS" != "completed" ]]; do
    echo "Waiting for batch job to complete... Status: $STATUS"
    sleep 10 # Wait for 10 seconds before checking again

    STATUS=$(curl -s https://api.kluster.ai/v1/batches/$BATCH_ID \
        -H "Authorization: Bearer $API_KEY" \
        -H "Content-Type: application/json" | jq -r '.status')
done

# Retrieve the batch output file
kluster_OUTPUT_FILE=$(curl -s https://api.kluster.ai/v1/batches/$BATCH_ID \
    -H "Authorization: Bearer $API_KEY" \
    -H "Content-Type: application/json" | jq -r '.output_file_id')

# Retrieve the results
OUTPUT_CONTENT=$(curl -s https://api.kluster.ai/v1/files/$kluster_OUTPUT_FILE/content \
    -H "Authorization: Bearer $API_KEY")

# Log results
echo -e "\n🔍 AI batch response:"
echo "$OUTPUT_CONTENT"
    ```

??? example "Qwen2.5-VL 7B"

    ```bash
    #!/bin/bash

# Check if API_KEY is set and not empty
if [[ -z "$API_KEY" ]]; then
    echo "Error: API_KEY environment variable is not set." >&2
    exit 1
fi

echo -e "📤 Sending batch request to kluster.ai...\n"

# Define image URLs
# Newton's cradle
image1_url="https://github.com/kluster-ai/klusterai-cookbook/blob/main/images/balls-image.jpeg?raw=true"
# Text with typos
image2_url="https://github.com/kluster-ai/klusterai-cookbook/blob/main/images/text-typo-image.jpeg?raw=true"
# Parking sign
image3_url="https://github.com/kluster-ai/klusterai-cookbook/blob/main/images/parking-image.jpeg?raw=true"

# Create request with specified structure
cat << EOF > my_batch_request.jsonl
{"custom_id": "request-1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "Qwen/Qwen2.5-VL-7B-Instruct", "messages": [{"role": "user", "content": [{"type": "text", "text": "What is this?"}, {"type": "image_url", "image_url": {"url": "$image1_url"}}]}],"max_completion_tokens": 1000}}
{"custom_id": "request-2", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "Qwen/Qwen2.5-VL-7B-Instruct", "messages": [{"role": "user", "content": [{"type": "text", "text": "Extract the text, find typos if any."}, {"type": "image_url", "image_url": {"url": "$image2_url"}}]}],"max_completion_tokens": 1000}}
{"custom_id": "request-3", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "Qwen/Qwen2.5-VL-7B-Instruct", "messages": [{"role": "user", "content": [{"type": "text", "text": "Who can park in the area?"}, {"type": "image_url", "image_url": {"url": "$image3_url"}}]}],"max_completion_tokens": 1000}}
EOF

# Upload batch job file
FILE_ID=$(curl -s https://api.kluster.ai/v1/files \
    -H "Authorization: Bearer $API_KEY" \
    -H "Content-Type: multipart/form-data" \
    -F "file=@my_batch_request.jsonl" \
    -F "purpose=batch" | jq -r '.id')
echo "File uploaded, file ID: $FILE_ID"

# Submit batch job
BATCH_ID=$(curl -s https://api.kluster.ai/v1/batches \
    -H "Authorization: Bearer $API_KEY" \
    -H "Content-Type: application/json" \
    -d '{
        "input_file_id": "'$FILE_ID'",
        "endpoint": "/v1/chat/completions",
        "completion_window": "24h"
    }' | jq -r '.id')
echo "Batch job submitted, job ID: $BATCH_ID"

# Poll the batch status until it's completed
STATUS="in_progress"
while [[ "$STATUS" != "completed" ]]; do
    echo "Waiting for batch job to complete... Status: $STATUS"
    sleep 10 # Wait for 10 seconds before checking again

    STATUS=$(curl -s https://api.kluster.ai/v1/batches/$BATCH_ID \
        -H "Authorization: Bearer $API_KEY" \
        -H "Content-Type: application/json" | jq -r '.status')
done

# Retrieve the batch output file
kluster_OUTPUT_FILE=$(curl -s https://api.kluster.ai/v1/batches/$BATCH_ID \
    -H "Authorization: Bearer $API_KEY" \
    -H "Content-Type: application/json" | jq -r '.output_file_id')

# Retrieve the results
OUTPUT_CONTENT=$(curl -s https://api.kluster.ai/v1/files/$kluster_OUTPUT_FILE/content \
    -H "Authorization: Bearer $API_KEY")

# Log results
echo -e "\nImage1 URL: $image1_url"
echo -e "\nImage2 URL: $image2_url"
echo -e "\nImage3 URL: $image3_url"
echo -e "\n🔍 AI batch response:"
echo "$OUTPUT_CONTENT"
    ```

??? example "Qwen3-235B-A22B"

    ```bash
    #!/bin/bash

# Check if API_KEY is set and not empty
if [[ -z "$API_KEY" ]]; then
    echo "Error: API_KEY environment variable is not set." >&2
    exit 1
fi

echo -e "📤 Sending batch request to kluster.ai...
"

# Create request with specified structure
cat << EOF > my_batch_request.jsonl
{"custom_id": "request-1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "Qwen/Qwen3-235B-A22B-FP8", "messages": [{"role": "system", "content": "You are an experienced cook."}, {"role": "user", "content": "What is the ultimate breakfast sandwich?"}],"max_completion_tokens":1000}}
{"custom_id": "request-2", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "Qwen/Qwen3-235B-A22B-FP8", "messages": [{"role": "system", "content": "You are an experienced maths tutor."}, {"role": "user", "content": "Explain the Pythagorean theorem."}],"max_completion_tokens":1000}}
{"custom_id": "request-4", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "Qwen/Qwen3-235B-A22B-FP8", "messages":[{"role": "system", "content": "You are a multilingual, experienced maths tutor."}, {"role": "user", "content": "Explain the Pythagorean theorem in Spanish"}],"max_completion_tokens":1000}}
EOF

# Upload batch job file
FILE_ID=$(curl -s https://api.kluster.ai/v1/files \
    -H "Authorization: Bearer $API_KEY" \
    -H "Content-Type: multipart/form-data" \
    -F "file=@my_batch_request.jsonl" \
    -F "purpose=batch" | jq -r '.id')
echo "File uploaded, file ID: $FILE_ID"

# Submit batch job
BATCH_ID=$(curl -s https://api.kluster.ai/v1/batches \
    -H "Authorization: Bearer $API_KEY" \
    -H "Content-Type: application/json" \
    -d '{
        "input_file_id": "'$FILE_ID'",
        "endpoint": "/v1/chat/completions",
        "completion_window": "24h"
    }' | jq -r '.id')
echo "Batch job submitted, job ID: $BATCH_ID"


# Poll the batch status until it's completed
STATUS="in_progress"
while [[ "$STATUS" != "completed" ]]; do
    echo "Waiting for batch job to complete... Status: $STATUS"
    sleep 10 # Wait for 10 seconds before checking again

    STATUS=$(curl -s https://api.kluster.ai/v1/batches/$BATCH_ID \
        -H "Authorization: Bearer $API_KEY" \
        -H "Content-Type: application/json" | jq -r '.status')
done

# Retrieve the batch output file
kluster_OUTPUT_FILE=$(curl -s https://api.kluster.ai/v1/batches/$BATCH_ID \
    -H "Authorization: Bearer $API_KEY" \
    -H "Content-Type: application/json" | jq -r '.output_file_id')

# Retrieve the results
OUTPUT_CONTENT=$(curl -s https://api.kluster.ai/v1/files/$kluster_OUTPUT_FILE/content \
    -H "Authorization: Bearer $API_KEY")

# Log results
echo -e "\n🔍 AI batch response:"
echo "$OUTPUT_CONTENT"
    ```


## Batch inference flow

This section details the batch inference process using the kluster.ai API and DeepSeek R1 model, but you can adapt it to any of the [supported models](/get-started/models/){target=\_blank}.

### Create batch jobs as JSON files

To begin the batch job workflow, you'll need to assemble your batch requests and add them to a [JSON Lines](https://jsonlines.org/){target=\_blank} file (`.jsonl`).

Each request must include the following arguments:

- `custom_id` ++"string"++: A unique request ID to match outputs to inputs.
- `method` ++"string"++: The HTTP method to use for the request. Currently, only `POST` is supported.
- `url` ++"string"++:  The `/v1/chat/completions` endpoint.
- `body` ++"object"++: A request body containing:
    - `model` ++"string"++ <span class="required" markdown>++"required"++</span>: Name of one of the [supported models](/get-started/models/){target=\_blank}.
    - `messages` ++"array"++ <span class="required" markdown>++"required"++</span>: A list of chat messages (`system`, `user`, or `assistant` roles, and also `image_url` for images).
    - Any optional [chat completion parameters](/api-reference/reference/#/http/api-endpoints/realtime/v1-chat-completions-post){target=\_blank}, such as `temperature`, `max_completion_tokens`, etc.

!!! tip
    You can use a different model for each request you submit.

The following examples generate requests and save them in a JSONL file, which is ready to be uploaded for processing.

=== "Python"

    ```python
    import json
import time
import os
from getpass import getpass

from openai import OpenAI

# Get API key from user input
api_key = os.environ.get("API_KEY") or getpass("Enter your kluster.ai API key: ")

# Initialize OpenAI client pointing to kluster.ai API
client = OpenAI(
    base_url="https://api.kluster.ai/v1",
    api_key=api_key,
)

print(f"📤 Sending batch request to kluster.ai...\n")

# Create request with specified structure
requests = [
    {
        "custom_id": "request-1",
        "method": "POST",
        "url": "/v1/chat/completions",
        "body": {
            "model": "deepseek-ai/DeepSeek-V3-0324",
            "messages": [
                {"role": "system", "content": "You are an experienced cook."},
                {"role": "user", "content": "What is the ultimate breakfast sandwich?"},
            ],
            "max_completion_tokens": 1000,
        },
    },
    {
        "custom_id": "request-2",
        "method": "POST",
        "url": "/v1/chat/completions",
        "body": {
            "model": "Qwen/Qwen3-235B-A22B-FP8",
            "messages": [
                {"role": "system", "content": "You are a maths tutor."},
                {"role": "user", "content": "Explain the Pythagorean theorem."},
            ],
            "max_completion_tokens": 1000,
        },
    },
    {
        "custom_id": "request-3",
        "method": "POST",
        "url": "/v1/chat/completions",
        "body": {
            "model": "meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8",
            "messages": [
                {
                    "role": "user",
                    "content": [
                        {"type": "text", "text": "Who can park in the area?"},
                        {
                            "type": "image_url",
                            "image_url": {
                                "url": "https://github.com/kluster-ai/klusterai-cookbook/blob/main/images/parking-image.jpeg?raw=true"
                            },
                        },
                    ],
                }
            ],
            "max_completion_tokens": 1000,
        },
    },
    # Additional tasks can be added here
]

# Save tasks to a JSONL file (newline-delimited JSON)
file_name = "my_batch_request.jsonl"
with open(file_name, "w") as file:
    for request in requests:
        file.write(json.dumps(request) + "\n")
    ```

=== "CLI"

    ```bash
    cat << EOF > my_batch_request.jsonl
    {"custom_id": "request-1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "deepseek-ai/DeepSeek-V3-0324", "messages": [{"role": "system", "content": "You are an experienced cook."}, {"role": "user", "content": "What is the ultimate breakfast sandwich?"}],"max_completion_tokens":1000}}
    {"custom_id": "request-2", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "Qwen/Qwen3-235B-A22B-FP8", "messages": [{"role": "system", "content": "You are a maths tutor."}, {"role": "user", "content": "Explain the Pythagorean theorem."}],"max_completion_tokens":1000}}
    {"custom_id": "request-3", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8", "messages": [{"role": "user", "content": [{"type": "text", "text": "Who can park in the area?"}, {"type": "image_url", "image_url": {"url": "https://github.com/kluster-ai/klusterai-cookbook/blob/main/images/parking-image.jpeg?raw=true"}}]}],"max_completion_tokens":1000}}
EOF
    ```

!!! warning
    For the free tier, the maximum number of batch requests (lines in the JSONL file) must be less than {{ batch.max_lines_free }}, and each file must not exceed {{ batch.max_size }}. For the standard tier, there is no limit to the number of batch requests, but the maximum batch file size is {{ batch.max_size }} per file. 

### Upload batch job files

After you've created the JSON Lines file, you need to upload it using the `files` endpoint along with the intended purpose. Consequently, you need to set the `purpose` value to `"batch"` for batch jobs.

The response will contain an `id` field; save this value as you'll need it in the next step, where it's referred to as `input_file_id`. You can view your uploaded files in the [**Files** tab](https://platform.kluster.ai/files){target=\_blank} of the kluster.ai platform.

Use the following command examples to upload your batch job files:

=== "Python"

    ```python
    # Upload batch job file
batch_input_file = client.files.create(file=open(file_name, "rb"), purpose="batch")
    ```

=== "curl"

    ```bash
    curl -s https://api.kluster.ai/v1/files \
        -H "Authorization: Bearer $API_KEY" \
        -H "Content-Type: multipart/form-data" \
        -F "file=@my_batch_request.jsonl" \
        -F "purpose=batch"
    ```


```Json title="Response"
{
    "id": "myfile-123",
    "bytes": 2797,
    "created_at": "1733832768",
    "filename": "my_batch_request.jsonl",
    "object": "file",
    "purpose": "batch"
}
```

!!! warning
    Remember that the maximum file size permitted is {{ batch.max_size }}.

### Submit a batch job

Next, submit a batch job by calling the `batches` endpoint and providing the `id` of the uploaded batch job file (from the previous section) as the [`input_file_id`, and additional parameters](/api-reference/reference/#/http/models/structures/v1-batches-request){target=\_blank} to specify the job's configuration.

The response includes an `id` that can be used to monitor the job's progress, as demonstrated in the next section.

You can use the following snippets to submit your batch job:

=== "Python"

    ```python
    # Submit batch job
batch_request = client.batches.create(
    input_file_id=batch_input_file.id,
    endpoint="/v1/chat/completions",
    completion_window="24h",
)
    ```

=== "curl"

    ```bash
    curl -s https://api.kluster.ai/v1/batches \
        -H "Authorization: Bearer $API_KEY" \
        -H "Content-Type: application/json" \
        -d '{
        "input_file_id": "myfile-123",
        "endpoint": "/v1/chat/completions",
        "completion_window": "24h"
        }'
    ```

```Json title="Response"
{
    "id": "mybatch-123",
    "completion_window": "24h",
    "created_at": 1733832777,
    "endpoint": "/v1/chat/completions",
    "input_file_id": "myfile-123",
    "object": "batch",
    "status": "validating",
    "cancelled_at": null,
    "cancelling_at": null,
    "completed_at": null,
    "error_file_id": null,
    "errors": null,
    "expired_at": null,
    "expires_at": 1733919177,
    "failed_at": null,
    "finalizing_at": null,
    "in_progress_at": null,
    "metadata": {},
    "output_file_id": null,
    "request_counts": {
        "completed": 0,
        "failed": 0,
        "total": 0
 }
}
```

### Monitor job progress

You can make periodic requests to the `batches` endpoint to monitor your batch job's progress. Use the `id` of the batch request from the preceding section as the [`batch_id`](/api-reference/reference/#/http/api-endpoints/batch/v1-batches-by-batch-id-get){target=\_blank} to check its status. The job is complete when the `status` field returns `"completed"`. You can also monitor jobs in the [**Batch** tab](https://platform.kluster.ai/batch) of the kluster.ai platform UI.

View a complete list of the [supported statuses](/api-reference/reference/#/http/models/enumerations/status){target=\_blank} on the API reference page.

You can use the following snippets to monitor your batch job:


=== "Python"

    ```python
    # Poll the batch status until it's complete
while True:
    batch_status = client.batches.retrieve(batch_request.id)
    print(f"Batch status: {batch_status.status}")
    print(
        f"Completed tasks: {batch_status.request_counts.completed} / {batch_status.request_counts.total}"
    )

    if batch_status.status.lower() in ["completed", "failed", "cancelled"]:
        break

    time.sleep(10)  # Wait for 10 seconds before checking again
    ```

=== "curl"

    ```bash
    curl -s https://api.kluster.ai/v1/batches/mybatch-123 \
        -H "Authorization: Bearer $API_KEY" \
        -H "Content-Type: application/json"
    ```


```Json title="Response"
{
    "id": "mybatch-123",
    "object": "batch",
    "endpoint": "/v1/chat/completions",
    "errors": null,
    "input_file_id": "myfile-123",
    "completion_window": "24h",
    "status": "completed",
    "output_file_id": "myfile-123-output",
    "error_file_id": null,
    "created_at": "1733832777",
    "in_progress_at": "1733832777",
    "expires_at": "1733919177",
    "finalizing_at": "1733832781",
    "completed_at": "1733832781",
    "failed_at": null,
    "expired_at": null,
    "cancelling_at": null,
    "cancelled_at": null,
    "request_counts": {
        "total": 4,
        "completed": 4,
        "failed": 0
 },
    "metadata": {}
}
```

### Retrieve results

To retrieve the content of your batch jobs output file, send a request to the `files` endpoint specifying the `output_file_id`, which is returned from querying the batch's status (from the previous section).

The output file will be a JSONL file, where each line contains the `custom_id` from your input file request and the corresponding response.

You can use the following snippets to retrieve the results from your batch job:

=== "Python"

    ```python 
    import json
import time
import os
from getpass import getpass

from openai import OpenAI

# Get API key from user input
api_key = os.environ.get("API_KEY") or getpass("Enter your kluster.ai API key: ")

# Initialize OpenAI client pointing to kluster.ai API
client = OpenAI(
    base_url="https://api.kluster.ai/v1",
    api_key=api_key,
)

print(f"📤 Sending batch request to kluster.ai...\n")

# Create request with specified structure
requests = [
    {
        "custom_id": "request-1",
        "method": "POST",
        "url": "/v1/chat/completions",
        "body": {
            "model": "deepseek-ai/DeepSeek-V3-0324",
            "messages": [
                {"role": "system", "content": "You are an experienced cook."},
                {"role": "user", "content": "What is the ultimate breakfast sandwich?"},
            ],
            "max_completion_tokens": 1000,
        },
    },
    {
        "custom_id": "request-2",
        "method": "POST",
        "url": "/v1/chat/completions",
        "body": {
            "model": "Qwen/Qwen3-235B-A22B-FP8",
            "messages": [
                {"role": "system", "content": "You are a maths tutor."},
                {"role": "user", "content": "Explain the Pythagorean theorem."},
            ],
            "max_completion_tokens": 1000,
        },
    },
    {
        "custom_id": "request-3",
        "method": "POST",
        "url": "/v1/chat/completions",
        "body": {
            "model": "meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8",
            "messages": [
                {
                    "role": "user",
                    "content": [
                        {"type": "text", "text": "Who can park in the area?"},
                        {
                            "type": "image_url",
                            "image_url": {
                                "url": "https://github.com/kluster-ai/klusterai-cookbook/blob/main/images/parking-image.jpeg?raw=true"
                            },
                        },
                    ],
                }
            ],
            "max_completion_tokens": 1000,
        },
    },
    # Additional tasks can be added here
]

# Save tasks to a JSONL file (newline-delimited JSON)
file_name = "my_batch_request.jsonl"
with open(file_name, "w") as file:
    for request in requests:
        file.write(json.dumps(request) + "\n")

# Upload batch job file
batch_input_file = client.files.create(file=open(file_name, "rb"), purpose="batch")

# Submit batch job
batch_request = client.batches.create(
    input_file_id=batch_input_file.id,
    endpoint="/v1/chat/completions",
    completion_window="24h",
)

# Poll the batch status until it's complete
while True:
    batch_status = client.batches.retrieve(batch_request.id)
    print(f"Batch status: {batch_status.status}")
    print(
        f"Completed tasks: {batch_status.request_counts.completed} / {batch_status.request_counts.total}"
    )

    if batch_status.status.lower() in ["completed", "failed", "cancelled"]:
        break

    time.sleep(10)  # Wait for 10 seconds before checking again

# Check if the Batch completed successfully
if batch_status.status.lower() == "completed":
    # Retrieve the results
    result_file_id = batch_status.output_file_id
    results = client.files.content(result_file_id).content

    # Save results to a file
    result_file_name = "batch_results.jsonl"
    with open(result_file_name, "wb") as file:
        file.write(results)
    print(f"💾 Response saved to {result_file_name}")
else:
    print(f"Batch failed with status: {batch_status.status}")
    ```

=== "curl"

    ```bash
    curl -s https://api.kluster.ai/v1/files/kluster-output-file-123/content \
        -H "Authorization: Bearer $API_KEY" > batch_results.jsonl
    ```

??? code "View the complete script"

    === "Python"

        ```python
        import json
import time
import os
from getpass import getpass

from openai import OpenAI

# Get API key from user input
api_key = os.environ.get("API_KEY") or getpass("Enter your kluster.ai API key: ")

# Initialize OpenAI client pointing to kluster.ai API
client = OpenAI(
    base_url="https://api.kluster.ai/v1",
    api_key=api_key,
)

print(f"📤 Sending batch request to kluster.ai...\n")

# Create request with specified structure
requests = [
    {
        "custom_id": "request-1",
        "method": "POST",
        "url": "/v1/chat/completions",
        "body": {
            "model": "deepseek-ai/DeepSeek-V3-0324",
            "messages": [
                {"role": "system", "content": "You are an experienced cook."},
                {"role": "user", "content": "What is the ultimate breakfast sandwich?"},
            ],
            "max_completion_tokens": 1000,
        },
    },
    {
        "custom_id": "request-2",
        "method": "POST",
        "url": "/v1/chat/completions",
        "body": {
            "model": "Qwen/Qwen3-235B-A22B-FP8",
            "messages": [
                {"role": "system", "content": "You are a maths tutor."},
                {"role": "user", "content": "Explain the Pythagorean theorem."},
            ],
            "max_completion_tokens": 1000,
        },
    },
    {
        "custom_id": "request-3",
        "method": "POST",
        "url": "/v1/chat/completions",
        "body": {
            "model": "meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8",
            "messages": [
                {
                    "role": "user",
                    "content": [
                        {"type": "text", "text": "Who can park in the area?"},
                        {
                            "type": "image_url",
                            "image_url": {
                                "url": "https://github.com/kluster-ai/klusterai-cookbook/blob/main/images/parking-image.jpeg?raw=true"
                            },
                        },
                    ],
                }
            ],
            "max_completion_tokens": 1000,
        },
    },
    # Additional tasks can be added here
]

# Save tasks to a JSONL file (newline-delimited JSON)
file_name = "my_batch_request.jsonl"
with open(file_name, "w") as file:
    for request in requests:
        file.write(json.dumps(request) + "\n")

# Upload batch job file
batch_input_file = client.files.create(file=open(file_name, "rb"), purpose="batch")

# Submit batch job
batch_request = client.batches.create(
    input_file_id=batch_input_file.id,
    endpoint="/v1/chat/completions",
    completion_window="24h",
)

# Poll the batch status until it's complete
while True:
    batch_status = client.batches.retrieve(batch_request.id)
    print(f"Batch status: {batch_status.status}")
    print(
        f"Completed tasks: {batch_status.request_counts.completed} / {batch_status.request_counts.total}"
    )

    if batch_status.status.lower() in ["completed", "failed", "cancelled"]:
        break

    time.sleep(10)  # Wait for 10 seconds before checking again

# Check if the Batch completed successfully
if batch_status.status.lower() == "completed":
    # Retrieve the results
    result_file_id = batch_status.output_file_id
    results = client.files.content(result_file_id).content

    # Save results to a file
    result_file_name = "batch_results.jsonl"
    with open(result_file_name, "wb") as file:
        file.write(results)
    print(f"💾 Response saved to {result_file_name}")
else:
    print(f"Batch failed with status: {batch_status.status}")
        ```

## List all batch jobs

To list all of your batch jobs, send a request to the `batches` endpoint without specifying a `batch_id`. To constrain the query response, you can also use a `limit` parameter.

You can use the following snippets to list all of your batch jobs:

=== "Python"

    ```python
    import os
    from openai import OpenAI
    from getpass import getpass
    
    # Get API key from user input
    api_key = os.environ.get("API_KEY") or getpass("Enter your kluster.ai API key: ")
    
    # Initialize OpenAI client pointing to kluster.ai API
    client = OpenAI(
        base_url="https://api.kluster.ai/v1",
        api_key=api_key,
    )

    # Log all batch jobs (limit to 3)
    print(client.batches.list(limit=3).to_dict())
    ```

=== "curl"

    ```bash
    curl -s https://api.kluster.ai/v1/batches \
        -H "Authorization: Bearer $API_KEY"
    ```

```Json title="Response"
{
"object": "list",
"data": [
    {
    "id": "mybatch-123",
    "object": "batch",
    "endpoint": "/v1/chat/completions",
    "errors": null,
    "input_file_id": "myfile-123",
    "completion_window": "24h",
    "status": "completed",
    "output_file_id": "myfile-123-output",
    "error_file_id": null,
    "created_at": "1733832777",
    "in_progress_at": "1733832777",
    "expires_at": "1733919177",
    "finalizing_at": "1733832781",
    "completed_at": "1733832781",
    "failed_at": null,
    "expired_at": null,
    "cancelling_at": null,
    "cancelled_at": null,
    "request_counts": {
        "total": 4,
        "completed": 4,
        "failed": 0
    },
    "metadata": {}
    },
{ ... },
],
"first_id": "mybatch-123",
"last_id": "mybatch-789",
"has_more": false,
"count": 1,
"page": 1,
"page_count": -1,
"items_per_page": 9223372036854775807
}
```

## Cancel a batch job

To cancel a batch job currently in progress, send a request to the `cancel` endpoint with your `batch_id`. Note that cancellation may take up to 10 minutes to complete, and the status will show as `canceling.` Once complete, the status will show as `cancelled`.

You can use the following snippets to cancel a batch job:

=== "Python"

    ```python title="Example"
    import os
    from openai import OpenAI
    from getpass import getpass
    
    # Get API key from user input
    api_key = os.environ.get("API_KEY") or getpass("Enter your kluster.ai API key: ")
    
    # Initialize OpenAI client pointing to kluster.ai API
    client = OpenAI(
        base_url="https://api.kluster.ai/v1",
        api_key=api_key,
    )

    # Cancel batch job with specified ID
    client.batches.cancel("mybatch-123")
    ```

=== "curl"

    ```bash title="Example"
    curl -s https://api.kluster.ai/v1/batches/$BATCH_ID/cancel \
        -H "Authorization: Bearer $API_KEY" \
        -H "Content-Type: application/json" \
        -X POST
    ```
```Json title="Response"
{
    "id": "mybatch-123",
    "object": "batch",
    "endpoint": "/v1/chat/completions",
    "errors": null,
    "input_file_id": "myfile-123",
    "completion_window": "24h",
    "status": "cancelling",
    "output_file_id": "myfile-123-output",
    "error_file_id": null,
    "created_at": "1730821906",
    "in_progress_at": "1730821911",
    "expires_at": "1730821906",
    "finalizing_at": null,
    "completed_at": null,
    "failed_at": null,
    "expired_at": null,
    "cancelling_at": "1730821906",
    "cancelled_at": null,
    "request_counts": {
        "total": 3,
        "completed": 3,
        "failed": 0
    },
    "metadata": {}
}
```

## Summary

You have now experienced the complete batch inference job lifecycle using kluster.ai's batch API. In this guide, you've learned how to:

- Prepare and submit batch jobs with structured request inputs.
- Track your job's progress in real-time.
- Retrieve and handle job results.
- View and manage your batch jobs.
- Cancel jobs when needed.

The kluster.ai batch API is designed to efficiently and reliably handle your large-scale LLM workloads. If you have questions or suggestions, the [support](mailto:support@kluster.ai){target=\_blank} team would love to hear from you.
--- END CONTENT ---

Doc-Content: https://docs.kluster.ai/get-started/start-building/real-time/
--- BEGIN CONTENT ---
---

title: Perform real-time inference jobs
description: This page provides examples and instructions for submitting and managing real-time jobs using kluster.ai's OpenAI-compatible API.
---

# Perform real-time inference jobs

## Overview

This guide provides guidance about how to use real-time inference with the [kluster.ai](https://www.kluster.ai/){target=\_blank} API. This type of inference is best suited for use cases requiring instant, synchronous responses for user-facing features like chat interactions, live recommendations, or real-time decision-making.

You will learn how to submit a request and retrieve responses, and where to find integration guides for using kluster.ai's API with some of your favorite third-party LLM interfaces. Please make sure you check the [API request limits](/get-started/models/#api-request-limits){target=\_blank}.

## Prerequisites

This guide assumes familiarity with Large Language Model (LLM) development and OpenAI libraries. Before getting started, make sure you have:

- **A kluster.ai account**: Sign up on the [kluster.ai platform](https://platform.kluster.ai/signup){target=\_blank} if you don't have one.
- **A kluster.ai API key**: After signing in, go to the [**API Keys**](https://platform.kluster.ai/apikeys){target=\_blank} section and create a new key. For detailed instructions, check out the [Get an API key](/get-started/get-api-key/){target=\_blank} guide.
- **A virtual Python environment (optional)**: Recommended for developers using Python. It helps isolate Python installations in a [virtual environment](https://packaging.python.org/en/latest/guides/installing-using-pip-and-virtual-environments/){target=\_blank} to reduce the risk of environment or package conflicts between your projects.
- **Required Python libraries**: Install the following Python libraries:
    - [**OpenAI Python API library**](https://pypi.org/project/openai/): To access the `openai` module.
    - [**`getpass`**](https://pypi.org/project/getpass4/): To handle API keys safely.

If you plan to use cURL via the CLI, you can export kluster.ai API key as a variable:

```bash
export API_KEY=INSERT_API_KEY
```

## Supported models

Please visit the [Models](/get-started/models/){target=\_blank} page to learn more about all the models supported by the kluster.ai batch API.

In addition, you can see the complete list of available models programmatically using the [list supported models](/api-reference/reference/#/http/api-endpoints/models/v1-models-get){target=\_blank} endpoint.

## Quickstart snippets

The following code snippets provide a complete end-to-end real-time inference example for different models supported by kluster.ai. You can copy and paste the snippet into your local environment. 

### Python

To use these snippets, run the Python script and enter your kluster.ai API key when prompted.

??? example "DeepSeek-R1"

    ```python
    # Real-time completions with the DeepSeek-R1 model on kluster.ai

from os import environ
from openai import OpenAI
from getpass import getpass

# Get API key from user input
api_key = environ.get("API_KEY") or getpass("Enter your kluster.ai API key: ")

print(f"📤 Sending a chat completion request to kluster.ai...\n")

# Initialize OpenAI client pointing to kluster.ai API
client = OpenAI(
    api_key=api_key,
    base_url="https://api.kluster.ai/v1"
)

# Create chat completion request
completion = client.chat.completions.create(
    model="deepseek-ai/DeepSeek-R1",
    messages=[
        {"role": "user", "content": "What is the ultimate breakfast sandwich?"}
    ]
)

"""Logs the full AI response to terminal."""

# Extract model name and AI-generated text
model_name = completion.model
text_response = completion.choices[0].message.content

# Print response to console
print(f"\n🔍 AI response (model: {model_name}):")
print(text_response)
    ```

??? example "DeepSeek-R1-0528"

    ```python
    # Real-time completions with the DeepSeek-R1-0528 model on kluster.ai

from os import environ
from openai import OpenAI
from getpass import getpass

# Get API key from user input
api_key = environ.get("API_KEY") or getpass("Enter your kluster.ai API key: ")

print(f"📤 Sending a chat completion request to kluster.ai...\n")

# Initialize OpenAI client pointing to kluster.ai API
client = OpenAI(
    api_key=api_key,
    base_url="https://api.kluster.ai/v1"
)

# Create chat completion request
completion = client.chat.completions.create(
    model="deepseek-ai/DeepSeek-R1-0528",
    messages=[
        {"role": "user", "content": "What is the ultimate breakfast sandwich?"}
    ]
)

"""Logs the full AI response to terminal."""

# Extract model name and AI-generated text
model_name = completion.model
text_response = completion.choices[0].message.content

# Print response to console
print(f"\n🔍 AI response (model: {model_name}):")
print(text_response)
    ```

??? example "DeepSeek-V3-0324"

    ```python
    # Real-time completions with the DeepSeek-V3-0324 model on kluster.ai

from os import environ
from openai import OpenAI
from getpass import getpass

# Get API key from user input
api_key = environ.get("API_KEY") or getpass("Enter your kluster.ai API key: ")

print(f"📤 Sending a chat completion request to kluster.ai...\n")

# Initialize OpenAI client pointing to kluster.ai API
client = OpenAI(
    api_key=api_key,
    base_url="https://api.kluster.ai/v1"
)

# Create chat completion request
completion = client.chat.completions.create(
    model="deepseek-ai/DeepSeek-V3-0324",
    messages=[
        {"role": "user", "content": "What is the ultimate breakfast sandwich?"}
    ]
)

"""Logs the full AI response to terminal."""

# Extract model name and AI-generated text
model_name = completion.model
text_response = completion.choices[0].message.content

# Print response to console
print(f"\n🔍 AI response (model: {model_name}):")
print(text_response)
    ```

??? example "Gemma 3 27B"

    ```python
    # Real-time completions with the Gemma 3 27B model on kluster.ai

from os import environ
from openai import OpenAI
from getpass import getpass

image_url = "https://github.com/kluster-ai/klusterai-cookbook/blob/main/images/parking-image.jpeg?raw=true"

# Get API key from user input
api_key = environ.get("API_KEY") or getpass("Enter your kluster.ai API key: ")

# Initialize OpenAI client pointing to kluster.ai API
client = OpenAI(api_key=api_key, base_url="https://api.kluster.ai/v1")

print(f"📤 Sending a chat completion request to kluster.ai...\n")

# Create chat completion request
completion = client.chat.completions.create(
    model="google/gemma-3-27b-it",
    messages=[
        {
            "role": "user",
            "content": [
                {"type": "text", "text": "Who can park in the area?"},
                {"type": "image_url", "image_url": {"url": image_url}},
            ],
        }
    ],
)

print(f"\nImage URL: {image_url}")

"""Logs the full AI response to terminal."""

# Extract model name and AI-generated text
model_name = completion.model
text_response = completion.choices[0].message.content

# Print response to console
print(f"\n🔍 AI response (model: {model_name}):")
print(text_response)
    ```

??? example "Magistral Small"

    ```python
    # Real-time completions with the Magistral Small model on kluster.ai

from os import environ
from openai import OpenAI
from getpass import getpass

# Get API key from user input
api_key = environ.get("API_KEY") or getpass("Enter your kluster.ai API key: ")

print(f"📤 Sending a chat completion request to kluster.ai...\n")

# Initialize OpenAI client pointing to kluster.ai API
client = OpenAI(
    api_key=api_key,
    base_url="https://api.kluster.ai/v1"
)

# Create chat completion request
completion = client.chat.completions.create(
    model="mistralai/Magistral-Small-2506",
    messages=[
        {"role": "user", "content": "What is the ultimate breakfast sandwich?"}
    ]
)

"""Logs the full AI response to terminal."""

# Extract model name and AI-generated text
model_name = completion.model
text_response = completion.choices[0].message.content

# Print response to console
print(f"\n🔍 AI response (model: {model_name}):")
print(text_response)
    ```

??? example "Meta Llama 3.1 8B"

    ```python
    # Real-time completions with the Meta Llama 3.1 8B model on kluster.ai

from os import environ
from openai import OpenAI
from getpass import getpass

# Get API key from user input
api_key = environ.get("API_KEY") or getpass("Enter your kluster.ai API key: ")

print(f"📤 Sending a chat completion request to kluster.ai...\n")

# Initialize OpenAI client pointing to kluster.ai API
client = OpenAI(
    api_key=api_key,
    base_url="https://api.kluster.ai/v1"
)

# Create chat completion request
completion = client.chat.completions.create(
    model="klusterai/Meta-Llama-3.1-8B-Instruct-Turbo",
    messages=[
        {"role": "user", "content": "What is the ultimate breakfast sandwich?"}
    ]
)

"""Logs the full AI response to terminal."""

# Extract model name and AI-generated text
model_name = completion.model
text_response = completion.choices[0].message.content

# Print response to console
print(f"\n🔍 AI response (model: {model_name}):")
print(text_response)
    ```

??? example "Meta Llama 3.3 70B"

    ```python
    # Real-time completions with the Meta Llama 3.3 70B model on kluster.ai

from os import environ
from openai import OpenAI
from getpass import getpass

# Get API key from user input
api_key = environ.get("API_KEY") or getpass("Enter your kluster.ai API key: ")

print(f"📤 Sending a chat completion request to kluster.ai...\n")

# Initialize OpenAI client pointing to kluster.ai API
client = OpenAI(
    api_key=api_key,
    base_url="https://api.kluster.ai/v1"
)

# Create chat completion request
completion = client.chat.completions.create(
    model="klusterai/Meta-Llama-3.3-70B-Instruct-Turbo",
    messages=[
        {"role": "user", "content": "What is the ultimate breakfast sandwich?"}
    ]
)

"""Logs the full AI response to terminal."""

# Extract model name and AI-generated text
model_name = completion.model
text_response = completion.choices[0].message.content

# Print response to console
print(f"\n🔍 AI response (model: {model_name}):")
print(text_response)
    ```

??? example "Meta Llama 4 Maverick"

    ```python
    # Real-time completions with the Meta Llama 4 Maverick model on kluster.ai

from os import environ
from openai import OpenAI
from getpass import getpass

image_url = "https://github.com/kluster-ai/klusterai-cookbook/blob/main/images/parking-image.jpeg?raw=true"

# Get API key from user input
api_key = environ.get("API_KEY") or getpass("Enter your kluster.ai API key: ")

# Initialize OpenAI client pointing to kluster.ai API
client = OpenAI(api_key=api_key, base_url="https://api.kluster.ai/v1")

print(f"📤 Sending a chat completion request to kluster.ai...\n")

# Create chat completion request
completion = client.chat.completions.create(
    model="meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8",
    messages=[
        {
            "role": "user",
            "content": [
                {"type": "text", "text": "Who can park in the area?"},
                {"type": "image_url", "image_url": {"url": image_url}},
            ],
        }
    ],
)

print(f"\nImage URL: {image_url}")

"""Logs the full AI response to terminal."""

# Extract model name and AI-generated text
model_name = completion.model
text_response = completion.choices[0].message.content

# Print response to console
print(f"\n🔍 AI response (model: {model_name}):")
print(text_response)
    ```

??? example "Meta Llama 4 Scout"

    ```python
    # Real-time completions with the Meta Llama 4 Scout model on kluster.ai

from os import environ
from openai import OpenAI
from getpass import getpass

image_url = "https://github.com/kluster-ai/klusterai-cookbook/blob/main/images/parking-image.jpeg?raw=true"

# Get API key from user input
api_key = environ.get("API_KEY") or getpass("Enter your kluster.ai API key: ")

# Initialize OpenAI client pointing to kluster.ai API
client = OpenAI(api_key=api_key, base_url="https://api.kluster.ai/v1")

print(f"📤 Sending a chat completion request to kluster.ai...\n")

# Create chat completion request
completion = client.chat.completions.create(
    model="meta-llama/Llama-4-Scout-17B-16E-Instruct",
    messages=[
        {
            "role": "user",
            "content": [
                {"type": "text", "text": "Who can park in the area?"},
                {"type": "image_url", "image_url": {"url": image_url}},
            ],
        }
    ],
)

print(f"\nImage URL: {image_url}")

"""Logs the full AI response to terminal."""

# Extract model name and AI-generated text
model_name = completion.model
text_response = completion.choices[0].message.content

# Print response to console
print(f"\n🔍 AI response (model: {model_name}):")
print(text_response)
    ```

??? example "Mistral NeMo"

    ```python
    # Real-time completions with the Mistral NeMo model on kluster.ai

from os import environ
from openai import OpenAI
from getpass import getpass

# Get API key from user input
api_key = environ.get("API_KEY") or getpass("Enter your kluster.ai API key: ")

print(f"📤 Sending a chat completion request to kluster.ai...\n")

# Initialize OpenAI client pointing to kluster.ai API
client = OpenAI(
    api_key=api_key,
    base_url="https://api.kluster.ai/v1"
)

# Create chat completion request
completion = client.chat.completions.create(
    model="mistralai/Mistral-Nemo-Instruct-2407",
    messages=[
        {"role": "user", "content": "What is the ultimate breakfast sandwich?"}
    ]
)

"""Logs the full AI response to terminal."""

# Extract model name and AI-generated text
model_name = completion.model
text_response = completion.choices[0].message.content

# Print response to console
print(f"\n🔍 AI response (model: {model_name}):")
print(text_response)
    ```

??? example "Mistral Small"

    ```python
    # Real-time completions with the Mistral Small model on kluster.ai

from os import environ
from openai import OpenAI
from getpass import getpass

# Get API key from user input
api_key = environ.get("API_KEY") or getpass("Enter your kluster.ai API key: ")

print(f"📤 Sending a chat completion request to kluster.ai...\n")

# Initialize OpenAI client pointing to kluster.ai API
client = OpenAI(
    api_key=api_key,
    base_url="https://api.kluster.ai/v1"
)

# Create chat completion request
completion = client.chat.completions.create(
    model="mistralai/Mistral-Small-24B-Instruct-2501",
    messages=[
        {"role": "user", "content": "What is the ultimate breakfast sandwich?"}
    ]
)

"""Logs the full AI response to terminal."""

# Extract model name and AI-generated text
model_name = completion.model
text_response = completion.choices[0].message.content

# Print response to console
print(f"\n🔍 AI response (model: {model_name}):")
print(text_response)
    ```

??? example "Qwen2.5-VL 7B"

    ```python
    # Real-time completions with the Qwen2.5-VL 7B model on kluster.ai

from os import environ
from openai import OpenAI
from getpass import getpass

image_url = "https://github.com/kluster-ai/klusterai-cookbook/blob/main/images/parking-image.jpeg?raw=true"

# Get API key from user input
api_key = environ.get("API_KEY") or getpass("Enter your kluster.ai API key: ")

# Initialize OpenAI client pointing to kluster.ai API
client = OpenAI(api_key=api_key, base_url="https://api.kluster.ai/v1")

print(f"📤 Sending a chat completion request to kluster.ai...\n")

# Create chat completion request
completion = client.chat.completions.create(
    model="Qwen/Qwen2.5-VL-7B-Instruct",
    messages=[
        {
            "role": "user",
            "content": [
                {"type": "text", "text": "Who can park in the area?"},
                {"type": "image_url", "image_url": {"url": image_url}},
            ],
        }
    ],
)

print(f"\nImage URL: {image_url}")

"""Logs the full AI response to terminal."""

# Extract model name and AI-generated text
model_name = completion.model
text_response = completion.choices[0].message.content

# Print response to console
print(f"\n🔍 AI response (model: {model_name}):")
print(text_response)
    ```

??? example "Qwen3-235B-A22B"

    ```python
    # Real-time completions with the Qwen3-235B-A22B model on kluster.ai

from os import environ
from openai import OpenAI
from getpass import getpass

# Get API key from user input
api_key = environ.get("API_KEY") or getpass("Enter your kluster.ai API key: ")

print(f"📤 Sending a chat completion request to kluster.ai...\n")

# Initialize OpenAI client pointing to kluster.ai API
client = OpenAI(
    api_key=api_key,
    base_url="https://api.kluster.ai/v1"
)

# Create chat completion request
completion = client.chat.completions.create(
    model="Qwen/Qwen3-235B-A22B-FP8",
    messages=[
        {"role": "user", "content": "What is the ultimate breakfast sandwich?"}
    ]
)

"""Logs the full AI response to terminal."""

# Extract model name and AI-generated text
model_name = completion.model
text_response = completion.choices[0].message.content

# Print response to console
print(f"\n🔍 AI response (model: {model_name}):")
print(text_response)
    ```
### CLI

Similarly, the following curl commands showcase how to easily send a chat completion request to kluster.ai for the different supported models. This example assumes you've exported your kluster.ai API key as the variable `API_KEY`.


??? example "DeepSeek-R1"

    ```bash
    #!/bin/bash

# Check if API_KEY is set and not empty
if [[ -z "$API_KEY" ]]; then
    echo -e "\nError: API_KEY environment variable is not set.\n" >&2
fi

echo -e "📤 Sending a chat completion request to kluster.ai...\n"

# Submit real-time request
curl https://api.kluster.ai/v1/chat/completions \
    -H "Authorization: Bearer $API_KEY" \
    -H "Content-Type: application/json" \
    -d "{
            \"model\": \"deepseek-ai/DeepSeek-R1\", 
            \"messages\": [
                { 
                    \"role\": \"user\", 
                    \"content\": \"What is the ultimate breakfast sandwich?\"
                }
            ]
        }"
    ```

??? example "DeepSeek-R1-0528"

    ```bash
    #!/bin/bash

# Check if API_KEY is set and not empty
if [[ -z "$API_KEY" ]]; then
    echo -e "\nError: API_KEY environment variable is not set.\n" >&2
fi

echo -e "📤 Sending a chat completion request to kluster.ai...\n"

# Submit real-time request
curl https://api.kluster.ai/v1/chat/completions \
    -H "Authorization: Bearer $API_KEY" \
    -H "Content-Type: application/json" \
    -d "{
            \"model\": \"deepseek-ai/DeepSeek-R1-0528\", 
            \"messages\": [
                { 
                    \"role\": \"user\", 
                    \"content\": \"What is the ultimate breakfast sandwich?\"
                }
            ]
        }"
    ```

??? example "DeepSeek-V3-0324"

    ```bash
    #!/bin/bash

# Check if API_KEY is set and not empty
if [[ -z "$API_KEY" ]]; then
    echo -e "\nError: API_KEY environment variable is not set.\n" >&2
fi

echo -e "📤 Sending a chat completion request to kluster.ai...\n"

# Submit real-time request
curl https://api.kluster.ai/v1/chat/completions \
    -H "Authorization: Bearer $API_KEY" \
    -H "Content-Type: application/json" \
    -d "{
            \"model\": \"deepseek-ai/DeepSeek-V3-0324\", 
            \"messages\": [
                { 
                    \"role\": \"user\", 
                    \"content\": \"What is the ultimate breakfast sandwich?\"
                }
            ]
        }"
    ```

??? example "Gemma 3 27B"

    ```bash
    #!/bin/bash

# Check if API_KEY is set and not empty
if [[ -z "$API_KEY" ]]; then
    echo -e "\nError: API_KEY environment variable is not set.\n" >&2
fi

echo -e "📤 Sending a chat completion request to kluster.ai...\n"

image_url="https://github.com/kluster-ai/klusterai-cookbook/blob/main/images/parking-image.jpeg?raw=true"

# Submit real-time request
curl https://api.kluster.ai/v1/chat/completions \
    -H "Authorization: Bearer $API_KEY" \
    -H "Content-Type: application/json" \
    -d "{
        \"model\": \"google/gemma-3-27b-it\",
        \"messages\": [
            {
                \"role\": \"user\",
                \"content\": [
                    {\"type\": \"text\", \"text\": \"Who can park in the area?\"},
                    {\"type\": \"image_url\", \"image_url\": {\"url\": \"$image_url\"}}
                ]
            }
        ]
    }"
    ```

??? example "Magistral Small"

    ```bash
    #!/bin/bash

# Check if API_KEY is set and not empty
if [[ -z "$API_KEY" ]]; then
    echo -e "\nError: API_KEY environment variable is not set.\n" >&2
fi

echo -e "📤 Sending a chat completion request to kluster.ai...\n"

# Submit real-time request
curl https://api.kluster.ai/v1/chat/completions \
    -H "Authorization: Bearer $API_KEY" \
    -H "Content-Type: application/json" \
    -d "{
            \"model\": \"mistralai/Magistral-Small-2506\", 
            \"messages\": [
                { 
                    \"role\": \"user\", 
                    \"content\": \"What is the ultimate breakfast sandwich?\"
                }
            ]
        }"
    ```

??? example "Meta Llama 3.1 8B"

    ```bash
    #!/bin/bash

# Check if API_KEY is set and not empty
if [[ -z "$API_KEY" ]]; then
    echo -e "\nError: API_KEY environment variable is not set.\n" >&2
fi

echo -e "📤 Sending a chat completion request to kluster.ai...\n"

# Submit real-time request
curl https://api.kluster.ai/v1/chat/completions \
    -H "Authorization: Bearer $API_KEY" \
    -H "Content-Type: application/json" \
    -d "{
            \"model\": \"klusterai/Meta-Llama-3.1-8B-Instruct-Turbo\", 
            \"messages\": [
                { 
                    \"role\": \"user\", 
                    \"content\": \"What is the ultimate breakfast sandwich?\"
                }
            ]
        }"
    ```

??? example "Meta Llama 3.3 70B"

    ```bash
    #!/bin/bash

# Check if API_KEY is set and not empty
if [[ -z "$API_KEY" ]]; then
    echo -e "\nError: API_KEY environment variable is not set.\n" >&2
fi

echo -e "📤 Sending a chat completion request to kluster.ai...\n"

# Submit real-time request
curl https://api.kluster.ai/v1/chat/completions \
    -H "Authorization: Bearer $API_KEY" \
    -H "Content-Type: application/json" \
    -d "{
            \"model\": \"klusterai/Meta-Llama-3.3-70B-Instruct-Turbo\", 
            \"messages\": [
                { 
                    \"role\": \"user\", 
                    \"content\": \"What is the ultimate breakfast sandwich?\"
                }
            ]
        }"
    ```

??? example "Meta Llama 4 Maverick"

    ```bash
    #!/bin/bash

# Check if API_KEY is set and not empty
if [[ -z "$API_KEY" ]]; then
    echo -e "\nError: API_KEY environment variable is not set.\n" >&2
fi

echo -e "📤 Sending a chat completion request to kluster.ai...\n"

image_url="https://github.com/kluster-ai/klusterai-cookbook/blob/main/images/parking-image.jpeg?raw=true"

# Submit real-time request
curl https://api.kluster.ai/v1/chat/completions \
    -H "Authorization: Bearer $API_KEY" \
    -H "Content-Type: application/json" \
    -d "{
        \"model\": \"meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8\",
        \"messages\": [
            {
                \"role\": \"user\",
                \"content\": [
                    {\"type\": \"text\", \"text\": \"Who can park in the area?\"},
                    {\"type\": \"image_url\", \"image_url\": {\"url\": \"$image_url\"}}
                ]
            }
        ]
    }"
    ```

??? example "Meta Llama 4 Scout"

    ```bash
    #!/bin/bash

# Check if API_KEY is set and not empty
if [[ -z "$API_KEY" ]]; then
    echo -e "\nError: API_KEY environment variable is not set.\n" >&2
fi

echo -e "📤 Sending a chat completion request to kluster.ai...\n"

image_url="https://github.com/kluster-ai/klusterai-cookbook/blob/main/images/parking-image.jpeg?raw=true"

# Submit real-time request
curl https://api.kluster.ai/v1/chat/completions \
    -H "Authorization: Bearer $API_KEY" \
    -H "Content-Type: application/json" \
    -d "{
        \"model\": \"meta-llama/Llama-4-Scout-17B-16E-Instruct\",
        \"messages\": [
            {
                \"role\": \"user\",
                \"content\": [
                    {\"type\": \"text\", \"text\": \"Who can park in the area?\"},
                    {\"type\": \"image_url\", \"image_url\": {\"url\": \"$image_url\"}}
                ]
            }
        ]
    }"
    ```

??? example "Mistral NeMo"

    ```bash
    #!/bin/bash

# Check if API_KEY is set and not empty
if [[ -z "$API_KEY" ]]; then
    echo -e "\nError: API_KEY environment variable is not set.\n" >&2
fi

echo -e "📤 Sending a chat completion request to kluster.ai...\n"

# Submit real-time request
curl https://api.kluster.ai/v1/chat/completions \
    -H "Authorization: Bearer $API_KEY" \
    -H "Content-Type: application/json" \
    -d "{
            \"model\": \"mistralai/Mistral-Nemo-Instruct-2407\", 
            \"messages\": [
                { 
                    \"role\": \"user\", 
                    \"content\": \"What is the ultimate breakfast sandwich?\"
                }
            ]
        }"
    ```

??? example "Mistral Small"

    ```bash
    #!/bin/bash

# Check if API_KEY is set and not empty
if [[ -z "$API_KEY" ]]; then
    echo -e "\nError: API_KEY environment variable is not set.\n" >&2
fi

echo -e "📤 Sending a chat completion request to kluster.ai...\n"

# Submit real-time request
curl https://api.kluster.ai/v1/chat/completions \
    -H "Authorization: Bearer $API_KEY" \
    -H "Content-Type: application/json" \
    -d "{
            \"model\": \"mistralai/Mistral-Small-24B-Instruct-2501\", 
            \"messages\": [
                { 
                    \"role\": \"user\", 
                    \"content\": \"What is the ultimate breakfast sandwich?\"
                }
            ]
        }"
    ```

??? example "Qwen2.5-VL 7B"

    ```bash
    #!/bin/bash

# Check if API_KEY is set and not empty
if [[ -z "$API_KEY" ]]; then
    echo -e "\nError: API_KEY environment variable is not set.\n" >&2
fi

echo -e "📤 Sending a chat completion request to kluster.ai...\n"

image_url="https://github.com/kluster-ai/klusterai-cookbook/blob/main/images/parking-image.jpeg?raw=true"

# Submit real-time request
curl https://api.kluster.ai/v1/chat/completions \
    -H "Authorization: Bearer $API_KEY" \
    -H "Content-Type: application/json" \
    -d "{
        \"model\": \"Qwen/Qwen2.5-VL-7B-Instruct\",
        \"messages\": [
            {
                \"role\": \"user\",
                \"content\": [
                    {\"type\": \"text\", \"text\": \"Who can park in the area?\"},
                    {\"type\": \"image_url\", \"image_url\": {\"url\": \"$image_url\"}}
                ]
            }
        ]
    }"
    ```

??? example "Qwen3-235B-A22B"

    ```bash
    #!/bin/bash

# Check if API_KEY is set and not empty
if [[ -z "$API_KEY" ]]; then
    echo -e "\nError: API_KEY environment variable is not set.\n" >&2
fi

echo -e "📤 Sending a chat completion request to kluster.ai...\n"

# Submit real-time request
curl https://api.kluster.ai/v1/chat/completions \
    -H "Authorization: Bearer $API_KEY" \
    -H "Content-Type: application/json" \
    -d "{
            \"model\": \"Qwen/Qwen3-235B-A22B-FP8\", 
            \"messages\": [
                { 
                    \"role\": \"user\", 
                    \"content\": \"What is the ultimate breakfast sandwich?\"
                }
            ]
        }"
    ```


## Real-time inference flow

This section details the real-time inference process using the kluster.ai API and DeepSeek R1 model, but you can adapt it to any of the [supported models](/get-started/models/){target=\_blank}.

### Submitting a request

The kluster.ai platform offers a simple, [OpenAI-compatible](/get-started/openai-compatibility/){target=\_blank} interface, making it easy to integrate kluster.ai services seamlessly into your existing system.

The following code shows how to do a chat completions request using the OpenAI library.

=== "Python"

    ```python
    import json
import os
from getpass import getpass

from openai import OpenAI

# Get API key from user input
api_key = os.environ.get("API_KEY") or getpass("Enter your kluster.ai API key: ")

# Initialize OpenAI client pointing to kluster.ai API
client = OpenAI(
    api_key=api_key,
    base_url="https://api.kluster.ai/v1"
)

print(f"📤 Sending a chat completion request to kluster.ai...\n")

# Create chat completion request
completion = client.chat.completions.create(
    model="deepseek-ai/DeepSeek-V3-0324",
    messages=[
        {"role": "user", "content": "What is the ultimate breakfast sandwich?"}
    ]
)
    ```

If successful, the `completion` variable contains a full response, which you'll need to analyze to extract the answer you are looking for. In terms of configuration for real-time inferences, there are several parameters that you need to tweak:

- `model` ++"string"++ <span class="required" markdown>++"required"++</span>: Name of one of the [supported models](/get-started/models/){target=\_blank}.
- `messages` ++"array"++ <span class="required" markdown>++"required"++</span>: A list of chat messages (`system`, `user`, or `assistant` roles, and also `image_url` for images). In this example, the query is "What is the ultimate breakfast sandwich?". 

Once these parameters are configured, run your script to send the request.

### Fetching the response

If the request is successful, the response is contained in the `completion` variable from the example above. It should follow the structure below and include relevant data such as the generated output, metadata, and token usage details. 

```Json title="Response"
{
    "id": "a3af373493654dd195108b207e2faacf",
    "choices": [
        {
            "finish_reason": "stop",
            "index": 0,
            "logprobs": null,
            "message": {
                "content": "The \"ultimate\" breakfast sandwich is subjective and can vary based on personal preferences, but here's a classic, crowd-pleasing version that combines savory, sweet, and hearty elements for a satisfying morning meal:\n\n### **The Ultimate Breakfast Sandwich**\n**Ingredients:**\n- **Bread:** A toasted brioche bun, English muffin, or sourdough slice (your choice for texture and flavor).\n- **Protein:** Crispy bacon, sausage patty, or ham.\n- **Egg:** Fried, scrambled, or a fluffy omelet-style egg.\n- **Cheese:** Sharp cheddar, gooey American, or creamy Swiss.\n- **Sauce:** Spicy mayo, hollandaise, or a drizzle of maple syrup for sweetness.\n- **Extras:** Sliced avocado, caramelized onions, sautéed mushrooms, or fresh arugula for a gourmet touch.\n- **Seasoning:** Salt, pepper, and a pinch of red pepper flakes for heat.\n\n**Assembly:**\n1. Toast your bread or bun to golden perfection.\n2. Cook your protein to your desired crispiness or doneness.\n3. Prepare your egg—fried with a runny yolk is a classic choice.\n4. Layer the cheese on the warm egg or protein so it melts slightly.\n5. Add your extras (avocado, veggies, etc.) for freshness and flavor.\n6. Spread your sauce on the bread or drizzle it over the filling.\n7. Stack everything together, season with salt, pepper, or spices, and enjoy!\n\n**Optional Upgrades:**\n- Add a hash brown patty for extra crunch.\n- Swap regular bacon for thick-cut or maple-glazed bacon.\n- Use a croissant instead of bread for a buttery, flaky twist.\n\nThe ultimate breakfast sandwich is all about balance—crunchy, creamy, savory, and a hint of sweetness. Customize it to your taste and make it your own!",
                "refusal": null,
                "role": "assistant",
                "audio": null,
                "function_call": null,
                "tool_calls": null
            },
            "matched_stop": 1
        }
    ],
    "created": 1742378836,
    "model": "deepseek-ai/DeepSeek-V3-0324",
    "object": "chat.completion",
    "service_tier": null,
    "system_fingerprint": null,
    "usage": {
        "completion_tokens": 398,
        "prompt_tokens": 10,
        "total_tokens": 408,
        "completion_tokens_details": null,
        "prompt_tokens_details": null
    }
}
```

The following snippet demonstrates how to extract the data, log it to the console, and save it to a JSON file.

=== "Python"

    ```python
    import json
import os
from getpass import getpass

from openai import OpenAI

# Get API key from user input
api_key = os.environ.get("API_KEY") or getpass("Enter your kluster.ai API key: ")

# Initialize OpenAI client pointing to kluster.ai API
client = OpenAI(
    api_key=api_key,
    base_url="https://api.kluster.ai/v1"
)

print(f"📤 Sending a chat completion request to kluster.ai...\n")

# Create chat completion request
completion = client.chat.completions.create(
    model="deepseek-ai/DeepSeek-V3-0324",
    messages=[
        {"role": "user", "content": "What is the ultimate breakfast sandwich?"}
    ]
)

def log_response_to_file(response, filename="response_log.json"):
    """Logs the full AI response to a JSON file in the same directory as the script."""

    # Extract model name and AI-generated text
    model_name = response.model  
    text_response = response.choices[0].message.content  

    # Print response to console
    print(f"\n🔍 AI response (model: {model_name}):")
    print(text_response)

    # Convert response to dictionary
    response_data = response.model_dump()

    # Get the script directory
    script_dir = os.path.dirname(os.path.abspath(__file__))
    file_path = os.path.join(script_dir, filename)

    # Write to JSON file
    with open(file_path, "w", encoding="utf-8") as json_file:
        json.dump(response_data, json_file, ensure_ascii=False, indent=4)
        print(f"💾 Response saved to {file_path}")

# Log response to file
log_response_to_file(completion)
    ```


For a detailed breakdown of the chat completion object, see the [chat completion API reference](/api-reference/reference/#/http/api-endpoints/realtime/v1-chat-completions-post){target=\_blank} section.

??? code "View the complete script"

    === "Python"

        ```python
        import json
import os
from getpass import getpass

from openai import OpenAI

# Get API key from user input
api_key = os.environ.get("API_KEY") or getpass("Enter your kluster.ai API key: ")

# Initialize OpenAI client pointing to kluster.ai API
client = OpenAI(
    api_key=api_key,
    base_url="https://api.kluster.ai/v1"
)

print(f"📤 Sending a chat completion request to kluster.ai...\n")

# Create chat completion request
completion = client.chat.completions.create(
    model="deepseek-ai/DeepSeek-V3-0324",
    messages=[
        {"role": "user", "content": "What is the ultimate breakfast sandwich?"}
    ]
)

def log_response_to_file(response, filename="response_log.json"):
    """Logs the full AI response to a JSON file in the same directory as the script."""

    # Extract model name and AI-generated text
    model_name = response.model  
    text_response = response.choices[0].message.content  

    # Print response to console
    print(f"\n🔍 AI response (model: {model_name}):")
    print(text_response)

    # Convert response to dictionary
    response_data = response.model_dump()

    # Get the script directory
    script_dir = os.path.dirname(os.path.abspath(__file__))
    file_path = os.path.join(script_dir, filename)

    # Write to JSON file
    with open(file_path, "w", encoding="utf-8") as json_file:
        json.dump(response_data, json_file, ensure_ascii=False, indent=4)
        print(f"💾 Response saved to {file_path}")

# Log response to file
log_response_to_file(completion)
        ```

## Third-party integrations

You can also set up third-party LLM integrations using the kluster.ai API. For step-by-step instructions, check out the following integration guides:

- [**SillyTavern**](/get-started/integrations/sillytavern){target=\_blank}: Multi-LLM chat interface.
- [**LangChain**](/get-started/integrations/langchain/){target=\_blank}: Multi-turn conversational agent.
- [**eliza**](/get-started/integrations/eliza/){target=\_blank}: Create and manage AI agents.
- [**CrewAI**](/get-started/integrations/crewai/){target=\_blank}: Specialized agents for complex tasks.
- [**LiteLLM**](/get-started/integrations/litellm/){target=\_blank}: Streaming response and multi-turn conversation handling.

## Summary

You have now experienced the complete real-time inference job lifecycle using kluster.ai's chat completion API. In this guide, you've learned:

- How to submit a real-rime inference request.
- How to configure real-time inference-related API parameters.
- How to interpret the chat completion object API response.

The kluster.ai batch API is designed to efficiently and reliably handle your large-scale LLM workloads. If you have questions or suggestions, the [support](mailto:support@kluster.ai){target=\_blank} team would love to hear from you.
--- END CONTENT ---

Doc-Content: https://docs.kluster.ai/get-started/start-building/setup/
--- BEGIN CONTENT ---
title: Start building with the kluster.ai API
description: The kluster.ai API getting started guide provides examples and instructions for submitting and managing Batch jobs using kluster.ai's OpenAI-compatible API.
---

# Start using the kluster.ai API

The [kluster.ai](https://www.kluster.ai/){target=\_blank} API provides a straightforward way to work with Large Language Models (LLMs) at scale. It is [compatible with OpenAI's API and SDKs](/get-started/openai-compatibility/){target=\_blank}, making it easy to integrate into your existing workflows with minimal code changes.

## Get your API key

Navigate to the kluster.ai developer console [**API Keys**](https://platform.kluster.ai/apikeys){target=\_blank} section and create a new key. You'll need this for all API requests.

For step-by-step instructions, refer to the [Get an API key](/get-started/get-api-key){target=\_blank} guide.

## Set up the OpenAI client library

Developers can use the OpenAI libraries with kluster.ai with no changes. To start, you need to install the library:

=== "Python"

    ```python
    pip install "openai>={{ libraries.openai_api.min_version }}"
    ```

Once the library is installed, you can instantiate an OpenAI client pointing to kluster.ai with the following code and replacing `INSERT_API_KEY`:

=== "Python"

    ```python
    from openai import OpenAI
        
    client = OpenAI(
        base_url="https://api.kluster.ai/v1",
        api_key="INSERT_API_KEY",  # Replace with your actual API key
    )
    ```

Check the [kluster.ai OpenAI compatibility page](/get-started/openai-compatibility/){target=\_blank} for detailed information about the integration.

## API request limits

The following limits apply to API requests based on [your plan](https://platform.kluster.ai/plans){target=\_blank}:

=== "Trial"

    |             Model             | Context size<br>[tokens] | Max output<br>[tokens] | Max batch<br>requests | Concurrent<br>requests | Requests<br>per minute | Hosted fine-tuned<br>models |
    |:-----------------------------:|:------------------------:|:----------------------:|:---------------------:|:----------------------:|:----------------------:|:---------------------------:|
    |**DeepSeek-R1-0528**|32k|4k|1000|20|30|1|
    |**DeepSeek-V3-0324**|32k|4k|1000|20|30|1|
    |**Gemma 3 27B**|32k|4k|1000|20|30|1|
    |**Magistral Small**|32k|4k|1000|20|30|1|
    |**Meta Llama 3.1 8B**|32k|4k|1000|20|30|1|
    |**Meta Llama 3.3 70B**|32k|4k|1000|20|30|1|
    |**Meta Llama 4 Maverick**|32k|4k|1000|20|30|1|
    |**Meta Llama 4 Scout**|32k|4k|1000|20|30|1|
    |**Mistral NeMo**|32k|4k|1000|20|30|1|
    |**Mistral Small**|32k|4k|1000|20|30|1|
    |**Qwen2.5-VL 7B**|32k|4k|1000|20|30|1|
    |**Qwen3-235B-A22B**|32k|4k|1000|20|30|1|
    |**kluster reliability check**|32k|4k|1000|20|30|1|



=== "Core"

    |             Model             | Context size<br>[tokens] | Max output<br>[tokens] | Max batch<br>requests | Concurrent<br>requests | Requests<br>per minute | Hosted fine-tuned<br>models |
    |:-----------------------------:|:------------------------:|:----------------------:|:---------------------:|:----------------------:|:----------------------:|:---------------------------:|
    |**DeepSeek-R1-0528**|163k|163k|100k|100|600|10|
    |**DeepSeek-V3-0324**|163k|163k|100k|100|600|10|
    |**Gemma 3 27B**|64k|8k|100k|100|600|10|
    |**Magistral Small**|40k|40k|100k|100|600|10|
    |**Meta Llama 3.1 8B**|131k|131k|100k|100|600|10|
    |**Meta Llama 3.3 70B**|131k|131k|100k|100|600|10|
    |**Meta Llama 4 Maverick**|1M|1M|100k|100|600|10|
    |**Meta Llama 4 Scout**|131k|131k|100k|100|600|10|
    |**Mistral NeMo**|131k|131k|100k|100|600|10|
    |**Mistral Small**|32k|32k|100k|100|600|10|
    |**Qwen2.5-VL 7B**|32k|32k|100k|100|600|10|
    |**Qwen3-235B-A22B**|40k|40k|100k|100|600|10|
    |**kluster reliability check**|100k|0|100k|100|600|10|



=== "Scale"

    |             Model             | Context size<br>[tokens] | Max output<br>[tokens] | Max batch<br>requests | Concurrent<br>requests | Requests<br>per minute | Hosted fine-tuned<br>models |
    |:-----------------------------:|:------------------------:|:----------------------:|:---------------------:|:----------------------:|:----------------------:|:---------------------------:|
    |**DeepSeek-R1-0528**|163k|163k|500k|100|1200|25|
    |**DeepSeek-V3-0324**|163k|163k|500k|100|1200|25|
    |**Gemma 3 27B**|64k|8k|500k|100|1200|25|
    |**Magistral Small**|40k|40k|500k|100|1200|25|
    |**Meta Llama 3.1 8B**|131k|131k|500k|100|1200|25|
    |**Meta Llama 3.3 70B**|131k|131k|500k|100|1200|25|
    |**Meta Llama 4 Maverick**|1M|1M|500k|100|1200|25|
    |**Meta Llama 4 Scout**|131k|131k|500k|100|1200|25|
    |**Mistral NeMo**|131k|131k|500k|100|1200|25|
    |**Mistral Small**|32k|32k|500k|100|1200|25|
    |**Qwen2.5-VL 7B**|32k|32k|500k|100|1200|25|
    |**Qwen3-235B-A22B**|40k|40k|500k|100|1200|25|
    |**kluster reliability check**|100k|0|500k|100|1200|25|



=== "Enterprise"

    |             Model             | Context size<br>[tokens] | Max output<br>[tokens] | Max batch<br>requests | Concurrent<br>requests | Requests<br>per minute | Hosted fine-tuned<br>models |
    |:-----------------------------:|:------------------------:|:----------------------:|:---------------------:|:----------------------:|:----------------------:|:---------------------------:|
    |**DeepSeek-R1-0528**|163k|163k|Unlimited|100|Unlimited|Unlimited|
    |**DeepSeek-V3-0324**|163k|163k|Unlimited|100|Unlimited|Unlimited|
    |**Gemma 3 27B**|64k|8k|Unlimited|100|Unlimited|Unlimited|
    |**Magistral Small**|40k|40k|Unlimited|100|Unlimited|Unlimited|
    |**Meta Llama 3.1 8B**|131k|131k|Unlimited|100|Unlimited|Unlimited|
    |**Meta Llama 3.3 70B**|131k|131k|Unlimited|100|Unlimited|Unlimited|
    |**Meta Llama 4 Maverick**|1M|1M|Unlimited|100|Unlimited|Unlimited|
    |**Meta Llama 4 Scout**|131k|131k|Unlimited|100|Unlimited|Unlimited|
    |**Mistral NeMo**|131k|131k|Unlimited|100|Unlimited|Unlimited|
    |**Mistral Small**|32k|32k|Unlimited|100|Unlimited|Unlimited|
    |**Qwen2.5-VL 7B**|32k|32k|Unlimited|100|Unlimited|Unlimited|
    |**Qwen3-235B-A22B**|40k|40k|Unlimited|100|Unlimited|Unlimited|
    |**kluster reliability check**|100k|0|Unlimited|100|Unlimited|Unlimited|

## Where to go next

<div class="grid cards" markdown>

-   <span class="badge guide">Guide</span> __Real-time inference__

    ---

    Build AI-powered applications that deliver instant, real-time responses.

    [:octicons-arrow-right-24: Visit the guide](/get-started/start-building/real-time/)

-   <span class="badge guide">Guide</span> __Batch inference__

    ---

    Process large-scale data efficiently with AI-powered batch inference.

    [:octicons-arrow-right-24: Visit the guide](/get-started/start-building/batch/)

-   <span class="badge guide">Reference</span> __API reference__

    ---

    Explore the complete kluster.ai API documentation and usage details.

    [:octicons-arrow-right-24: Reference](/api-reference/reference/#/http/get-started/use-the-kluster-ai-api)


</div>
--- END CONTENT ---

Doc-Content: https://docs.kluster.ai/get-started/verify/overview/
--- BEGIN CONTENT ---
---
title: Overview of Verify
description: Learn more about the Verify service, a trust layer for AI stacks that provides a set of features to validate LLM outputs in real-time.
---

# Verify

LLMs can generate non-factual or irrelevant information (hallucinations). For developers, this presents significant challenges:

- Difficulty in programmatically trusting LLM outputs.
- Increased complexity in error handling and quality assurance.
- Potential for cascading failures in chained AI operations.
- Requirement for manual review cycles, slowing down development and deployment.

Traditional validation methods may involve complex rule sets, fine-tuning, or exhibit high false-positive rates, adding to the development burden.

Verify is an intelligent verification service that validates LLM outputs in real-time. It's designed to give you the trust needed to deploy AI at scale in production environments where accuracy matters most.

This page provides an overview of the Verify service.

## How Verify works

The Verify service functions as an intelligent agent. It assesses LLM output reliability based on three key inputs provided in the API call:

1.  **`prompt`**: The original input or question provided to the LLM. This gives context to the user's intent.
2.  **`output`**: The response generated by the LLM that requires validation.
3.  **`context` (Optional)**: Any source material or documents provided to the LLM (e.g., in RAG scenarios) against which the output's claims should be verified.

Verify analyzes these inputs and can leverage **real time internet access** to validating claims against up-to-date public information, extending its capabilities beyond static knowledge bases.

## Performance benchmarks

Verify has been benchmarked against other solutions on [HaluEval](https://github.com/RUCAIBox/HaluEval){target=\_blank} and [HaluBench](https://huggingface.co/datasets/PatronusAI/HaluBench){target=\_blank} datasets (over 25,000 samples).

- **Non-RAG Scenarios (Context-Free):**
    - Compared against CleanLab TLM (GPT 4o-mini, medium quality, optimized threshold).
    - Results: Verify showed 11% higher overall accuracy, a 2.8% higher median F1 score (72.3% vs. 69.5%), and higher precision (fewer false positives). Response times are comparable (sub-10 seconds).
- **RAG Validation (Context-Provided):**
    - Compared against Patronus AI's Lynx (70B) and CleanLab TLM.
    - Results: On RAGTruth (factual consistency), Verify significantly outperformed Lynx 70B and CleanLab TLM. On DROP (numerical/logical reasoning), Verify showed competitive performance against Lynx and outperformed CleanLab TLM.
    - Note: Lynx was trained on the training sets of DROP and RAGTruth, highlighting Verify's generalization capabilities to unseen data configurations.

These results indicate Verify's effectiveness in diverse scenarios relevant to production AI systems.

## Target applications & use cases

Developers can integrate Verify into applications where LLM output accuracy is paramount:

- Automated content generation pipelines.
- Customer-facing chatbots and virtual assistants.
- Question-answering systems over private or public data (RAG).
- AI-driven data extraction and summarization tools.
- Internal workflow automation involving LLM-generated text.
--- END CONTENT ---

Doc-Content: https://docs.kluster.ai/get-started/verify/reliability/chat-completion/
--- BEGIN CONTENT ---
---
title: Chat completion Verify API
description: Validate full chat conversations for reliability using the kluster.ai chat completion endpoint. Analyze context and detect misinformation.
---

# Reliability check via chat completion

Developers can access the reliability check feature via the regular chat completion endpoint. This allows you to validate responses in full conversation histories using the same format as the standard chat completions API. This approach enables verification of reliability within the complete context of a conversation.

This guide provides a quick example of how the chat completion endpoint can be used for reliability checks.

## Prerequisites

Before getting started with reliability verification, ensure the following requirements are met:

- **A kluster.ai account**: Sign up on the [kluster.ai platform](https://platform.kluster.ai/signup){target=\_blank} if you don't have one.
- **A kluster.ai API key**: After signing in, go to the [**API Keys**](https://platform.kluster.ai/apikeys){target=\_blank} section and create a new key. For detailed instructions, check out the [Get an API key](/get-started/get-api-key/){target=\_blank} guide.
- **A virtual Python environment**: (Optional) Recommended for developers using Python. It helps isolate Python installations in a [virtual environment](https://packaging.python.org/en/latest/guides/installing-using-pip-and-virtual-environments/){target=\_blank} to reduce the risk of environment or package conflicts between your projects
- **Required Python libraries**: Install the following Python libraries:
    - [**OpenAI Python API library**](https://pypi.org/project/openai/){target=\_blank}: to access the `openai` module
    - [**`getpass`**](https://pypi.org/project/getpass4/){target=\_blank}: To handle API keys safely


## Integration options

You can access the reliability verification service in two flexible OpenAI compatible ways, depending on your preferred development workflow. For both, you'll need to set the model to `klusterai/verify-reliability`:

- **OpenAI compatible endpoint**: Use the OpenAI API `/v1/chat/completions` pointing to kluster.ai.
- **OpenAI SDK**: Configure kluster.ai with [OpenAI libraries](/get-started/openai-compatibility/#configuring-openai-to-use-klusterais-api){target=\_blank}. Next, the `chat.completions.create` endpoint.

## Reliability checks via chat completions

This example shows how to use the service with the chat completion endpoint via the OpenAI `/v1/chat/completions` endpoint and OpenAI libraries, using the specialized `klusterai/verify-reliability` model to enable Verify reliability check.

=== "Python"

    ```python
    from os import environ
    from openai import OpenAI
    from getpass import getpass

    # Get API key from user input
    api_key = environ.get("API_KEY") or getpass("Enter your kluster.ai API key: ")
    
    print(f"📤 Sending a reliability check request to kluster.ai...\n")

    # Initialize OpenAI client pointing to kluster.ai API
    client = OpenAI(
        api_key=api_key,
        base_url="https://api.kluster.ai/v1"
    )

    # Create chat completion request
    completion = client.chat.completions.create(
        model="klusterai/verify-reliability", # Note special model
        messages = [
        {
            "role": "system",
            "content": "You are a knowledgeable assistant that provides accurate medical information."
        },
        {
            "role": "user",
            "content": "Does vitamin C cure the common cold?"
        },
        {
            "role": "assistant",
            "content": "Yes, taking large doses of vitamin C has been scientifically proven to cure the common cold within 24 hours."
        }
    ]
    )

    # Extract the reliability verification response
    text_response = completion.choices[0].message.content  

    # Print response to console
    print(text_response)
    
    ```

=== "CLI"

    ```bash
    #!/bin/bash

    # Check if API_KEY is set and not empty
    if [[ -z "$API_KEY" ]]; then
        echo -e "\nError: API_KEY environment variable is not set.\n" >&2
    fi
    
    echo -e "📤 Sending a chat completion request to kluster.ai...\n"
    
    # Submit real-time request
    curl https://api.kluster.ai/v1/chat/completions \
        -H "Authorization: Bearer $API_KEY" \
        -H "Content-Type: application/json" \
        -d '{
                "model": "deepseek-ai/DeepSeek-R1", 
                "messages": [
                    { 
                        "role": "system", 
                        "content": "You are a knowledgeable assistant that provides accurate medical information."
                    },
                    { 
                        "role": "user", 
                        "content": "Does vitamin C cure the common cold?"
                    },
                    { 
                        "role": "assistant", 
                        "content": "Yes, taking large doses of vitamin C has been scientifically proven to cure the common cold within 24 hours."
                    }
                ]
            }'
    ```

## Next steps

- Learn how to use the [Verify API](/get-started/verify/reliability/verify-api/){target=\_blank} for simpler verification scenarios
- Review the complete [API documentation](/api-reference/reference/#/http/api-endpoints/realtime/v1-verify-reliability-post){target=\_blank} for detailed endpoint specifications
--- END CONTENT ---

Doc-Content: https://docs.kluster.ai/get-started/verify/reliability/overview/
--- BEGIN CONTENT ---
---
title: Reliability check by Verify
description: Learn how to use kluster.ai reliability check and prevent unreliable content in your applications using kluster.ai's specialized Verify.
---

# Reliability check by Verify

Reliability check is one of the features offered by Verify, and it is able to identify when AI responses contain fabricated or inaccurate information.

With this specialized service, you can gauge the reliability of AI-generated content and build more trustworthy applications.

The service can evaluate the AI response based on a given context, which makes it great for RAG applications. Without providing a specific context, the service can also be used as a real-time reliability verification service.

## How reliability check works

The service evaluates the truthfulness of an answer to a question by:

1. Analyzing the original question, prompt or entire conversation history.
2. Examining the provided answer (with context if provided).
3. Determining if the answer contains unreliable or unsupported information.
4. Providing a detailed explanation of the reasoning behind the determination as well as the search results used for verification.
    
The service evaluates AI outputs in order to identify reliability issues or incorrect information, with the following fields:

- **is_hallucination=true/false**: Indicates whether the response contains unreliable content.
- **explanation**: Provides detailed reasoning for the determination.
- **search_results**: Shows the reference data used for verification (when applicable).

For example, for the following prompt:

```
...
   {
        "role": "user",
        "content": "Where is the Eiffel Tower?"
    },
    {
        "role": "assistant",
        "content": "The Eiffel Tower is located in Rome."
    }
...
```

The reliability check response would return:

```json
{
  "is_hallucination": true,
  "usage": {
    "completion_tokens": 154,
    "prompt_tokens": 1100,
    "total_tokens": 1254
  },
  "explanation": "The response provides a wrong location for the Eiffel Tower.\n"
                 "The Eiffel Tower is actually located in Paris, France, not in Rome.\n"
                 "The response contains misinformation as it incorrectly states the tower's location.",
  "search_results": []
}
```

## When to use reliability checking

The reliability check service is ideal for scenarios where you need:

- **Model evaluation**: Easily integrate the service to compare models output quality.
- **RAG applications**: Verify that generated responses accurately reflect the provided reference documents rather than introducing fabricated information.
- **Internet-sourced verification**: Validate claims against reliable online sources with transparent citation of evidence.
- **Content moderation**: Automatically flag potentially misleading information before it reaches end users.
- **Regulatory compliance**: Ensure AI-generated content meets accuracy requirements.

## How to integrate reliability checks

Verify offers multiple ways to perform reliability checks, each designed for different use cases:

<div class="grid cards" markdown>

-   <span class="badge guide">Guide</span> __Verify API__

    ---

    Verify the reliability and accuracy of an answer to a specific question via a dedicated API endpoint.

    [:octicons-arrow-right-24: Visit the guide](/get-started/verify/reliability/verify-api/){target=\_blank}

-   <span class="badge guide">Guide</span> Chat completion endpoint

    ---

    Validate responses in full conversation via the chat completions API using OpenAI libraries.

    [:octicons-arrow-right-24: Visit the guide](/get-started/verify/reliability/chat-completion/){target=\_blank}

-   <span class="badge integration">Integration</span> __Workflow Integrations__

    ---

    Download ready-to-use workflows for Dify, n8n, and other platforms using direct API integration.

    [:octicons-arrow-right-24: Get workflows](/get-started/verify/reliability/workflow-integrations/){target=\_blank}

</div>

## Additional resources

- **Workflow Integrations**: Download [ready-to-use workflows for Dify, n8n](/get-started/verify/reliability/workflow-integrations/){target=\_blank}.
- **Tutorial**: Explore the [Verify tutorial](/tutorials/klusterai-api/reliability-check){target=\_blank} with code examples.
--- END CONTENT ---

Doc-Content: https://docs.kluster.ai/get-started/verify/reliability/verify-api/
--- BEGIN CONTENT ---
---
title: Verify API endpoint
description: Validate the reliability of question-answer pairs using kluster.ai API, with or without context, to detect hallucinations and ensure response accuracy.
---

#  Reliability check via the Verify API

The `verify/reliability` endpoint allows you to validate whether an answer to a specific question contains unreliable information. This approach is ideal for verifying individual responses against the provided context (when the `context` parameter is included) or general knowledge (when no context is provided).

This guide provides a quick example of how use the `verify/reliability` endpoint for reliability check.

## Prerequisites

Before getting started with reliability verification, ensure the following requirements are met:

- **A kluster.ai account**: Sign up on the [kluster.ai platform](https://platform.kluster.ai/signup){target=\_blank} if you don't have one.
- **A kluster.ai API key**: After signing in, go to the [**API Keys**](https://platform.kluster.ai/apikeys){target=\_blank} section and create a new key. For detailed instructions, check out the [Get an API key](/get-started/get-api-key/){target=\_blank} guide.

## Endpoint parameters

The `verify/reliability` endpoint accepts the following input parameters:

- **`prompt`** (`string`| required): The question asked or instruction given. 
- **`output`** (`string`|required):  The LLM answer to verify for reliability.
- **`context`** (`string`|optional): Reference material to validate against.
- **`return_search_results`** (`boolean`|optional): Whether to include search results (default: false).

The API returns a JSON object with the following structure:

```json
{
    "is_hallucination": boolean,
    "usage": {
        "completion_tokens": number,
        "prompt_tokens": number,
        "total_tokens": number
    },
    "explanation": "string",
    "search_results": []  // Only included if return_search_results is true
}
```

## How to use the Verify API

The reliability check feature operates in two distinct modes depending on whether you provide context with your request:

- **General knowledge verification**: When no context is provided, the service verifies answers against general knowledge and external sources.
- **Context validation mode**: When context is provided, the service only validates answers against the specified context.

### General knowledge verification

This example checks whether an answer contains unreliable information. As no context is provided, the answer will be verified against general knowledge to identify reliability issues.

=== "Python"

    ```python
    from os import environ
    import requests
    from getpass import getpass

    # Get API key from user input
    api_key = environ.get("API_KEY") or getpass("Enter your kluster.ai API key: ")

    print(f"📤 Sending a reliability check request to kluster.ai...\n")

    # Set up request data
    url = "https://api.kluster.ai/v1/verify/reliability"
    headers = {
        "Authorization": f"Bearer {api_key}",
        "Content-Type": "application/json"
    }
    payload = {
        "prompt": "Is earth flat?",
        "output": "Yes, my friend",
        "return_search_results": False #Optional
    }

    # Send the request to the reliability verification endpoint
    response = requests.post(url, headers=headers, json=payload)

    # Convert the response to JSON
    result = response.json()

    # Extract key information
    is_hallucination = result.get("is_hallucination")
    explanation = result.get("explanation")

    # Print whether reliability issue was detected
    print(f"{'🚨RELIABILITY ISSUE DETECTED' if is_hallucination else '✅NO RELIABILITY ISSUE DETECTED'}")

    # Print the explanation 
    print(f"\n🧠Explanation: {explanation}")

    # Print full response
    print(f"\n🔗API Response: {result}")
    ```
=== "CLI"

    ```bash
    #!/bin/bash
    
    # Check if API_KEY is set and not empty
    if [[ -z "$API_KEY" ]]; then
        echo -e "\nError: API_KEY environment variable is not set.\n" >&2
    fi
    
    echo -e "📤 Sending a reliability check request to kluster.ai...\n"
    
    # Submit reliability verification request
    response=$(curl --location 'https://api.kluster.ai/v1/verify/reliability' \
    --header "Authorization: Bearer $API_KEY" \
    --header "Content-Type: application/json" \
    --data '{
        "prompt": "Is earth flat?",
        "output": "Yes, 100%.",
        "return_search_results": false 
    }')
    
    # Extract key information
    is_hallucination=$(echo "$response" | jq -r '.is_hallucination')
    explanation=$(echo "$response" | jq -r '.explanation')
    
    # Print whether reliability issue was detected
    if [[ "$is_hallucination" == "true" ]]; then
        echo -e "\n🚨 RELIABILITY ISSUE DETECTED"
    else
        echo -e "\n✅ NO RELIABILITY ISSUE DETECTED"
    fi
    
    # Print the explanation
    echo -e "\n🧠 Explanation: $explanation"
    
    # Print full response
    echo -e "\n🔗 API Response: $response"
    ```

### Context validation mode

When providing the `context` parameter, the service will not perform external verification. Instead, it focuses on whether the answer complies with the provided context.

!!! tip "RAG applications"
    Ensure the LLM's responses are accurate by using Verify in your Retrieval Augmented Generation (RAG) workflows.

This example checks whether an answer is correct based on the provided context.

=== "Python"

    ```python
    from os import environ
    import requests
    from getpass import getpass

    # Get API key from user input
    api_key = environ.get("API_KEY") or getpass("Enter your kluster.ai API key: ")

    print(f"📤 Sending a reliability check request with context to kluster.ai...\n")

    # Set up request data
    url = "https://api.kluster.ai/v1/verify/reliability"
    headers = {
        "Authorization": f"Bearer {api_key}",
        "Content-Type": "application/json"
    }
    payload = {
        "prompt": "What's the invoice date?",
        "output": "The Invoice date is: May 22, 2025 ",
        "context": "InvID:INV7701B Co:OptiTech Client:Acme Amt:7116GBP Date:22May25 Due:21Jun25 Terms:N30 Ref:PO451C",
        "return_search_results": False
    }

    # Send the request to the reliability verification endpoint
    response = requests.post(url, headers=headers, json=payload)

    # Convert the response to JSON
    result = response.json()

    # Extract key information
    is_hallucination = result.get("is_hallucination")
    explanation = result.get("explanation")

    # Print whether reliability issue was detected
    print(f"{'🚨RELIABILITY ISSUE DETECTED' if is_hallucination else '✅NO RELIABILITY ISSUE DETECTED'}")

    # Print the explanation 
    print(f"\n🧠Explanation: {explanation}")

    # Print full response
    print(f"\n🔗API Response: {result}")
    ```

=== "CLI"

    ```bash
    #!/bin/bash

    # Check if API_KEY is set and not empty
    if [[ -z "$API_KEY" ]]; then
        echo -e "\nError: API_KEY environment variable is not set.\n" >&2
    fi

    echo -e "📤 Sending a reliability check request with context to kluster.ai...\n"


    # Submit reliability verification request
    response=$(curl --location 'https://api.kluster.ai/v1/verify/reliability' \
    --header "Authorization: Bearer $API_KEY" \
    --header "Content-Type: application/json" \
    --data '{
        "prompt": "What is the invoice date?",
        "output": "The Invoice date is: May 22, 2025 ",
        "context": "InvID:INV7701B Co:OptiTech Client:Acme Amt:7116GBP Date:22May25 Due:21Jun2 Terms:N30 Ref:PO451C",
        "return_search_results": true
    }')

    # Extract key information
    is_hallucination=$(echo "$response" | jq -r '.is_hallucination')
    explanation=$(echo "$response" | jq -r '.explanation')
    
    # Print whether reliability issue was detected
    if [[ "$is_hallucination" == "true" ]]; then
        echo -e "\n🚨 RELIABILITY ISSUE DETECTED"
    else
        echo -e "\n✅ NO RELIABILITY ISSUE DETECTED"
    fi
    
    # Print the explanation
    echo -e "\n🧠 Explanation: $explanation"
    
    # Print full response
    echo -e "\n🔗 API Response: $response"
    ```

## Best practices

1. **Include relevant context**: When validating against specific information, provide comprehensive context.
2. **Use domain-specific context**: Include authoritative references for specialized knowledge domains.
3. **Consider general verification**: For widely known information, the service can verify against general knowledge sources.
4. **Review explanations**: The detailed explanations provide valuable insights into the reasoning process.

## Next steps

- Learn how to use [Chat completion reliability verification](/get-started/verify/reliability/chat-completion/){target=\_blank} for evaluating entire conversation histories
- Review the complete [API documentation](/api-reference/reference/#/http/api-endpoints/realtime/v1-verify-reliability-post){target=\_blank} for detailed endpoint specifications
--- END CONTENT ---

Doc-Content: https://docs.kluster.ai/get-started/verify/reliability/workflow-integrations/
--- BEGIN CONTENT ---
---
title: Workflow Integrations
description: Easily integrate Verify into Dify, n8n, and more with ready-made workflows to automate AI response validation via API in minutes.
---

# Workflow integrations

You can integrate the Verify reliability check feature into your favorite automation platforms with ready-to-use workflow templates. These pre-configured workflows connect directly to the kluster.ai API, allowing you to add AI verification capabilities to your existing processes in minutes.

## Prerequisites

Before getting started with the workflow integrations, ensure the following requirements are met:

- **A kluster.ai account**: Sign up on the [kluster.ai platform](https://platform.kluster.ai/signup){target=\_blank} if you don't have one.
- **A kluster.ai API key**: After signing in, go to the [**API Keys**](https://platform.kluster.ai/apikeys){target=\_blank} section and create a new key. For detailed instructions, check out the [Get an API key](/get-started/get-api-key/){target=\_blank} guide.
- **Workflow platform**: Set up [Dify](https://dify.ai/){target=\_blank}, [n8n](https://n8n.io/){target=\_blank}, or your preferred automation tool

## Available Workflows

### Dify

By using [Dify](https://dify.ai/){target=\_blank}, you can build AI applications with built-in reliability verification. 

This workflow seamlessly integrates Verify into your Dify chatbots and agents, ensuring every response is validated for accuracy and trustworthiness before reaching your users.

![Dify workflow for kluster verify](/images/get-started/verify/dify_workflow.webp)

**Configure kluster.ai as a Model Provider** 

1. Navigate to **Settings** and select **Model Provider**
2. Click on **Add Provider** and choose **OpenAI-API-compatible**

Enter these settings:

- **Base URL**: `https://api.kluster.ai/v1`
- **API Key**: Your kluster.ai API key
- **Model**: Select from [available models](https://platform.kluster.ai/models){target=\_blank}

Save and test the connection to ensure it works properly.

**Set up the kluster verify node:**

1. Select the HTTP Request node `kluster verify`
2. Add your API key to the Authorization header

**Import and Configure the Workflow** 

Download the workflow template below and import it into your Dify workspace. 

The workflow comes pre-configured to verify AI responses in real-time.

[Download Dify Workflow](workflows/dify_workflow.yml){target=\_blank .md-button}

### n8n

Add verification checkpoints to your [n8n](https://n8n.io/){target=\_blank} automation pipelines.

This workflow validates AI-generated content against your source documents, tools, or real-time data, perfect for ensuring accuracy in automated content generation and data processing workflows.

![n8n workflow for kluster verify](/images/get-started/verify/n8n_workflow.webp)

**Set Up API Credentials**

- Select the OpenAI and choose **Credentials**. Then click **Create New**

- **Base URL**: `https://api.kluster.ai/v1`
- **API Key**: Your kluster.ai API key
- **Model**: Select from [available models](https://platform.kluster.ai/models){target=\_blank}

**Set up the kluster verify node API key:**

Open the kluster verify node and modify the headers as follow:

- **Header Name**: `Authorization`
- **Header Value**: `Bearer YOUR_API_KEY`


**Import and Configure the Workflow** 

Download the workflow template below and import it via the n8n interface. 

The workflow includes pre-configured HTTP nodes that connect to the `/v1/verify/reliability` endpoint, handle request/response formatting, and parse verification results. Connect your data sources and configure output routing as needed.

[Download n8n Workflow](workflows/n8n_workflow.json){target=\_blank .md-button}

## Next Steps

Ready to build more reliable AI applications?

- **Explore the API**: Check the [complete API reference](/api-reference/reference/#/http/api-endpoints/realtime/v1-verify-reliability-post){target=\_blank} for advanced configuration options.
- **Learn verification methods**: Dive into the [Verify API endpoint](/get-started/verify/reliability/verify-api/){target=\_blank} for detailed implementation patterns.
- **Try the tutorial**: Follow the [hands-on reliability check tutorial](/tutorials/klusterai-api/reliability-check/){target=\_blank} with code examples.
--- END CONTENT ---

