{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be189fde-4e5b-4f80-bae1-ded86a5075a7",
   "metadata": {
    "id": "be189fde-4e5b-4f80-bae1-ded86a5075a7"
   },
   "source": [
    "# Keyword extraction with kluster.ai API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b17a77d9",
   "metadata": {},
   "source": [
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/kluster-ai/klusterai-cookbook/blob/main/examples/keyword-extraction-api.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d1d06ea-79c1-4f28-b312-0e5aabe18ff3",
   "metadata": {
    "id": "6d1d06ea-79c1-4f28-b312-0e5aabe18ff3"
   },
   "source": [
    "Welcome to the keyword extraction notebook powered by the kluster.ai Batch API!\n",
    "\n",
    "In this notebook, we’ll demonstrate how to leverage the <a href=\"https://kluster.ai/\" target=\"_blank\">kluster.ai</a> Batch API and the Llama 70B Large Language Model (LLM) to identify keywords in a given dataset. By using an extract from the AG News dataset as an example, we’ll show you how to extract keywords from the dataset. You can easily modify this example for your own use case and data format. Our solution efficiently processes text of any size, small text samples to enterprise-scale datasets.\n",
    "\n",
    "To get started, simply input your API key and execute the preloaded cells to perform the keyword extraction. If you don’t have an API key, you can register for free <a href=\"https://platform.kluster.ai/signup\" target=\"_blank\">on our platform</a>.\n",
    "\n",
    "Let’s dive in!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "xU1WBQJ7Uh09",
   "metadata": {
    "id": "xU1WBQJ7Uh09"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d580f8-67d5-45d6-915f-0f6b60d3b543",
   "metadata": {},
   "source": [
    "Input your unique kluster.ai API key. If you haven’t obtained one yet, don’t forget to <a href=\"https://platform.kluster.ai/signup\" target=\"_blank\">sign up</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4911e5eb-7463-4a6b-8a99-9313cecd9d4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter your kluster.ai API key:  ········\n"
     ]
    }
   ],
   "source": [
    "from getpass import getpass\n",
    "\n",
    "api_key = getpass(\"Enter your kluster.ai API key: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bcc3d475-8f49-4fc4-9a5e-c6eb6866d2b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b89a4feb-37a9-430d-a742-cd58495b4eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "import pandas as pd\n",
    "import time\n",
    "import json\n",
    "from IPython.display import clear_output, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "zG9y_WO5rYaj",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the client\n",
    "client = OpenAI(\n",
    "    base_url=\"https://api.kluster.ai/v1\",\n",
    "    api_key=api_key,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "udPtLfTaisSw",
   "metadata": {
    "id": "udPtLfTaisSw"
   },
   "source": [
    "## Get the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "QjCVfg65jKz6",
   "metadata": {
    "id": "QjCVfg65jKz6"
   },
   "source": [
    "This notebook comes with a preloaded sample dataset based on the AG News dataset. It includes excerpts of news headlines and their leads, all set for processing. There’s no extra setup required—just move on to the next steps to start working with the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "07018f92-9a01-47d5-916a-12cd03dfa3a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\n",
    "    \"text\": [\n",
    "        \"Chorus Frog Found Croaking in Virginia - The Southern chorus frog has been found in southeastern Virginia, far outside its previously known range. The animal had never before been reported north of Beaufort County, N.C., about 125 miles to the south.\",\n",
    "        \"Expedition to Probe Gulf of Mexico - Scientists will use advanced technology never before deployed beneath the sea as they try to discover new creatures, behaviors and phenomena in a 10-day expedition to the Gulf of Mexico's deepest reaches.\",\n",
    "        \"Feds Accused of Exaggerating Fire ImpactP - The Forest Service exaggerated the effect of wildfires on California spotted owls in justifying a planned increase in logging in the Sierra Nevada, according to a longtime agency expert who worked on the plan.\",\n",
    "        \"New Method May Predict Quakes Weeks Ahead - Swedish geologists may have found a way to predict earthquakes weeks before they happen by monitoring the amount of metals like zinc and copper in subsoil water near earthquake sites, scientists said Wednesday.\",\n",
    "        \"Marine Expedition Finds New Species - Norwegian scientists who explored the deep waters of the Atlantic Ocean said Thursday their findings #151; including what appear to be new species of fish and squid #151; could be used to protect marine ecosystems worldwide.\"\n",
    "    ]\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "OyGuHllZllct",
   "metadata": {
    "id": "OyGuHllZllct"
   },
   "source": [
    "## Batch inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64c345aa-b6a7-4770-8368-b290e9e799dc",
   "metadata": {
    "id": "6-MZlfXAoiNv"
   },
   "source": [
    "To run the inference job, we’ll follow three simple steps:\n",
    "1. **Create the batch input file -** we’ll create a file containing the requests to be processed by the model.\n",
    "2. **Upload the batch input file to kluster.ai -** once the file is ready, we’ll upload it to the kluster.ai platform using the API, where it will be queued for processing.\n",
    "3. **Start the job -** after the upload, we’ll trigger the job to process the data.\n",
    "\n",
    "Everything is preconfigured for you—just execute the cells below to see it all in action!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Ew-R24Ltp5EW",
   "metadata": {
    "id": "Ew-R24Ltp5EW"
   },
   "source": [
    "### Create the Batch file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "qS4JXT52wGJ-",
   "metadata": {
    "id": "qS4JXT52wGJ-"
   },
   "source": [
    "This example uses the `klusterai/Meta-Llama-3.3-70B-Instruct-Turbo` model. If you’d prefer to use a different model, you can easily modify the model name in the next cell. For a full list of supported models, please check our <a href=\"https://docs.kluster.ai/getting-started/#list-supported-models\" target=\"_blank\">documentation</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fVtwyqZ_nEq7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_inference_file(df):\n",
    "    inference_list = []\n",
    "    for index, row in df.iterrows():\n",
    "        content = row['text']\n",
    "        \n",
    "        request = {\n",
    "            \"custom_id\": f\"keyword_extraction-{index}\",\n",
    "            \"method\": \"POST\",\n",
    "            \"url\": \"/v1/chat/completions\",\n",
    "            \"body\": {\n",
    "                \"model\": \"klusterai/Meta-Llama-3.3-70B-Instruct-Turbo\",\n",
    "                \"temperature\": 0.5,\n",
    "                \"messages\": [\n",
    "                    {\"role\": \"system\", \"content\": 'Extract up to 5 relevant keywords from the given text. Provide only the keywords between double quotes and separated by commas.'},\n",
    "                    {\"role\": \"user\", \"content\": content}\n",
    "                ],\n",
    "            }\n",
    "        }\n",
    "        inference_list.append(request)\n",
    "    return inference_list\n",
    "\n",
    "def save_inference_file(inference_list):\n",
    "    filename = f\"keyword_extraction_inference_request.jsonl\"\n",
    "    with open(filename, 'w') as file:\n",
    "        for request in inference_list:\n",
    "            file.write(json.dumps(request) + '\\n')\n",
    "    return filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "qNhmrmHdnp7g",
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_list = create_inference_file(df)\n",
    "filename = save_inference_file(inference_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada26fe3-acb9-48dc-b368-b57fc380cdb8",
   "metadata": {},
   "source": [
    "Let’s preview what that request file looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d65f5099-5add-4749-9a85-3c04a9b342bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"custom_id\": \"keyword_extraction-0\", \"method\": \"POST\", \"url\": \"/v1/chat/completions\", \"body\": {\"model\": \"klusterai/Meta-Llama-3.3-70B-Instruct-Turbo\", \"temperature\": 0.5, \"messages\": [{\"role\": \"system\", \"content\": \"Extract up to 5 relevant keywords from the given text. Provide only the keywords between double quotes and separated by commas.\"}, {\"role\": \"user\", \"content\": \"Chorus Frog Found Croaking in Virginia - The Southern chorus frog has been found in southeastern Virginia, far outside its previously known range. The animal had never before been reported north of Beaufort County, N.C., about 125 miles to the south.\"}]}}\n"
     ]
    }
   ],
   "source": [
    "!head -n 1 keyword_extraction_inference_request.jsonl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "xArKu7-sqSiR",
   "metadata": {
    "id": "xArKu7-sqSiR"
   },
   "source": [
    "### Upload inference file to kluster.ai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e48b2489-99bc-431b-8cb3-de816550d524",
   "metadata": {},
   "source": [
    "Now that we’ve prepared our input file, it’s time to upload it to the kluster.ai platform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "l5eu5UyAnEtk",
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_input_file = client.files.create(\n",
    "    file=open(filename, \"rb\"),\n",
    "    purpose=\"batch\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6438be35-1e73-4c34-9249-2dd16d102253",
   "metadata": {
    "id": "xArKu7-sqSiR"
   },
   "source": [
    "### Start the job"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "251a0b89-71a9-40d7-bf14-51be935afe10",
   "metadata": {},
   "source": [
    "Once the file has been successfully uploaded, we’re ready to start the inference job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "71a24704-7190-4e24-898f-c4eff062439a",
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_job = client.batches.create(\n",
    "    input_file_id=inference_input_file.id,\n",
    "    endpoint=\"/v1/chat/completions\",\n",
    "    completion_window=\"24h\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fd1d687",
   "metadata": {},
   "source": [
    "All requests are now being processed! "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e-ujphILqepu",
   "metadata": {
    "id": "e-ujphILqepu"
   },
   "source": [
    "### Check job progress"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "iFrDrriQqizC",
   "metadata": {
    "id": "iFrDrriQqizC"
   },
   "source": [
    "In the following section, we’ll monitor the status of each job to see how they’re progressing. Let’s take a look and keep track of their completion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "SuH0CfoqjP3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Job completed!'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def parse_json_objects(data_string):\n",
    "    if isinstance(data_string, bytes):\n",
    "        data_string = data_string.decode('utf-8')\n",
    "\n",
    "    json_strings = data_string.strip().split('\\n')\n",
    "    json_objects = []\n",
    "\n",
    "    for json_str in json_strings:\n",
    "        try:\n",
    "            json_obj = json.loads(json_str)\n",
    "            json_objects.append(json_obj)\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"Error parsing JSON: {e}\")\n",
    "\n",
    "    return json_objects\n",
    "\n",
    "all_completed = False\n",
    "while not all_completed:\n",
    "    all_completed = True\n",
    "    output_lines = []\n",
    "\n",
    "    updated_job = client.batches.retrieve(inference_job.id)\n",
    "\n",
    "    if updated_job.status != \"completed\":\n",
    "        all_completed = False\n",
    "        completed = updated_job.request_counts.completed\n",
    "        total = updated_job.request_counts.total\n",
    "        output_lines.append(f\"Job status: {updated_job.status} - Progress: {completed}/{total}\")\n",
    "    else:\n",
    "        output_lines.append(f\"Job completed!\")\n",
    "\n",
    "    # Clear the output and display updated status\n",
    "    clear_output(wait=True)\n",
    "    for line in output_lines:\n",
    "        display(line)\n",
    "\n",
    "    if not all_completed:\n",
    "        time.sleep(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "TkkhIG9HU0D9",
   "metadata": {
    "id": "TkkhIG9HU0D9"
   },
   "source": [
    "## Get the results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c1f6ac-8d60-4158-9036-de79fa274983",
   "metadata": {},
   "source": [
    "Now that the job is complete, we’ll fetch the results and examine the responses generated for each request."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "806a5eb1-f6d3-491d-b051-9d44bf046a7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " -------------------------- \n",
      "\n",
      "Task ID: keyword_extraction-0. \n",
      "\n",
      "INPUT TEXT: Chorus Frog Found Croaking in Virginia - The Southern chorus frog has been found in southeastern Virginia, far outside its previously known range. The animal had never before been reported north of Beaufort County, N.C., about 125 miles to the south.\n",
      "\n",
      "LLM ANSWER: \"Chorus Frog\", \"Virginia\", \"Southeastern\", \"Beaufort County\", \"North Carolina\"\n",
      "\n",
      " -------------------------- \n",
      "\n",
      "Task ID: keyword_extraction-1. \n",
      "\n",
      "INPUT TEXT: Expedition to Probe Gulf of Mexico - Scientists will use advanced technology never before deployed beneath the sea as they try to discover new creatures, behaviors and phenomena in a 10-day expedition to the Gulf of Mexico's deepest reaches.\n",
      "\n",
      "LLM ANSWER: \"Gulf of Mexico\", \"expedition\", \"scientists\", \"technology\", \"ocean\"\n",
      "\n",
      " -------------------------- \n",
      "\n",
      "Task ID: keyword_extraction-2. \n",
      "\n",
      "INPUT TEXT: Feds Accused of Exaggerating Fire ImpactP - The Forest Service exaggerated the effect of wildfires on California spotted owls in justifying a planned increase in logging in the Sierra Nevada, according to a longtime agency expert who worked on the plan.\n",
      "\n",
      "LLM ANSWER: \"Wildfires\", \"California\", \"Spotted Owls\", \"Logging\", \"Sierra Nevada\"\n",
      "\n",
      " -------------------------- \n",
      "\n",
      "Task ID: keyword_extraction-3. \n",
      "\n",
      "INPUT TEXT: New Method May Predict Quakes Weeks Ahead - Swedish geologists may have found a way to predict earthquakes weeks before they happen by monitoring the amount of metals like zinc and copper in subsoil water near earthquake sites, scientists said Wednesday.\n",
      "\n",
      "LLM ANSWER: \"earthquakes\", \"prediction\", \"geologists\", \"metals\", \"seismology\"\n",
      "\n",
      " -------------------------- \n",
      "\n",
      "Task ID: keyword_extraction-4. \n",
      "\n",
      "INPUT TEXT: Marine Expedition Finds New Species - Norwegian scientists who explored the deep waters of the Atlantic Ocean said Thursday their findings #151; including what appear to be new species of fish and squid #151; could be used to protect marine ecosystems worldwide.\n",
      "\n",
      "LLM ANSWER: \"Marine\", \"Expedition\", \"Species\", \"Atlantic Ocean\", \"Ecosystems\"\n"
     ]
    }
   ],
   "source": [
    "job = client.batches.retrieve(inference_job.id)\n",
    "result_file_id = job.output_file_id\n",
    "result = client.files.content(result_file_id).content\n",
    "results = parse_json_objects(result)\n",
    "\n",
    "for res in results:\n",
    "    task_id = res['custom_id']\n",
    "    index = task_id.split('-')[-1]\n",
    "    result = res['response']['body']['choices'][0]['message']['content']\n",
    "    text = df.iloc[int(index)]['text']\n",
    "    print(f'\\n -------------------------- \\n')\n",
    "    print(f\"Task ID: {task_id}. \\n\\nINPUT TEXT: {text}\\n\\nLLM ANSWER: {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e0e816-6558-4ff2-bab2-f85cff00bfc8",
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1732042430093,
     "user": {
      "displayName": "Joaquin Rodríguez",
      "userId": "09993043682054067997"
     },
     "user_tz": 180
    },
    "id": "tu2R8dGYimKc"
   },
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d195dd1-1293-4407-b6ad-cab7e77b14c0",
   "metadata": {},
   "source": [
    "Congratulations! You’ve successfully completed the keyword extraction task using the kluster.ai Batch API. This demonstration highlights how you can effortlessly manage large datasets and extract meaningful insights from them. With the Batch API, you can scale your workflows smoothly, making it an essential tool for processing large-scale datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c092343e-7672-46f4-aa27-00b3466ad210",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
