{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Integrating Tools with the kluster.ai API\n",
    "\n",
    "## Introduction\n",
    "\n",
    "A **tool** is a server-side capability you describe to the language-model (LLM) with a JSON Schema. During a chat completion the model can emit a `tool_calls` block by naming the function and filling in schema-valid arguments instead of plain-text prose. Your application (or one of ’s built-ins) runs that function, returns the result in a follow-up message, and the model weaves the data into its final answer.\n",
    "\n",
    "This pattern lets natural-language prompts trigger real-world effects such as querying a database, fetching the BTC/USD price, crunching numbers, scraping docs, or posting calendar events, without exposing credentials or business logic to the model. In short, tools turn a smart chat agent into a full-stack teammate: the LLM handles intent and dialogue, while your code performs deterministic, auditable side effects.\n",
    "\n",
    "This notebook shows how to use the kluster Tools endpoint with Python. We’ll cover:\n",
    "\n",
    "1. Setting up the environment  \n",
    "2. Calling a single tool  \n",
    "3. Trying multiple tools (calculator, web search, etc.)  \n",
    "4. Handling tool outputs and streaming responses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, let's install the required packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install openai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's set up our client:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "import json\n",
    "from IPython.display import display, Markdown, HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll need to set your kluster API key. For security, use environment variables or a secrets manager in a production environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your kluster API key:  ········\n"
     ]
    }
   ],
   "source": [
    "# Set your API key\n",
    "# For demo purposes, we'll ask for it here - in production, use environment variables\n",
    "from getpass import getpass\n",
    "\n",
    "kluster_api_key = getpass(\"Enter your kluster API key: \")\n",
    "\n",
    "# Initialize the client\n",
    "client = OpenAI(\n",
    "    base_url=\"https://api.kluster.ai/v1\",  # kluster API endpoint\n",
    "    api_key=kluster_api_key\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Tool Calling\n",
    "\n",
    "Kluster supports tool calling similar to OpenAI's function calling. Let's start with a simple example using a calculator tool. \n",
    "\n",
    "Kluster treats the calculator as a first‐class capability you expose to the model: by including its JSON-Schema in the tools array, you tell the LLM, “if the user asks for arithmetic, call this function instead of guessing the answer.” When we send the prompt “What is 1337 × 42?” with `tool_choice=\"auto\"`, the model recognises that the calculator is the best way to satisfy the request and answers not with prose but with a `tool_calls` block that contains the function name and a properly-formatted argument string (\"1337 * 42\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"id\": \"chatcmpl-8dd4c1cc-4ecc-4c79-aed6-d408bbad5081\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"finish_reason\": \"tool_calls\",\n",
      "      \"index\": 0,\n",
      "      \"logprobs\": null,\n",
      "      \"message\": {\n",
      "        \"content\": null,\n",
      "        \"refusal\": null,\n",
      "        \"role\": \"assistant\",\n",
      "        \"audio\": null,\n",
      "        \"function_call\": null,\n",
      "        \"tool_calls\": [\n",
      "          {\n",
      "            \"id\": \"chatcmpl-tool-fa51f55519ff412bb1798262c3a5a797\",\n",
      "            \"function\": {\n",
      "              \"arguments\": \"{\\\"expression\\\": \\\"1337 * 42\\\"}\",\n",
      "              \"name\": \"calculator\"\n",
      "            },\n",
      "            \"type\": \"function\"\n",
      "          }\n",
      "        ]\n",
      "      },\n",
      "      \"stop_reason\": 128008\n",
      "    }\n",
      "  ],\n",
      "  \"created\": 1746741970,\n",
      "  \"model\": \"klusterai/Meta-Llama-3.1-8B-Instruct-Turbo\",\n",
      "  \"object\": \"chat.completion\",\n",
      "  \"service_tier\": null,\n",
      "  \"system_fingerprint\": null,\n",
      "  \"usage\": {\n",
      "    \"completion_tokens\": 20,\n",
      "    \"prompt_tokens\": 252,\n",
      "    \"total_tokens\": 272,\n",
      "    \"completion_tokens_details\": null,\n",
      "    \"prompt_tokens_details\": null\n",
      "  },\n",
      "  \"prompt_logprobs\": null\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "def run_with_tools(prompt, tools, model=\"klusterai/Meta-Llama-3.1-8B-Instruct-Turbo\"):\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        tools=tools,\n",
    "        tool_choice=\"auto\"\n",
    "    )\n",
    "    \n",
    "    return response\n",
    "\n",
    "# Define a calculator tool\n",
    "calculator_tools = [\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"calculator\",\n",
    "            \"description\": \"Perform arithmetic calculations\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"expression\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"The mathematical expression to evaluate\"\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"expression\"]\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "# Test with a math problem\n",
    "calculator_response = run_with_tools(\n",
    "    \"What is 1337 multiplied by 42?\", \n",
    "    calculator_tools\n",
    ")\n",
    "\n",
    "print(json.dumps(calculator_response.model_dump(), indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tool Response Processing\n",
    "\n",
    "Once the model pauses with its `tool_calls` block we, not the LLM, take the wheel.\n",
    "The helper below does four things in one sweep:\n",
    "\n",
    "1. **Parse the call**: It inspects `response.choices[0].message.tool_calls`, pulls out the function name, and JSON-decodes the arguments\n",
    "\n",
    "2. **Run the side-effect safely**: Here we hand the expression to `execute_calculator()`, which first whitelists characters and then evaluates it. This is a placeholder for demonstration purposes only - use a proper math parser in production\n",
    "\n",
    "3. **Hand the result back to the model**: We craft a new chat turn with `role:\"tool\"`, include the original `tool_call_id`, and embed the JSON `{ \"result\": 56154 }`\n",
    "\n",
    "4. **Let the model finish the thought**: A second `chat.completions.create()` lets the LLM convert that raw number into friendly prose, e.g. “The result of multiplying 1337 by 42 is 56,154”\n",
    "\n",
    "This two-step dance—model → tool → model keeps business logic in your codebase while giving users a seamless conversational experience.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The result of 1337 multiplied by 42 is 56,154.\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import re\n",
    "\n",
    "def execute_calculator(expression):\n",
    "    # Simple calculator using eval() (note: never use this in production without proper validation)\n",
    "    # In production, use a safer method for evaluation\n",
    "    try:\n",
    "        # Basic sanitization\n",
    "        if not re.match(r'^[0-9+\\-*/().%\\s]+$', expression):\n",
    "            return {\"error\": \"Invalid expression. Only basic arithmetic operations are allowed.\"}\n",
    "        \n",
    "        result = eval(expression)\n",
    "        return {\"result\": result}\n",
    "    except Exception as e:\n",
    "        return {\"error\": str(e)}\n",
    "\n",
    "def process_tool_calls(response):\n",
    "    message = response.choices[0].message\n",
    "    \n",
    "    # If there are no tool calls, return the message content\n",
    "    if not message.tool_calls:\n",
    "        return message.content\n",
    "    \n",
    "    # Process each tool call\n",
    "    tool_results = []\n",
    "    for tool_call in message.tool_calls:\n",
    "        function_name = tool_call.function.name\n",
    "        arguments = json.loads(tool_call.function.arguments)\n",
    "        \n",
    "        # Execute the appropriate function based on the tool call\n",
    "        if function_name == \"calculator\":\n",
    "            result = execute_calculator(arguments[\"expression\"])\n",
    "            tool_results.append({\n",
    "                \"tool_call_id\": tool_call.id,\n",
    "                \"function_name\": function_name,\n",
    "                \"result\": result\n",
    "            })\n",
    "    \n",
    "    # Create a new message with the tool results\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": \"What is 1337 multiplied by 42?\"},\n",
    "        message.model_dump(),\n",
    "    ]\n",
    "    \n",
    "    # Add the tool results\n",
    "    for result in tool_results:\n",
    "        messages.append({\n",
    "            \"role\": \"tool\",\n",
    "            \"tool_call_id\": result[\"tool_call_id\"],\n",
    "            \"content\": json.dumps(result[\"result\"])\n",
    "        })\n",
    "    \n",
    "    # Get the final response\n",
    "    final_response = client.chat.completions.create(\n",
    "        model=\"klusterai/Meta-Llama-3.1-8B-Instruct-Turbo\",\n",
    "        messages=messages\n",
    "    )\n",
    "    \n",
    "    return final_response.choices[0].message.content\n",
    "\n",
    "# Process the calculator response\n",
    "final_answer = process_tool_calls(calculator_response)\n",
    "print(final_answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Tool Calling Example - Web Search\n",
    "\n",
    "The calculator example kept all the logic on our machine, but many real-world tasks need fresh external data. Here we register a `web_search(query: str)` tool that the model can invoke whenever the user’s request can’t be answered from its training set alone. When we ask “What are the latest findings on climate change?” the LLM recognises it needs up-to-date information, pauses with a `tool_calls` block that contains the search term, and lets our app take over.\n",
    "\n",
    "`Execute_web_search()` (stubbed here with mock results) returns a list of title/snippet/URL triples. We wrap that payload in a `{role:\"tool\"}` message, preserving the original `tool_call_id`, and hand it back to the model. The LLM then synthesises a readable summary including bullet points of ecosystem impacts, renewable energy advances, tipping point research, and so on.\n",
    "\n",
    "This pattern of LLM for intent, tool for retrieval, LLM for synthesis is the backbone of production grade assistants that need live data such as news digests, domain-specific knowledge chatbots, or anything where “I don’t know” isn’t acceptable. Although the results here are fabricated for demo purposes, the flow shows exactly how you’d slot in a real search API or internal knowledge service in production."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The latest findings on climate change indicate that its impacts are becoming more pronounced and widespread. Some of the key developments include:\n",
      "\n",
      "1. **Accelerating Sea-Level Rise**: New research suggests that sea levels are rising at an accelerating rate, with some studies predicting increases of up to 26 inches by 2050 and 82 inches by 2100.\n",
      "2. **Large-Scale Ice Sheet Collapse**: Scientists have discovered that the West Antarctic Ice Sheet is undergoing a sudden and irreversible collapse, which could lead to a catastrophic sea-level rise of up to 10 feet.\n",
      "3. **Rate of Global Warming**: The rate of global warming has been accelerating, with 2020 and 2021 ranking as the two hottest years on record globally.\n",
      "4. **Impacts on Global Ecosystems**: Climate change is having devastating impacts on global ecosystems, including coral bleaching, species extinctions, and loss of biodiversity.\n",
      "5. **Climate Change and Human Migration**: New studies suggest that climate change is driving human migration and displacement, particularly in vulnerable communities.\n",
      "\n",
      "Renewable energy solutions are also emerging as a key strategy for addressing climate change. Advances in solar and wind energy technologies have made them increasingly cost-competitive with fossil fuels, while other innovations such as hydrogen fuel cells and carbon capture and storage are also showing promise.\n",
      "\n",
      "Overall, the latest findings on climate change underscore the need for urgent action to reduce greenhouse gas emissions and transition to a low-carbon economy.\n"
     ]
    }
   ],
   "source": [
    "# Define the web search tool\n",
    "web_search_tools = [\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"web_search\",\n",
    "            \"description\": \"Search the web for information\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"query\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"The search query\"\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"query\"]\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "# Sample function to simulate web search results\n",
    "def execute_web_search(query):\n",
    "    # In a real application, this would call an actual search API\n",
    "    # For this demo, we'll return mock results\n",
    "    if \"climate\" in query.lower():\n",
    "        return {\n",
    "            \"results\": [\n",
    "                {\n",
    "                    \"title\": \"Climate Change Effects - Latest Research\",\n",
    "                    \"snippet\": \"New studies show increasing impacts of climate change on global ecosystems.\",\n",
    "                    \"url\": \"https://example.com/climate-research\"\n",
    "                },\n",
    "                {\n",
    "                    \"title\": \"Renewable Energy Solutions for Climate Change\",\n",
    "                    \"snippet\": \"Advancements in renewable energy technologies show promise in addressing climate challenges.\",\n",
    "                    \"url\": \"https://example.com/renewable-climate\"\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    else:\n",
    "        return {\n",
    "            \"results\": [\n",
    "                {\n",
    "                    \"title\": \"Search results for: \" + query,\n",
    "                    \"snippet\": \"Sample search result for demonstration purposes\",\n",
    "                    \"url\": \"https://example.com/search\"\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "\n",
    "# Function to process web search tool calls\n",
    "def process_web_search(response, original_query):\n",
    "    message = response.choices[0].message\n",
    "    \n",
    "    # If there are no tool calls, return the message content\n",
    "    if not message.tool_calls:\n",
    "        return message.content\n",
    "    \n",
    "    # Process each tool call\n",
    "    tool_results = []\n",
    "    for tool_call in message.tool_calls:\n",
    "        function_name = tool_call.function.name\n",
    "        arguments = json.loads(tool_call.function.arguments)\n",
    "        \n",
    "        # Execute the appropriate function based on the tool call\n",
    "        if function_name == \"web_search\":\n",
    "            result = execute_web_search(arguments[\"query\"])\n",
    "            tool_results.append({\n",
    "                \"tool_call_id\": tool_call.id,\n",
    "                \"function_name\": function_name,\n",
    "                \"result\": result\n",
    "            })\n",
    "    \n",
    "    # Create a new message with the tool results\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": original_query},\n",
    "        message.model_dump(),\n",
    "    ]\n",
    "    \n",
    "    # Add the tool results\n",
    "    for result in tool_results:\n",
    "        messages.append({\n",
    "            \"role\": \"tool\",\n",
    "            \"tool_call_id\": result[\"tool_call_id\"],\n",
    "            \"content\": json.dumps(result[\"result\"])\n",
    "        })\n",
    "    \n",
    "    # Get the final response\n",
    "    final_response = client.chat.completions.create(\n",
    "        model=\"klusterai/Meta-Llama-3.1-8B-Instruct-Turbo\",\n",
    "        messages=messages\n",
    "    )\n",
    "    \n",
    "    return final_response.choices[0].message.content\n",
    "\n",
    "# Test with a query that would benefit from web search\n",
    "search_query = \"What are the latest findings on climate change?\"\n",
    "search_response = run_with_tools(search_query, web_search_tools)\n",
    "final_search_result = process_web_search(search_response, search_query)\n",
    "\n",
    "print(final_search_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Tool Example\n",
    "\n",
    "Real-world questions often need more than one capability. For example, a user might ask “look up Bitcoin’s market cap and convert it to euros.” By registering several tools in the same tools array we give the LLM a menu of options. In the prompt below we include both web_search and calculator, then ask:\n",
    "\n",
    "“If Earth’s temperature rises by 2 °C, what percentage increase is that from the current average of 15 °C?”\n",
    "\n",
    "The model inspects the schemas and decides this is purely arithmetic, so it emits a single `tool_calls` entry for the calculator. Our `process_multi_tool_calls()` helper loops over each requested tool, dispatches to the matching execution function, bundles every result into `{role:\"tool\"}` messages, and lets the model wrap things up in plain English.\n",
    "\n",
    "Had the question also required fresh data, e.g. “…and cite a recent study on global warming”, the LLM could have issued two calls in one turn: first web_search, then calculator. That illustrates the power of multi-tool orchestration: the model can plan a mini-workflow, while your code executes each deterministic step and feeds the results back for seamless narration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If the Earth's temperature rises by 2 degrees at a current average global temperature of 15 degrees Celsius, the percentage increase would be approximately 13.33%.\n"
     ]
    }
   ],
   "source": [
    "# Define multiple tools\n",
    "multi_tools = [\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"calculator\",\n",
    "            \"description\": \"Perform arithmetic calculations\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"expression\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"The mathematical expression to evaluate\"\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"expression\"]\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"web_search\",\n",
    "            \"description\": \"Search the web for information\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"query\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"The search query\"\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"query\"]\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "# Enhanced function to process multi-tool calls\n",
    "def process_multi_tool_calls(response, original_query):\n",
    "    message = response.choices[0].message\n",
    "    \n",
    "    # If there are no tool calls, return the message content\n",
    "    if not message.tool_calls:\n",
    "        return message.content\n",
    "    \n",
    "    # Process each tool call\n",
    "    tool_results = []\n",
    "    for tool_call in message.tool_calls:\n",
    "        function_name = tool_call.function.name\n",
    "        arguments = json.loads(tool_call.function.arguments)\n",
    "        \n",
    "        # Execute the appropriate function based on the tool call\n",
    "        if function_name == \"calculator\":\n",
    "            result = execute_calculator(arguments[\"expression\"])\n",
    "        elif function_name == \"web_search\":\n",
    "            result = execute_web_search(arguments[\"query\"])\n",
    "        else:\n",
    "            result = {\"error\": f\"Unknown tool: {function_name}\"}\n",
    "            \n",
    "        tool_results.append({\n",
    "            \"tool_call_id\": tool_call.id,\n",
    "            \"function_name\": function_name,\n",
    "            \"result\": result\n",
    "        })\n",
    "    \n",
    "    # Create a new message with the tool results\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": original_query},\n",
    "        message.model_dump(),\n",
    "    ]\n",
    "    \n",
    "    # Add the tool results\n",
    "    for result in tool_results:\n",
    "        messages.append({\n",
    "            \"role\": \"tool\",\n",
    "            \"tool_call_id\": result[\"tool_call_id\"],\n",
    "            \"content\": json.dumps(result[\"result\"])\n",
    "        })\n",
    "    \n",
    "    # Get the final response\n",
    "    final_response = client.chat.completions.create(\n",
    "        model=\"klusterai/Meta-Llama-3.1-8B-Instruct-Turbo\",\n",
    "        messages=messages\n",
    "    )\n",
    "    \n",
    "    return final_response.choices[0].message.content\n",
    "\n",
    "# Test with a complex query that might use multiple tools\n",
    "multi_query = \"If the Earth's temperature rises by 2 degrees, what percentage increase is that from the current average global temperature of 15 degrees Celsius?\"\n",
    "multi_response = run_with_tools(multi_query, multi_tools)\n",
    "final_multi_result = process_multi_tool_calls(multi_response, multi_query)\n",
    "\n",
    "print(final_multi_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Real-world Use Case: Document Analysis with Tools\n",
    "\n",
    "Here we treat an entire report as context and give the model two powers: read the text and, if it spots numbers that need crunching, invoke the calculator. The helper wraps the document and question into a single prompt, passes in our multi_tools array, and lets the model decide the workflow. In the revenue example the LLM extracts $1.2 M → $1.8 M, calls the calculator to compute the delta, then replies in prose: “The percentage increase in revenue was 50 %.” This pattern scales to meeting minutes, legal contracts, or log files- any place the model must combine natural language comprehension with deterministic math."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "According to the report, the percentage increase in revenue was 50%.\n"
     ]
    }
   ],
   "source": [
    "def document_analysis_with_tools(document, question):\n",
    "    # Prepare the prompt\n",
    "    prompt = f\"\"\"\n",
    "Document: \n",
    "{document}\n",
    "\n",
    "Question about the document: {question}\n",
    "\n",
    "Please answer the question based on the document. If calculations are needed, use the calculator tool.\n",
    "\"\"\"\n",
    "    \n",
    "    # Use the multi-tools from before\n",
    "    response = run_with_tools(prompt, multi_tools)\n",
    "    final_answer = process_multi_tool_calls(response, prompt)\n",
    "    \n",
    "    return final_answer\n",
    "\n",
    "# Sample document and question\n",
    "sample_document = \"\"\"\n",
    "Kluster.ai Performance Report 2024\n",
    "\n",
    "In Q1 2024, our platform processed 2.5 million requests, a 25% increase from Q4 2023 (2 million requests). \n",
    "The average response time was reduced from 350ms to 280ms, representing a 20% improvement.\n",
    "Our customer base grew from 500 to 800 companies, and revenue increased from $1.2M to $1.8M.\n",
    "\"\"\"\n",
    "\n",
    "sample_question = \"What was the percentage increase in revenue according to the report?\"\n",
    "\n",
    "document_analysis_result = document_analysis_with_tools(sample_document, sample_question)\n",
    "print(document_analysis_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Streaming with Tool Calls\n",
    "\n",
    "When you set `stream=True`,  pushes delta chunks to your client as soon as they’re ready. That means you can render tokens to the user in real time and watch the model decide mid-sentence to invoke a tool. In the helper above we listen to the stream, print regular text as it arrives, and intercept any `tool_calls` deltas: the moment we see `\"function\": {\"name\": \"calculator\" …}`, we log “Calling tool: calculator” and keep appending argument fragments until the model finishes the call. Only after the stream closes do we execute the tool and send the result back for a final completion. Streaming makes interactions feel instant, lets you show spinners or live-update UIs, and still preserves the deterministic tool-calling workflow you’ve seen in the earlier examples.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Calling tool: calculator\n",
      "Arguments: {\"expression\": \"17 * 43 + 125\"}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def stream_with_tools(prompt, tools, model=\"klusterai/Meta-Llama-3.1-8B-Instruct-Turbo\"):\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "    \n",
    "    stream = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        tools=tools,\n",
    "        tool_choice=\"auto\",\n",
    "        stream=True\n",
    "    )\n",
    "    \n",
    "    # Track current tool call and accumulate arguments\n",
    "    current_tool_calls = {}\n",
    "    \n",
    "    # In a notebook, we'd display this differently than in a script\n",
    "    for chunk in stream:\n",
    "        if not chunk.choices or len(chunk.choices) == 0:\n",
    "            continue\n",
    "            \n",
    "        delta = chunk.choices[0].delta\n",
    "        \n",
    "        # Handle regular content\n",
    "        if hasattr(delta, 'content') and delta.content:\n",
    "            print(delta.content, end=\"\")\n",
    "            \n",
    "        # Handle tool calls\n",
    "        elif hasattr(delta, 'tool_calls') and delta.tool_calls:\n",
    "            for tool_call in delta.tool_calls:\n",
    "                # Skip if no function data\n",
    "                if not tool_call.function:\n",
    "                    continue\n",
    "                    \n",
    "                # Get or create entry for this tool call\n",
    "                tool_id = tool_call.id\n",
    "                if tool_id not in current_tool_calls:\n",
    "                    current_tool_calls[tool_id] = {\n",
    "                        \"name\": \"\",\n",
    "                        \"arguments\": \"\"\n",
    "                    }\n",
    "                \n",
    "                # Update tool name if present\n",
    "                if hasattr(tool_call.function, 'name') and tool_call.function.name:\n",
    "                    if not current_tool_calls[tool_id][\"name\"]:\n",
    "                        print(f\"\\nCalling tool: {tool_call.function.name}\")\n",
    "                    current_tool_calls[tool_id][\"name\"] = tool_call.function.name\n",
    "                \n",
    "                # Accumulate arguments if present\n",
    "                if hasattr(tool_call.function, 'arguments') and tool_call.function.arguments:\n",
    "                    current_tool_calls[tool_id][\"arguments\"] += tool_call.function.arguments\n",
    "    \n",
    "    # Print the final, complete arguments for each tool call\n",
    "    for tool_id, tool_data in current_tool_calls.items():\n",
    "        if tool_data[\"arguments\"]:\n",
    "            print(f\"Arguments: {tool_data['arguments']}\")\n",
    "    \n",
    "    print(\"\\n\")\n",
    "\n",
    "# Test streaming with a simple query\n",
    "stream_with_tools(\"Calculate: 17 * 43 + 125\", calculator_tools)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "You’ve now seen ’s tool-calling API end-to-end: from authentication all the way to streaming, multi-tool orchestration. This notebook covered:\n",
    "\n",
    "1. Basic setup and authentication\n",
    "2. Single tool calling (calculator)\n",
    "3. Web search tool usage\n",
    "4. Multiple tool combinations\n",
    "5. Real-world document analysis use case\n",
    "6. Streaming tool calls\n",
    "\n",
    "You can extend this pattern to use other tools by defining their schemas and implementing the corresponding execution functions. Kluster's OpenAI-compatible API makes it straightforward to integrate with existing codebases.\n",
    "\n",
    "For production use, remember to:\n",
    "- Store API keys securely\n",
    "- Implement proper error handling\n",
    "- Use more sophisticated tool execution methods\n",
    "- Consider rate limits and costs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
