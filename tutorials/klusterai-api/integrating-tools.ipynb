{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Integrating tools with the kluster.ai API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/kluster-ai/klusterai-cookbook/blob/main/examples/integrating-tools.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tools let you give an LLM safe, schema-defined superpowers. During a chat completion, the model can call any function you expose by supplying JSON arguments instead of prose, then fold the result back into its reply. Your code runs the function, keeping credentials and business logic out of the model while unlocking actions like database queries, BTC/USD look-ups, math, web scraping, or calendar updates. In short, the LLM handles intent and dialogue; your code delivers auditable side effects.\n",
    "\n",
    "This notebook shows how to use the kluster.ai tools endpoint with Python. We’ll cover:\n",
    "\n",
    "1. Setting up the environment  \n",
    "2. Calling a single tool  \n",
    "3. Trying multiple tools (calculator, web search, etc.)  \n",
    "4. Handling tool outputs and streaming responses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before getting started, ensure you have the following:\n",
    "\n",
    "- **A kluster.ai account** - sign up on the <a href=\"https://platform.kluster.ai/signup\" target=\"_blank\">kluster.ai platform</a> if you don't have one\n",
    "- **A kluster.ai API key** - after signing in, go to the <a href=\"https://platform.kluster.ai/apikeys\" target=\"_blank\">**API Keys**</a> section and create a new key. For detailed instructions, check out the <a href=\"/get-started/get-api-key/\" target=\"_blank\">Get an API key</a> guide"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xU1WBQJ7Uh09"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b8d580f8-67d5-45d6-915f-0f6b60d3b543"
   },
   "source": [
    "In this notebook, we'll use Python's `getpass` module to input the key safely. After execution, please provide your unique kluster.ai API key (ensure no spaces)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "4911e5eb-7463-4a6b-8a99-9313cecd9d4d"
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your kluster.ai API key:  ········\n"
     ]
    }
   ],
   "source": [
    "from getpass import getpass\n",
    "\n",
    "api_key = getpass(\"Enter your kluster.ai API key: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install the OpenAI Python client library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "bcc3d475-8f49-4fc4-9a5e-c6eb6866d2b5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q openai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the OpenAI Python library installed, import the dependencies for this tutorial:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "6af45325-7087-49fe-b32b-0ff1d6537af7"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "import json\n",
    "from IPython.display import display, Markdown, HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, create the client pointing to the kluster.ai endpoint with your API key:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "44a6f805-1c74-48a5-8572-0a5fb2c48286"
   },
   "outputs": [],
   "source": [
    "# Set up the client\n",
    "client = OpenAI(\n",
    "    base_url=\"https://api.kluster.ai/v1\",\n",
    "    api_key=api_key,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the model\n",
    "\n",
    "This example selects the `klusterai/Meta-Llama-3.1-8B-Instruct-Turbo` model. If you'd like to use a different model, feel free to change it by modifying the model field. Remember to use the full length model name to avoid errors.\n",
    "\n",
    "Please refer to the [Supported models](https://docs.kluster.ai/get-started/models/) section for a list of the models we support."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the LLM to use throughout this tutorial\n",
    "model = \"klusterai/Meta-Llama-3.1-8B-Instruct-Turbo\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the prompt\n",
    "\n",
    "We’ll store the baseline prompt in a variable so we can reuse it when we invoke the model. This baseline prompt will be changed and expanded later in the tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_prompt = \"What is 1337 multiplied by 42?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic tool calling\n",
    "\n",
    "kluster.ai supports tool calling similar to OpenAI's function calling. Let's start with a simple example using a calculator tool. \n",
    "\n",
    "kluster.ai treats tools as a capability you expose to the model: by including its JSON-Schema in the tools array, you tell the LLM, “if the user asks for arithmetic, call this function instead of guessing the answer.” When we send the prompt “What is 1337 × 42?” with `tool_choice=\"auto\"`, the model recognizes that the calculator is the best way to satisfy the request and answers not with prose but with a `tool_calls` block that contains the function name and a properly-formatted argument string (\"1337 * 42\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"id\": \"chatcmpl-47b7dfc6-a17e-4fac-b5bb-2ed5150fa0ca\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"finish_reason\": \"tool_calls\",\n",
      "      \"index\": 0,\n",
      "      \"logprobs\": null,\n",
      "      \"message\": {\n",
      "        \"content\": null,\n",
      "        \"refusal\": null,\n",
      "        \"role\": \"assistant\",\n",
      "        \"audio\": null,\n",
      "        \"function_call\": null,\n",
      "        \"tool_calls\": [\n",
      "          {\n",
      "            \"id\": \"chatcmpl-tool-4db47677c5684e8db141ecd02c6c5623\",\n",
      "            \"function\": {\n",
      "              \"arguments\": \"{\\\"expression\\\": \\\"1337 * 42\\\"}\",\n",
      "              \"name\": \"calculator\"\n",
      "            },\n",
      "            \"type\": \"function\"\n",
      "          }\n",
      "        ]\n",
      "      },\n",
      "      \"stop_reason\": 128008\n",
      "    }\n",
      "  ],\n",
      "  \"created\": 1747705029,\n",
      "  \"model\": \"klusterai/Meta-Llama-3.1-8B-Instruct-Turbo\",\n",
      "  \"object\": \"chat.completion\",\n",
      "  \"service_tier\": null,\n",
      "  \"system_fingerprint\": null,\n",
      "  \"usage\": {\n",
      "    \"completion_tokens\": 21,\n",
      "    \"prompt_tokens\": 252,\n",
      "    \"total_tokens\": 273,\n",
      "    \"completion_tokens_details\": null,\n",
      "    \"prompt_tokens_details\": null\n",
      "  },\n",
      "  \"prompt_logprobs\": null\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "def run_with_tools(prompt, tools, model=model):\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        tools=tools,\n",
    "        tool_choice=\"auto\"\n",
    "    )\n",
    "    \n",
    "    return response\n",
    "\n",
    "# Define a calculator tool\n",
    "calculator_tools = [\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"calculator\",\n",
    "            \"description\": \"Perform arithmetic calculations\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"expression\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"The mathematical expression to evaluate\"\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"expression\"]\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "# Test with a math problem\n",
    "calculator_response = run_with_tools(\n",
    "    baseline_prompt, \n",
    "    calculator_tools\n",
    ")\n",
    "\n",
    "print(json.dumps(calculator_response.model_dump(), indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpreting the tool-call response\n",
    "\n",
    "Let's take a closer look at the response above. The assistant’s reply isn’t prose; rather, it’s a structured tool call:\n",
    "\n",
    "1. **`finish_reason: \"tool_calls\"`** – signals the model has paused, waiting for us to run one or more tools\n",
    "2. **`message.tool_calls[0]`** – an array item that describes what to run:\n",
    "   * `id` – a unique identifier we must echo back\n",
    "   * `function.name` – here it’s `calculator`\n",
    "   * `function.arguments` – JSON-encoded string with the expression `\"1337 * 42\"`\n",
    "3. **`content: null`** – no human-readable answer yet; that will come after we execute the tool and return the result\n",
    "\n",
    "In short, the model has delegated the arithmetic. Our job is to run `execute_calculator(\"1337 * 42\")`, package the numeric result in a `{role:\"tool\"}` message (preserving the `tool_call_id`), and feed it back to the chat endpoint.\n",
    "\n",
    "The next section will walk through that hand-off step by step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tool-response processing\n",
    "\n",
    "To turn an LLM tool call into a human-friendly answer, we’ll take the following steps:\n",
    "\n",
    "1. **Parse the tool call** – inspect `response.choices[0].message.tool_calls`, grab the function name, and JSON-decode its arguments.  \n",
    "2. **Run the side-effect safely** – hand the expression to `execute_calculator()`, which allowlists characters and evaluates it (placeholder logic; swap in a real math parser for production).  \n",
    "3. **Return the result to the model** – craft a new chat turn with `role:\"tool\"`, preserve the original `tool_call_id`, and embed a JSON payload such as `{ \"result\": 56154 }`.  \n",
    "4. **Let the model finish the thought** – call `chat.completions.create()` again so the LLM can weave the raw number into friendly prose (e.g., “The result of multiplying 1337 by 42 is 56,154”).  \n",
    "\n",
    "Run the cells below to see this two-step dance **model → tool → model** in action.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The result of multiplying 1337 by 42 is 56154.\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import re\n",
    "\n",
    "def execute_calculator(expression):\n",
    "    # Simple calculator using eval() (note: never use this in production without proper validation)\n",
    "    # In production, use a safer method for evaluation\n",
    "    try:\n",
    "        # Basic sanitization\n",
    "        if not re.match(r'^[0-9+\\-*/().%\\s]+$', expression):\n",
    "            return {\"error\": \"Invalid expression. Only basic arithmetic operations are allowed.\"}\n",
    "        \n",
    "        result = eval(expression)\n",
    "        return {\"result\": result}\n",
    "    except Exception as e:\n",
    "        return {\"error\": str(e)}\n",
    "\n",
    "def process_tool_calls(response):\n",
    "    message = response.choices[0].message\n",
    "    \n",
    "    # If there are no tool calls, return the message content\n",
    "    if not message.tool_calls:\n",
    "        return message.content\n",
    "    \n",
    "    # Process each tool call\n",
    "    tool_results = []\n",
    "    for tool_call in message.tool_calls:\n",
    "        function_name = tool_call.function.name\n",
    "        arguments = json.loads(tool_call.function.arguments)\n",
    "        \n",
    "        # Execute the appropriate function based on the tool call\n",
    "        if function_name == \"calculator\":\n",
    "            result = execute_calculator(arguments[\"expression\"])\n",
    "            tool_results.append({\n",
    "                \"tool_call_id\": tool_call.id,\n",
    "                \"function_name\": function_name,\n",
    "                \"result\": result\n",
    "            })\n",
    "    \n",
    "    # Create a new message with the tool results\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": baseline_prompt},\n",
    "        message.model_dump(),\n",
    "    ]\n",
    "    \n",
    "    # Add the tool results\n",
    "    for result in tool_results:\n",
    "        messages.append({\n",
    "            \"role\": \"tool\",\n",
    "            \"tool_call_id\": result[\"tool_call_id\"],\n",
    "            \"content\": json.dumps(result[\"result\"])\n",
    "        })\n",
    "    \n",
    "    # Get the final response\n",
    "    final_response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages\n",
    "    )\n",
    "    \n",
    "    return final_response.choices[0].message.content\n",
    "\n",
    "# Process the calculator response\n",
    "final_answer = process_tool_calls(calculator_response)\n",
    "print(final_answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced tool-calling example: live web search\n",
    "\n",
    "The calculator example kept all logic local, but real-world apps often need fresh data. We'll register a `web_search(query: str)` tool so the LLM can pause, fetch live results, and then weave them into its answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Describe the tool in JSON-schema form\n",
    "web_search_tools = [\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"web_search\",\n",
    "            \"description\": \"Search the web for information\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"query\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"The search query\"\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"query\"]\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why a stub? In production, you'd call Bing, Google, or an internal search API. For this demo, we return deterministic mock data so you can run the notebook offline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_web_search(query: str):\n",
    "    \"\"\"Return mock search results.\"\"\"\n",
    "    if \"climate\" in query.lower():\n",
    "        return {\n",
    "            \"results\": [\n",
    "                {\n",
    "                    \"title\": \"Climate Change Effects – Latest Research\",\n",
    "                    \"snippet\": \"New studies show increasing impacts of climate change on global ecosystems.\",\n",
    "                    \"url\": \"https://example.com/climate-research\"\n",
    "                },\n",
    "                {\n",
    "                    \"title\": \"Renewable Energy Solutions for Climate Change\",\n",
    "                    \"snippet\": \"Advancements in renewable energy technologies show promise in addressing climate challenges.\",\n",
    "                    \"url\": \"https://example.com/renewable-climate\"\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    return {\n",
    "        \"results\": [\n",
    "            {\n",
    "                \"title\": f\"Search results for: {query}\",\n",
    "                \"snippet\": \"Sample search result for demonstration purposes\",\n",
    "                \"url\": \"https://example.com/search\"\n",
    "            }\n",
    "        ]\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the model emits a tool call, we run the tool and post the result back as a `role=\"tool\"` message.  \n",
    "Finally, we ask the model to finish the answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def process_web_search(response, original_query):\n",
    "    \"\"\"Handle any web_search tool calls and get the model’s final synthesis.\"\"\"\n",
    "    msg = response.choices[0].message\n",
    "    if not msg.tool_calls:\n",
    "        return msg.content  # nothing to do\n",
    "\n",
    "    tool_msgs = []\n",
    "    for call in msg.tool_calls:\n",
    "        args = json.loads(call.function.arguments)\n",
    "        if call.function.name == \"web_search\":\n",
    "            result = execute_web_search(args[\"query\"])\n",
    "            tool_msgs.append({\n",
    "                \"role\": \"tool\",\n",
    "                \"tool_call_id\": call.id,\n",
    "                \"content\": json.dumps(result)\n",
    "            })\n",
    "\n",
    "    follow_up_messages = [\n",
    "        {\"role\": \"user\", \"content\": original_query},\n",
    "        msg.model_dump(),\n",
    "        *tool_msgs,\n",
    "    ]\n",
    "\n",
    "    final = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=follow_up_messages\n",
    "    )\n",
    "    return final.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_query = \"What are the latest findings on climate change?\"\n",
    "search_response = run_with_tools(search_query, web_search_tools)\n",
    "print(process_web_search(search_response, search_query))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-tool example\n",
    "\n",
    "Real-world questions often need more than one capability. For instance, a user might ask:\n",
    "\n",
    "> “Look up Bitcoin’s market cap **and** convert it to euros.”\n",
    "\n",
    "By registering several tools in a single `tools` array, we give the LLM a menu of options. We’ll demonstrate with the question:\n",
    "\n",
    "> “If Earth’s temperature rises by 2 °C, what percentage increase is that from the current average of 15 °C?”\n",
    "\n",
    "To answer, the model only needs arithmetic, but if the prompt also required live data—e.g., “*…and cite a recent study on global warming*”—it could call **two** tools in one turn: first `web_search`, then `calculator`. Here’s how we orchestrate that multi-tool workflow:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Describe each tool** – supply JSON-schema specs for `calculator` and `web_search`\n",
    "2. **Let the LLM plan** – pass both specs in `multi_tools`; the LLM can issue one or many `tool_calls`\n",
    "3. **Dispatch and execute** – `process_multi_tool_calls()` loops over each call, runs the matching helper, and returns results as `{role:\"tool\"}` messages\n",
    "4. **Finish in plain English** – a follow-up `chat.completions.create()` lets the model weave everything into a readable answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Tool specs ────────────────────────────────────────────────────────────\n",
    "multi_tools = [\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"calculator\",\n",
    "            \"description\": \"Perform arithmetic calculations\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"expression\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"The mathematical expression to evaluate\"\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"expression\"]\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"web_search\",\n",
    "            \"description\": \"Search the web for information\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"query\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"The search query\"\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"query\"]\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below code will run each requested tool, then feed the results back to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def process_multi_tool_calls(response, original_query):\n",
    "    msg = response.choices[0].message\n",
    "    if not msg.tool_calls:\n",
    "        return msg.content\n",
    "\n",
    "    tool_msgs = []\n",
    "    for call in msg.tool_calls:\n",
    "        args = json.loads(call.function.arguments)\n",
    "        if call.function.name == \"calculator\":\n",
    "            result = execute_calculator(args[\"expression\"])\n",
    "        elif call.function.name == \"web_search\":\n",
    "            result = execute_web_search(args[\"query\"])\n",
    "        else:\n",
    "            result = {\"error\": f\"Unknown tool: {call.function.name}\"}\n",
    "\n",
    "        tool_msgs.append({\n",
    "            \"role\": \"tool\",\n",
    "            \"tool_call_id\": call.id,\n",
    "            \"content\": json.dumps(result)\n",
    "        })\n",
    "\n",
    "    final = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": original_query},\n",
    "            msg.model_dump(),\n",
    "            *tool_msgs,\n",
    "        ]\n",
    "    )\n",
    "    return final.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try out the multi-tool demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Demo ─────────────────────────────────────────────────────────────────\n",
    "multi_query = (\n",
    "    \"If the Earth's temperature rises by 2 degrees, what percentage increase \"\n",
    "    \"is that from the current average global temperature of 15 degrees Celsius?\"\n",
    ")\n",
    "\n",
    "multi_response = run_with_tools(multi_query, multi_tools)\n",
    "print(process_multi_tool_calls(multi_response, multi_query))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Real-world use case: document analysis with tools\n",
    "\n",
    "We’ll let the model read a report and, when it spots numbers, invoke the calculator:\n",
    "\n",
    "1. **Wrap the context** – embed the document and the user’s question in a single prompt  \n",
    "2. **Let the LLM decide** – pass the prompt along with `multi_tools`; the model can choose to read or calculate  \n",
    "3. **Process tool calls** – if the model pauses with a `tool_calls` block, we run the appropriate tools and feed the results back  \n",
    "4. **Return prose** – the model weaves the numeric answer into a natural-language response\n",
    "\n",
    "This pattern scales to meeting minutes, legal contracts, or server log files—anywhere the model must blend language understanding with deterministic math.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def document_analysis_with_tools(document, question):\n",
    "    # Prepare the prompt\n",
    "    prompt = f\"\"\"\n",
    "Document: \n",
    "{document}\n",
    "\n",
    "Question about the document: {question}\n",
    "\n",
    "Please answer the question based on the document. If calculations are needed, use the calculator tool.\n",
    "\"\"\"\n",
    "    \n",
    "    # Use the multi-tools from before\n",
    "    response = run_with_tools(prompt, multi_tools)\n",
    "    final_answer = process_multi_tool_calls(response, prompt)\n",
    "    \n",
    "    return final_answer\n",
    "\n",
    "# Sample document and question\n",
    "sample_document = \"\"\"\n",
    "kluster.ai Performance Report 2024\n",
    "\n",
    "In Q1 2024, our platform processed 2.5 million requests, a 25% increase from Q4 2023 (2 million requests). \n",
    "The average response time was reduced from 350ms to 280ms, representing a 20% improvement.\n",
    "Our customer base grew from 500 to 800 companies, and revenue increased from $1.2M to $1.8M.\n",
    "\"\"\"\n",
    "\n",
    "sample_question = \"What was the percentage increase in revenue according to the report?\"\n",
    "\n",
    "document_analysis_result = document_analysis_with_tools(sample_document, sample_question)\n",
    "print(document_analysis_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Real-world use case: Bitcoin to Satoshi USD conversion\n",
    "\n",
    "Let's implement a more advanced use case that demonstrates tool chaining to calculate the price of a Satoshi in USD. A Satoshi is 100,000,000 of a Bitcoin. Our example will:\n",
    "\n",
    "1. **Fetch market data** – use web search to look up the current BTC/USD price\n",
    "2. **Parse the results** – extract the numeric price from text-based search results\n",
    "3. **Calculate the conversion** – pass the extracted price to the calculator tool to find one Satoshi's value\n",
    "4. **Format the response** – combine the data into a user-friendly answer\n",
    "\n",
    "As in prior examples, we are using mock data with simplified results for demonstration purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def btc_satoshi_conversion_with_tools():\n",
    "    \"\"\"\n",
    "    Demonstrate a multi-tool chain using the existing calculator tool:\n",
    "    1. Get mock Bitcoin price data (simulating web search)\n",
    "    2. Use the calculator tool to find the value of one Satoshi\n",
    "    \"\"\"\n",
    "    print(\"Step 1: Looking up the current Bitcoin price...\")\n",
    "    \n",
    "    # Simulate the web search result for Bitcoin price\n",
    "    # In a real implementation, this would be the result of a web_search tool call\n",
    "    btc_search_results = {\n",
    "        \"results\": [\n",
    "            {\n",
    "                \"title\": \"Bitcoin Price Today | CoinMarketCap\",\n",
    "                \"snippet\": \"The current price of Bitcoin (BTC) is $105,248.63 USD with a 24-hour trading volume of $52.7B USD.\",\n",
    "                \"url\": \"https://coinmarketcap.com/currencies/bitcoin/\"\n",
    "            },\n",
    "            {\n",
    "                \"title\": \"Bitcoin USD (BTC-USD) Price, News, Quote & History - Yahoo Finance\",\n",
    "                \"snippet\": \"Bitcoin (BTC) is trading at $105,248.63, up 3.7% in the past 24 hours. Market cap stands at $2.07T with a circulating supply of 19.7M BTC.\",\n",
    "                \"url\": \"https://finance.yahoo.com/quote/BTC-USD\"\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    # Extract the Bitcoin price from the search results\n",
    "    btc_price_str = btc_search_results[\"results\"][0][\"snippet\"].split(\"$\")[1].split(\" \")[0]\n",
    "    btc_price = btc_price_str.replace(\",\", \"\")\n",
    "    print(f\"Found Bitcoin price: ${btc_price}\")\n",
    "    \n",
    "    print(\"\\nStep 2: Using calculator tool to find the value of one Satoshi...\")\n",
    "    \n",
    "    # Use the existing calculator tool to perform the calculation\n",
    "    # 1 Satoshi = 1/100,000,000 of a Bitcoin\n",
    "    calculation_expression = f\"{btc_price} / 100000000\"\n",
    "    \n",
    "    # Call the execute_calculator function that was defined earlier in the notebook\n",
    "    calculation_result = execute_calculator(calculation_expression)\n",
    "    \n",
    "    if \"error\" in calculation_result:\n",
    "        print(f\"Calculator error: {calculation_result['error']}\")\n",
    "        # Fallback to manual calculation if the calculator tool fails\n",
    "        satoshi_value = float(btc_price) / 100000000\n",
    "        print(f\"Fallback calculation: One Satoshi = ${satoshi_value:.8f} USD\")\n",
    "    else:\n",
    "        satoshi_value = calculation_result[\"result\"]\n",
    "        print(f\"Calculator result: One Satoshi = ${satoshi_value:.8f} USD\")\n",
    "    \n",
    "    # Format the final answer as the LLM would\n",
    "    final_answer = f\"\"\"\n",
    "Based on the current Bitcoin price of ${btc_price} USD, I used the calculator tool to find the value of one Satoshi.\n",
    "\n",
    "Since 1 Bitcoin (BTC) = 100,000,000 Satoshi, the value of one Satoshi is:\n",
    "${btc_price} ÷ 100,000,000 = ${satoshi_value:.8f} USD\n",
    "\n",
    "Therefore, one Satoshi is currently worth ${satoshi_value:.8f} USD.\n",
    "\"\"\"\n",
    "    return final_answer\n",
    "\n",
    "btc_satoshi_result = btc_satoshi_conversion_with_tools()\n",
    "print(btc_satoshi_result)"
   ]
  },
  {
  "cell_type": "code",
  "execution_count": 12,
  "metadata": {},
  "outputs": [
    {
      "name": "stdout",
      "output_type": "stream",
      "text": [
        "Step 1: Looking up the current Bitcoin price...\n",
        "Found Bitcoin price: $105248.63\n",
        "\n",
        "Step 2: Using calculator tool to find the value of one Satoshi...\n",
        "Calculator result: One Satoshi = $0.00105249 USD\n",
        "\n",
        "Based on the current Bitcoin price of $105248.63 USD, I used the calculator tool to find the value of one Satoshi.\n",
        "\n",
        "Since 1 Bitcoin (BTC) = 100,000,000 Satoshi, the value of one Satoshi is:\n",
        "$105248.63 ÷ 100,000,000 = $0.00105249 USD\n",
        "\n",
        "Therefore, one Satoshi is currently worth $0.00105249 USD.\n"
      ]
    }
  ],
  "source": [
    "# Demo: Bitcoin → Satoshi conversion\n",
    "btc_satoshi_result = btc_satoshi_conversion_with_tools()\n",
    "print(btc_satoshi_result)"
  ]
},
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "You’ve now seen kluster.ai’s tool-calling API end-to-end: from authentication all the way to streaming, multi-tool orchestration. This notebook covered:\n",
    "\n",
    "1. Basic setup and authentication\n",
    "2. Single tool calling (calculator)\n",
    "3. Web search tool usage\n",
    "4. Multiple tool combinations\n",
    "5. Real-world document analysis use case\n",
    "6. Currency conversion with web search and calculator tools\n",
    "\n",
    "You can extend this pattern to use other tools by defining their schemas and implementing the corresponding execution functions. kluster.ai's OpenAI-compatible API makes it straightforward to integrate with existing codebases.\n",
    "\n",
    "For production use, remember to:\n",
    "- Store API keys securely\n",
    "- Implement proper error handling\n",
    "- Use more sophisticated tool execution methods\n",
    "- Consider rate limits and costs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
