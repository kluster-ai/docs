{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Integrating tools with the kluster.ai API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/kluster-ai/klusterai-cookbook/blob/main/examples/integrating-tools.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tools let you give an LLM safe, schema-defined superpowers. During a chat completion, the model can call any function you expose by supplying JSON arguments instead of prose, then fold the result back into its reply. Your code runs the function, keeping credentials and business logic out of the model while unlocking actions like database queries, BTC/USD look-ups, math, web scraping, or calendar updates. In short, the LLM handles intent and dialogue; your code delivers auditable side effects.\n",
    "\n",
    "This notebook shows how to use the kluster.ai tools endpoint with Python. We’ll cover:\n",
    "\n",
    "1. Setting up the environment  \n",
    "2. Calling a single tool  \n",
    "3. Trying multiple tools (calculator, web search, etc.)  \n",
    "4. Handling tool outputs and streaming responses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ea62a1",
   "metadata": {},
   "source": [
    "## Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83111fd4",
   "metadata": {},
   "source": [
    "Before getting started, ensure you have the following:\n",
    "\n",
    "- **A kluster.ai account** - sign up on the <a href=\"https://platform.kluster.ai/signup\" target=\"_blank\">kluster.ai platform</a> if you don't have one\n",
    "- **A kluster.ai API key** - after signing in, go to the <a href=\"https://platform.kluster.ai/apikeys\" target=\"_blank\">**API Keys**</a> section and create a new key. For detailed instructions, check out the <a href=\"/get-started/get-api-key/\" target=\"_blank\">Get an API key</a> guide"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xU1WBQJ7Uh09"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b8d580f8-67d5-45d6-915f-0f6b60d3b543"
   },
   "source": [
    "In this notebook, we'll use Python's `getpass` module to input the key safely. After execution, please provide your unique kluster.ai API key (ensure no spaces)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "4911e5eb-7463-4a6b-8a99-9313cecd9d4d"
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your kluster.ai API key:  ········\n"
     ]
    }
   ],
   "source": [
    "from getpass import getpass\n",
    "\n",
    "api_key = getpass(\"Enter your kluster.ai API key: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install the OpenAI Python client library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "bcc3d475-8f49-4fc4-9a5e-c6eb6866d2b5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q openai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the OpenAI Python library installed, import the dependencies for this tutorial:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "6af45325-7087-49fe-b32b-0ff1d6537af7"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "import json\n",
    "from IPython.display import display, Markdown, HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, create the client pointing to the kluster.ai endpoint with your API key:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "44a6f805-1c74-48a5-8572-0a5fb2c48286"
   },
   "outputs": [],
   "source": [
    "# Set up the client\n",
    "client = OpenAI(\n",
    "    base_url=\"https://api.kluster.ai/v1\",\n",
    "    api_key=api_key,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the model\n",
    "\n",
    "This example selects the `klusterai/Meta-Llama-3.1-8B-Instruct-Turbo` model. If you'd like to use a different model, feel free to change it by modifying the model field. Remember to use the full length model name to avoid errors.\n",
    "\n",
    "Please refer to the [Supported models](https://docs.kluster.ai/get-started/models/) section for a list of the models we support."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the LLM to use throughout this tutorial\n",
    "model = \"klusterai/Meta-Llama-3.1-8B-Instruct-Turbo\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the prompt\n",
    "\n",
    "We’ll store the baseline prompt in a variable so we can reuse it when we invoke the model. This baseline prompt will be changed and expanded later in the tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_prompt = \"What is 1337 multiplied by 42?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic tool calling\n",
    "\n",
    "kluster.ai supports tool calling similar to OpenAI's function calling. Let's start with a simple example using a calculator tool. \n",
    "\n",
    "kluster.ai treats tools as a capability you expose to the model: by including its JSON-Schema in the tools array, you tell the LLM, “if the user asks for arithmetic, call this function instead of guessing the answer.” When we send the prompt “What is 1337 × 42?” with `tool_choice=\"auto\"`, the model recognizes that the calculator is the best way to satisfy the request and answers not with prose but with a `tool_calls` block that contains the function name and a properly-formatted argument string (\"1337 * 42\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"id\": \"chatcmpl-a8059d75-cff4-444c-81f9-c8ffffb368ca\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"finish_reason\": \"tool_calls\",\n",
      "      \"index\": 0,\n",
      "      \"logprobs\": null,\n",
      "      \"message\": {\n",
      "        \"content\": null,\n",
      "        \"refusal\": null,\n",
      "        \"role\": \"assistant\",\n",
      "        \"audio\": null,\n",
      "        \"function_call\": null,\n",
      "        \"tool_calls\": [\n",
      "          {\n",
      "            \"id\": \"chatcmpl-tool-1415b5225e9c4526b29c9df125a3b11e\",\n",
      "            \"function\": {\n",
      "              \"arguments\": \"{\\\"expression\\\": \\\"1337 * 42\\\"}\",\n",
      "              \"name\": \"calculator\"\n",
      "            },\n",
      "            \"type\": \"function\"\n",
      "          }\n",
      "        ]\n",
      "      },\n",
      "      \"stop_reason\": 128008\n",
      "    }\n",
      "  ],\n",
      "  \"created\": 1747264145,\n",
      "  \"model\": \"klusterai/Meta-Llama-3.1-8B-Instruct-Turbo\",\n",
      "  \"object\": \"chat.completion\",\n",
      "  \"service_tier\": null,\n",
      "  \"system_fingerprint\": null,\n",
      "  \"usage\": {\n",
      "    \"completion_tokens\": 21,\n",
      "    \"prompt_tokens\": 252,\n",
      "    \"total_tokens\": 273,\n",
      "    \"completion_tokens_details\": null,\n",
      "    \"prompt_tokens_details\": null\n",
      "  },\n",
      "  \"prompt_logprobs\": null\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "def run_with_tools(prompt, tools, model=model):\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        tools=tools,\n",
    "        tool_choice=\"auto\"\n",
    "    )\n",
    "    \n",
    "    return response\n",
    "\n",
    "# Define a calculator tool\n",
    "calculator_tools = [\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"calculator\",\n",
    "            \"description\": \"Perform arithmetic calculations\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"expression\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"The mathematical expression to evaluate\"\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"expression\"]\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "# Test with a math problem\n",
    "calculator_response = run_with_tools(\n",
    "    baseline_prompt, \n",
    "    calculator_tools\n",
    ")\n",
    "\n",
    "print(json.dumps(calculator_response.model_dump(), indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpreting the tool-call response\n",
    "\n",
    "Let's take a closer look at the response above. The assistant’s reply isn’t prose; rather, it’s a structured tool call:\n",
    "\n",
    "1. **`finish_reason: \"tool_calls\"`** – signals the model has paused, waiting for us to run one or more tools\n",
    "2. **`message.tool_calls[0]`** – an array item that describes what to run:\n",
    "   * `id` – a unique identifier we must echo back\n",
    "   * `function.name` – here it’s `calculator`\n",
    "   * `function.arguments` – JSON-encoded string with the expression `\"1337 * 42\"`\n",
    "3. **`content: null`** – no human-readable answer yet; that will come after we execute the tool and return the result\n",
    "\n",
    "In short, the model has delegated the arithmetic. Our job is to run `execute_calculator(\"1337 * 42\")`, package the numeric result in a `{role:\"tool\"}` message (preserving the `tool_call_id`), and feed it back to the chat endpoint.\n",
    "\n",
    "The next section will walk through that hand-off step by step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tool-response processing\n",
    "\n",
    "To turn an LLM tool call into a human-friendly answer, we’ll take the following steps:\n",
    "\n",
    "1. **Parse the tool call** – inspect `response.choices[0].message.tool_calls`, grab the function name, and JSON-decode its arguments.  \n",
    "2. **Run the side-effect safely** – hand the expression to `execute_calculator()`, which allowlists characters and evaluates it (placeholder logic; swap in a real math parser for production).  \n",
    "3. **Return the result to the model** – craft a new chat turn with `role:\"tool\"`, preserve the original `tool_call_id`, and embed a JSON payload such as `{ \"result\": 56154 }`.  \n",
    "4. **Let the model finish the thought** – call `chat.completions.create()` again so the LLM can weave the raw number into friendly prose (e.g., “The result of multiplying 1337 by 42 is 56,154”).  \n",
    "\n",
    "Run the cells below to see this two-step dance **model → tool → model** in action.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The result of multiplying 1337 by 42 is 56,154.\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import re\n",
    "\n",
    "def execute_calculator(expression):\n",
    "    # Simple calculator using eval() (note: never use this in production without proper validation)\n",
    "    # In production, use a safer method for evaluation\n",
    "    try:\n",
    "        # Basic sanitization\n",
    "        if not re.match(r'^[0-9+\\-*/().%\\s]+$', expression):\n",
    "            return {\"error\": \"Invalid expression. Only basic arithmetic operations are allowed.\"}\n",
    "        \n",
    "        result = eval(expression)\n",
    "        return {\"result\": result}\n",
    "    except Exception as e:\n",
    "        return {\"error\": str(e)}\n",
    "\n",
    "def process_tool_calls(response):\n",
    "    message = response.choices[0].message\n",
    "    \n",
    "    # If there are no tool calls, return the message content\n",
    "    if not message.tool_calls:\n",
    "        return message.content\n",
    "    \n",
    "    # Process each tool call\n",
    "    tool_results = []\n",
    "    for tool_call in message.tool_calls:\n",
    "        function_name = tool_call.function.name\n",
    "        arguments = json.loads(tool_call.function.arguments)\n",
    "        \n",
    "        # Execute the appropriate function based on the tool call\n",
    "        if function_name == \"calculator\":\n",
    "            result = execute_calculator(arguments[\"expression\"])\n",
    "            tool_results.append({\n",
    "                \"tool_call_id\": tool_call.id,\n",
    "                \"function_name\": function_name,\n",
    "                \"result\": result\n",
    "            })\n",
    "    \n",
    "    # Create a new message with the tool results\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": baseline_prompt},\n",
    "        message.model_dump(),\n",
    "    ]\n",
    "    \n",
    "    # Add the tool results\n",
    "    for result in tool_results:\n",
    "        messages.append({\n",
    "            \"role\": \"tool\",\n",
    "            \"tool_call_id\": result[\"tool_call_id\"],\n",
    "            \"content\": json.dumps(result[\"result\"])\n",
    "        })\n",
    "    \n",
    "    # Get the final response\n",
    "    final_response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages\n",
    "    )\n",
    "    \n",
    "    return final_response.choices[0].message.content\n",
    "\n",
    "# Process the calculator response\n",
    "final_answer = process_tool_calls(calculator_response)\n",
    "print(final_answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced tool-calling example: live web search\n",
    "\n",
    "The calculator example kept all logic local, but real-world apps often need fresh data. We'll register a `web_search(query: str)` tool so the LLM can pause, fetch live results, and then weave them into its answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Describe the tool in JSON-schema form\n",
    "web_search_tools = [\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"web_search\",\n",
    "            \"description\": \"Search the web for information\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"query\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"The search query\"\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"query\"]\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why a stub? In production, you'd call Bing, Google, or an internal search API. For this demo, we return deterministic mock data so you can run the notebook offline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_web_search(query: str):\n",
    "    \"\"\"Return mock search results.\"\"\"\n",
    "    if \"climate\" in query.lower():\n",
    "        return {\n",
    "            \"results\": [\n",
    "                {\n",
    "                    \"title\": \"Climate Change Effects – Latest Research\",\n",
    "                    \"snippet\": \"New studies show increasing impacts of climate change on global ecosystems.\",\n",
    "                    \"url\": \"https://example.com/climate-research\"\n",
    "                },\n",
    "                {\n",
    "                    \"title\": \"Renewable Energy Solutions for Climate Change\",\n",
    "                    \"snippet\": \"Advancements in renewable energy technologies show promise in addressing climate challenges.\",\n",
    "                    \"url\": \"https://example.com/renewable-climate\"\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    return {\n",
    "        \"results\": [\n",
    "            {\n",
    "                \"title\": f\"Search results for: {query}\",\n",
    "                \"snippet\": \"Sample search result for demonstration purposes\",\n",
    "                \"url\": \"https://example.com/search\"\n",
    "            }\n",
    "        ]\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the model emits a tool call, we run the tool and post the result back as a `role=\"tool\"` message.  \n",
    "Finally, we ask the model to finish the answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def process_web_search(response, original_query):\n",
    "    \"\"\"Handle any web_search tool calls and get the model’s final synthesis.\"\"\"\n",
    "    msg = response.choices[0].message\n",
    "    if not msg.tool_calls:\n",
    "        return msg.content  # nothing to do\n",
    "\n",
    "    tool_msgs = []\n",
    "    for call in msg.tool_calls:\n",
    "        args = json.loads(call.function.arguments)\n",
    "        if call.function.name == \"web_search\":\n",
    "            result = execute_web_search(args[\"query\"])\n",
    "            tool_msgs.append({\n",
    "                \"role\": \"tool\",\n",
    "                \"tool_call_id\": call.id,\n",
    "                \"content\": json.dumps(result)\n",
    "            })\n",
    "\n",
    "    follow_up_messages = [\n",
    "        {\"role\": \"user\", \"content\": original_query},\n",
    "        msg.model_dump(),\n",
    "        *tool_msgs,\n",
    "    ]\n",
    "\n",
    "    final = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=follow_up_messages\n",
    "    )\n",
    "    return final.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The latest findings on climate change include:\n",
      "\n",
      "1. **Increasing impacts of climate change on global ecosystems**: New studies have shown the effects of climate change on various ecosystems, including melting glaciers, rising sea levels, and changes in weather patterns.\n",
      "2. **Advancements in renewable energy technologies**: Research has been focusing on improving renewable energy sources such as solar and wind power, which have shown promise in reducing carbon emissions and addressing climate challenges.\n",
      "3. **Rising global temperatures**: Climate models predict that the planet will continue to warm, with an increase in average global temperatures by 2-4°C by the end of the century if greenhouse gas emissions continue to rise.\n",
      "4. **Arctic ice sheet melting**: The Arctic ice sheet has been melting at an unprecedented rate, leading to changes in ocean currents and sea levels.\n",
      "5. **Sea-level rise**: Coastal areas and low-lying islands are at risk of flooding due to rising sea levels, which is expected to continue in the coming decades.\n",
      "6. **Extreme weather events**: Climate change is linked to an increase in extreme weather events such as heatwaves, droughts, and heavy rainfall events.\n",
      "7. **Ocean acidification**: The absorption of carbon dioxide by the ocean has led to a decrease in pH levels, which can have negative impacts on marine life.\n",
      "8. **Loss of biodiversity**: Climate change is causing shifts in species ranges and extinction risk, with many species facing threats to their survival.\n",
      "9. **Water scarcity**: Changes in precipitation patterns and increasing evaporation due to warmer temperatures are leading to water shortages, affecting agriculture and human consumption.\n",
      "10. **Climate-related displacement**: Rising temperatures and extreme weather events are leading to displacement of people, with a projected 143 million people facing forced migration by 2050.\n",
      "\n",
      "These findings emphasize the urgent need for action to reduce greenhouse gas emissions and mitigate the effects of climate change.\n"
     ]
    }
   ],
   "source": [
    "search_query = \"What are the latest findings on climate change?\"\n",
    "search_response = run_with_tools(search_query, web_search_tools)\n",
    "print(process_web_search(search_response, search_query))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-tool example\n",
    "\n",
    "Real-world questions often need more than one capability. For instance, a user might ask:\n",
    "\n",
    "> “Look up Bitcoin’s market cap **and** convert it to euros.”\n",
    "\n",
    "By registering several tools in a single `tools` array, we give the LLM a menu of options. We’ll demonstrate with the question:\n",
    "\n",
    "> “If Earth’s temperature rises by 2 °C, what percentage increase is that from the current average of 15 °C?”\n",
    "\n",
    "To answer, the model only needs arithmetic, but if the prompt also required live data—e.g., “*…and cite a recent study on global warming*”—it could call **two** tools in one turn: first `web_search`, then `calculator`. Here’s how we orchestrate that multi-tool workflow:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Describe each tool** – supply JSON-schema specs for `calculator` and `web_search`\n",
    "2. **Let the LLM plan** – pass both specs in `multi_tools`; the LLM can issue one or many `tool_calls`\n",
    "3. **Dispatch and execute** – `process_multi_tool_calls()` loops over each call, runs the matching helper, and returns results as `{role:\"tool\"}` messages\n",
    "4. **Finish in plain English** – a follow-up `chat.completions.create()` lets the model weave everything into a readable answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Describe multiple tools\n",
    "multi_tools = [\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"calculator\",\n",
    "            \"description\": \"Perform arithmetic calculations\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"expression\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"The mathematical expression to evaluate\"\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"expression\"]\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"web_search\",\n",
    "            \"description\": \"Search the web for information\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"query\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"The search query\"\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"query\"]\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below code will run each requested tool, then feed the results back to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def process_multi_tool_calls(response, original_query):\n",
    "    msg = response.choices[0].message\n",
    "    if not msg.tool_calls:\n",
    "        return msg.content\n",
    "\n",
    "    tool_msgs = []\n",
    "    for call in msg.tool_calls:\n",
    "        args = json.loads(call.function.arguments)\n",
    "        if call.function.name == \"calculator\":\n",
    "            result = execute_calculator(args[\"expression\"])\n",
    "        elif call.function.name == \"web_search\":\n",
    "            result = execute_web_search(args[\"query\"])\n",
    "        else:\n",
    "            result = {\"error\": f\"Unknown tool: {call.function.name}\"}\n",
    "\n",
    "        tool_msgs.append({\n",
    "            \"role\": \"tool\",\n",
    "            \"tool_call_id\": call.id,\n",
    "            \"content\": json.dumps(result)\n",
    "        })\n",
    "\n",
    "    follow_up = [\n",
    "        {\"role\": \"user\", \"content\": original_query},\n",
    "        msg.model_dump(),\n",
    "        *tool_msgs,\n",
    "    ]\n",
    "\n",
    "    final = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=follow_up\n",
    "    )\n",
    "    return final.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try out the multi-tool demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The percentage increase in the Earth's temperature would be approximately 13.33%.\n"
     ]
    }
   ],
   "source": [
    "multi_query = (\n",
    "    \"If the Earth's temperature rises by 2 degrees, what percentage increase \"\n",
    "    \"is that from the current average global temperature of 15 degrees Celsius?\"\n",
    ")\n",
    "multi_response = run_with_tools(multi_query, multi_tools)\n",
    "print(process_multi_tool_calls(multi_response, multi_query))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Real-world use case: document analysis with tools\n",
    "\n",
    "We’ll let the model read a report and, when it spots numbers, invoke the calculator:\n",
    "\n",
    "1. **Wrap the context** – embed the document and the user’s question in a single prompt  \n",
    "2. **Let the LLM decide** – pass the prompt along with `multi_tools`; the model can choose to read or calculate  \n",
    "3. **Process tool calls** – if the model pauses with a `tool_calls` block, we run the appropriate tools and feed the results back  \n",
    "4. **Return prose** – the model weaves the numeric answer into a natural-language response\n",
    "\n",
    "This pattern scales to meeting minutes, legal contracts, or server log files—anywhere the model must blend language understanding with deterministic math.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The error message indicates that the calculator does not support the multiplication and division operators in the expression.\n",
      "\n",
      "To calculate the percentage increase in revenue, we can rewrite the expression as:\n",
      "\n",
      "(($1.8M - $1.2M) / $1.2M) * 100\n",
      "\n",
      "First, let's calculate the numerator:\n",
      "\n",
      "$1.8M - $1.2M = $0.6M\n",
      "\n",
      "Now, divide the result by $1.2M:\n",
      "\n",
      "$0.6M / $1.2M = 0.5\n",
      "\n",
      "Multiply the result by 100 to get the percentage increase:\n",
      "\n",
      "0.5 * 100 = 50%\n",
      "\n",
      "Therefore, the percentage increase in revenue according to the report is 50%.\n"
     ]
    }
   ],
   "source": [
    "def document_analysis_with_tools(document, question):\n",
    "    # Prepare the prompt\n",
    "    prompt = f\"\"\"\n",
    "Document: \n",
    "{document}\n",
    "\n",
    "Question about the document: {question}\n",
    "\n",
    "Please answer the question based on the document. If calculations are needed, use the calculator tool.\n",
    "\"\"\"\n",
    "    \n",
    "    # Use the multi-tools from before\n",
    "    response = run_with_tools(prompt, multi_tools)\n",
    "    final_answer = process_multi_tool_calls(response, prompt)\n",
    "    \n",
    "    return final_answer\n",
    "\n",
    "# Sample document and question\n",
    "sample_document = \"\"\"\n",
    "Kluster.ai Performance Report 2024\n",
    "\n",
    "In Q1 2024, our platform processed 2.5 million requests, a 25% increase from Q4 2023 (2 million requests). \n",
    "The average response time was reduced from 350ms to 280ms, representing a 20% improvement.\n",
    "Our customer base grew from 500 to 800 companies, and revenue increased from $1.2M to $1.8M.\n",
    "\"\"\"\n",
    "\n",
    "sample_question = \"What was the percentage increase in revenue according to the report?\"\n",
    "\n",
    "document_analysis_result = document_analysis_with_tools(sample_document, sample_question)\n",
    "print(document_analysis_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Streaming with tool calls\n",
    "\n",
    "When you set `stream=True`, Kluster.ai pushes delta chunks to your client as soon as they’re ready. That means you can render tokens to the user in real time and watch the model decide mid-sentence to invoke a tool. \n",
    "\n",
    "In the helper above we listen to the stream, print regular text as it arrives, and intercept any `tool_calls` deltas: the moment we see `\"function\": {\"name\": \"calculator\" …}`, we log “Calling tool: calculator” and keep appending argument fragments until the model finishes the call. Only after the stream closes do we execute the tool and send the result back for a final completion. Streaming makes interactions feel instant, lets you show spinners or live-update UIs, and still preserves the deterministic tool-calling workflow you’ve seen in the earlier examples.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Calling tool: calculator\n",
      "Arguments: {\"expression\": \"17 * 43 + 125\"}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def stream_with_tools(prompt, tools, model):\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "    \n",
    "    stream = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        tools=tools,\n",
    "        tool_choice=\"auto\",\n",
    "        stream=True\n",
    "    )\n",
    "    \n",
    "    # Track current tool call and accumulate arguments\n",
    "    current_tool_calls = {}\n",
    "    \n",
    "    # In a notebook, we'd display this differently than in a script\n",
    "    for chunk in stream:\n",
    "        if not chunk.choices or len(chunk.choices) == 0:\n",
    "            continue\n",
    "            \n",
    "        delta = chunk.choices[0].delta\n",
    "        \n",
    "        # Handle regular content\n",
    "        if hasattr(delta, 'content') and delta.content:\n",
    "            print(delta.content, end=\"\")\n",
    "            \n",
    "        # Handle tool calls\n",
    "        elif hasattr(delta, 'tool_calls') and delta.tool_calls:\n",
    "            for tool_call in delta.tool_calls:\n",
    "                # Skip if no function data\n",
    "                if not tool_call.function:\n",
    "                    continue\n",
    "                    \n",
    "                # Get or create entry for this tool call\n",
    "                tool_id = tool_call.id\n",
    "                if tool_id not in current_tool_calls:\n",
    "                    current_tool_calls[tool_id] = {\n",
    "                        \"name\": \"\",\n",
    "                        \"arguments\": \"\"\n",
    "                    }\n",
    "                \n",
    "                # Update tool name if present\n",
    "                if hasattr(tool_call.function, 'name') and tool_call.function.name:\n",
    "                    if not current_tool_calls[tool_id][\"name\"]:\n",
    "                        print(f\"\\nCalling tool: {tool_call.function.name}\")\n",
    "                    current_tool_calls[tool_id][\"name\"] = tool_call.function.name\n",
    "                \n",
    "                # Accumulate arguments if present\n",
    "                if hasattr(tool_call.function, 'arguments') and tool_call.function.arguments:\n",
    "                    current_tool_calls[tool_id][\"arguments\"] += tool_call.function.arguments\n",
    "    \n",
    "    # Print the final, complete arguments for each tool call\n",
    "    for tool_id, tool_data in current_tool_calls.items():\n",
    "        if tool_data[\"arguments\"]:\n",
    "            print(f\"Arguments: {tool_data['arguments']}\")\n",
    "    \n",
    "    print(\"\\n\")\n",
    "\n",
    "# Test streaming with a simple query\n",
    "stream_with_tools(\"Calculate: 17 * 43 + 125\", calculator_tools, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "You’ve now seen kluster.ai’s tool-calling API end-to-end: from authentication all the way to streaming, multi-tool orchestration. This notebook covered:\n",
    "\n",
    "1. Basic setup and authentication\n",
    "2. Single tool calling (calculator)\n",
    "3. Web search tool usage\n",
    "4. Multiple tool combinations\n",
    "5. Real-world document analysis use case\n",
    "6. Streaming tool calls\n",
    "\n",
    "You can extend this pattern to use other tools by defining their schemas and implementing the corresponding execution functions. Kluster.ai's OpenAI-compatible API makes it straightforward to integrate with existing codebases.\n",
    "\n",
    "For production use, remember to:\n",
    "- Store API keys securely\n",
    "- Implement proper error handling\n",
    "- Use more sophisticated tool execution methods\n",
    "- Consider rate limits and costs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
