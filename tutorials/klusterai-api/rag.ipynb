{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "566e7e41",
   "metadata": {},
   "source": [
    "# Enhanced RAG Tutorial with PDF Support using LlamaIndex and KlusterAI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8566fe2a",
   "metadata": {},
   "source": [
    "This notebook demonstrates a Retrieval Augmented Generation (RAG) pipeline with PDF document support using LlamaIndex.\n",
    "\n",
    "We will use KlusterAI's OpenAPI compatible endpoints for:\n",
    "1. **Embeddings**: Leveraging the `BAAI/bge-m3` model.\n",
    "2. **Language Model (LLM) for Querying**: Utilizing the `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` model.\n",
    "\n",
    "**Steps:**\n",
    "1. Install necessary libraries\n",
    "2. Set up API keys and model endpoints\n",
    "3. Demonstrate embedding generation with a sample text\n",
    "4. Load a PDF document as our knowledge base\n",
    "5. Configure the LlamaIndex components (LLM and Embeddings)\n",
    "6. Create an index from the document\n",
    "7. Query the index and compare with non-RAG responses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82108a06",
   "metadata": {},
   "source": [
    "## 1. Install Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e42cda8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the necessary packages, including the PDF reader for LlamaIndex\n",
    "!pip install llama-index llama-index-llms-openai-like llama-index-embeddings-openai-like llama-index-readers-file requests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25590afc",
   "metadata": {},
   "source": [
    "## 2. Set up API Keys and Model Endpoints\n",
    "\n",
    "We'll use getpass to securely input your KlusterAI API key without displaying it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f3cfcc54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import sys\n",
    "import requests\n",
    "import json\n",
    "from getpass import getpass\n",
    "from pprint import pprint\n",
    "\n",
    "# Optional: Enable detailed logging for LlamaIndex\n",
    "# logging.basicConfig(stream=sys.stdout, level=logging.DEBUG)\n",
    "# logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\n",
    "\n",
    "# Get API key securely using getpass\n",
    "KLUSTER_API_KEY = getpass(\"Enter your KlusterAI API Key: \")\n",
    "KLUSTER_BASE_URL = \"https://api.kluster.ai/v1\" # KlusterAI base URL\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = KLUSTER_API_KEY # LlamaIndex uses this env var for OpenAI-compatible APIs\n",
    "os.environ[\"OPENAI_API_BASE\"] = KLUSTER_BASE_URL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99d4865a",
   "metadata": {},
   "source": [
    "## 3. Embedding Demonstration\n",
    "\n",
    "Let's first demonstrate how to generate embeddings directly using the KlusterAI API. This helps illustrate what embeddings look like and how they're used in RAG systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ca0e710d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample text: 'The capital of France is Paris. It is known for the Eiffel Tower.'\n",
      "Model used: BAAI/bge-m3\n",
      "Embedding dimensions: 1024\n",
      "\n",
      "First 10 dimensions of the embedding vector:\n",
      "[0.01739501953125, 0.048370361328125, -0.006679534912109375, 0.0302734375, -0.01477813720703125, -0.00627899169921875, -0.0053253173828125, 0.004657745361328125, -0.019866943359375, 0.03375244140625]\n",
      "\n",
      "Token usage: 17 tokens\n"
     ]
    }
   ],
   "source": [
    "def generate_embedding(text):\n",
    "    \"\"\"Generate embeddings for the given text using KlusterAI API\"\"\"\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {KLUSTER_API_KEY}\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "    \n",
    "    response = requests.post(\n",
    "        f\"{KLUSTER_BASE_URL}/embeddings\",\n",
    "        headers=headers,\n",
    "        json={\n",
    "            \"model\": \"BAAI/bge-m3\",\n",
    "            \"input\": text,\n",
    "            \"encoding_format\": \"float\"\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    if response.status_code != 200:\n",
    "        raise Exception(f\"Error generating embedding: {response.text}\")\n",
    "        \n",
    "    return response.json()\n",
    "\n",
    "# Generate embedding for our example text about Paris\n",
    "sample_text = \"The capital of France is Paris. It is known for the Eiffel Tower.\"\n",
    "embedding_result = generate_embedding(sample_text)\n",
    "\n",
    "# Pretty print the first 10 dimensions of the embedding vector\n",
    "print(f\"Sample text: '{sample_text}'\")\n",
    "print(f\"Model used: {embedding_result['model']}\")\n",
    "print(f\"Embedding dimensions: {len(embedding_result['data'][0]['embedding'])}\")\n",
    "print(\"\\nFirst 10 dimensions of the embedding vector:\")\n",
    "print(embedding_result['data'][0]['embedding'][:10])\n",
    "\n",
    "# Show token usage information\n",
    "print(f\"\\nToken usage: {embedding_result['usage']['prompt_tokens']} tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91011ad2",
   "metadata": {},
   "source": [
    "## 4. Loading PDF Document\n",
    "\n",
    "We'll download and load a Polar Bear PDF document to use as our knowledge base for the RAG system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b17b05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading sample PDF to sample_pdfs/polar_bears.pdf...\n",
      "Download complete!\n"
     ]
    }
   ],
   "source": [
    "import urllib.request\n",
    "import os\n",
    "\n",
    "# Create a directory for our PDFs if it doesn't exist\n",
    "pdf_dir = \"sample_pdfs\"\n",
    "os.makedirs(pdf_dir, exist_ok=True)\n",
    "\n",
    "# Download a sample PDF about Polar Bears (you can replace with your own PDFs)\n",
    "sample_pdf_url = \"https://portals.iucn.org/library/sites/library/files/documents/SSC-OP-007.pdf\"  \n",
    "pdf_path = os.path.join(pdf_dir, \"polar_bears.pdf\")\n",
    "\n",
    "if not os.path.exists(pdf_path):\n",
    "    print(f\"Downloading sample PDF to {pdf_path}...\")\n",
    "    urllib.request.urlretrieve(sample_pdf_url, pdf_path)\n",
    "    print(\"Download complete!\")\n",
    "else:\n",
    "    print(f\"Sample PDF already exists at {pdf_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "458616cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading PDF from sample_pdfs...\n",
      "Loaded 115 document(s) from PDF file\n",
      "\n",
      "Preview of PDF content:\n",
      "...\n"
     ]
    }
   ],
   "source": [
    "# Import the necessary document loader from llama_index\n",
    "from llama_index.core import Document\n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "\n",
    "# Load documents from the PDF file\n",
    "print(f\"Loading PDF from {pdf_dir}...\")\n",
    "pdf_reader = SimpleDirectoryReader(input_dir=pdf_dir)\n",
    "documents = pdf_reader.load_data()\n",
    "\n",
    "print(f\"Loaded {len(documents)} document(s) from PDF file\")\n",
    "\n",
    "# Print a preview of the PDF document (first 500 characters)\n",
    "if documents:\n",
    "    print(\"\\nPreview of PDF content:\")\n",
    "    print(f\"{documents[0].text[:500]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df42ac97",
   "metadata": {},
   "source": [
    "## 5. Configure LlamaIndex Components (LLM and Embeddings)\n",
    "\n",
    "We will use `OpenAILike` for the LLM and `OpenAILikeEmbedding` for the embedding model, pointing them to the KlusterAI endpoints and specifying the desired models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "52d3b654",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaIndex LLM and Kluster AI Embedding Model configured.\n"
     ]
    }
   ],
   "source": [
    "from llama_index.llms.openai_like import OpenAILike\n",
    "from llama_index.embeddings.openai_like import OpenAILikeEmbedding\n",
    "from llama_index.core import Settings\n",
    "\n",
    "# Configure the LLM (Query Model)\n",
    "llm = OpenAILike(\n",
    "    model=\"meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8\",\n",
    "    api_base=KLUSTER_BASE_URL,\n",
    "    api_key=KLUSTER_API_KEY,\n",
    "    is_chat_model=True\n",
    ")\n",
    "\n",
    "# Configure the embedding model using OpenAILikeEmbedding\n",
    "embed_model = OpenAILikeEmbedding(\n",
    "    model_name=\"BAAI/bge-m3\",\n",
    "    api_base=KLUSTER_BASE_URL, \n",
    "    api_key=KLUSTER_API_KEY\n",
    "    # No dimensions parameter - it's not supported by KlusterAI\n",
    ")\n",
    "\n",
    "# Set the global settings for LlamaIndex\n",
    "Settings.llm = llm\n",
    "Settings.embed_model = embed_model\n",
    "Settings.chunk_size = 512\n",
    "Settings.chunk_overlap = 20\n",
    "\n",
    "print(\"LlamaIndex LLM and Kluster AI Embedding Model configured.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9cc04f1",
   "metadata": {},
   "source": [
    "## 6. Create an Index\n",
    "\n",
    "Now we'll create a `VectorStoreIndex` from our PDF document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "390b784f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating index from PDF document...\n",
      "Index created successfully!\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import VectorStoreIndex\n",
    "\n",
    "# Create the index from the PDF document\n",
    "print(\"Creating index from PDF document...\")\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    documents\n",
    ")\n",
    "print(\"Index created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1239a8c0",
   "metadata": {},
   "source": [
    "## 7. Query the Index and Compare with Non-RAG Responses\n",
    "\n",
    "Now we'll compare responses using RAG (with our knowledge base) versus direct LLM responses without context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e415d1f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query engines prepared. Ready to compare RAG vs non-RAG responses!\n"
     ]
    }
   ],
   "source": [
    "# Create a query engine for RAG\n",
    "query_engine = index.as_query_engine()\n",
    "\n",
    "# Function to get a direct response from the LLM without using RAG\n",
    "def get_direct_llm_response(query):\n",
    "    \"\"\"Get a response directly from the LLM without using RAG\"\"\"\n",
    "    return llm.complete(query).text\n",
    "\n",
    "print(\"Query engines prepared. Ready to compare RAG vs non-RAG responses!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd55f027",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: Fact check this: (The NWTsuggested caution regarding a proposal that polar bear hides be transportable to the U.S. on CITES permits. It was suggested that whalebone carvings and seal-skin products be considered first and then if there are no political problems, possibly consider polar bears. T)) If you dont know, say 'I don't know'.\n",
      "\n",
      "--- RAG Response (using our knowledge base) ---\n",
      "T. The given statement is true according to the provided context information. The relevant text states: \"The NWT suggested caution regarding a proposal that polar bear hides be transportable to the U.S. on CITES permits. It was suggested that whalebone carvings and seal-skin products be considered first and then if there are no political problems, possibly consider polar bears. The PBAC concurred because of the potential for damaging progress made in the management of polar bears in Canada.\"\n",
      "--- Direct LLM Response (without RAG) ---\n",
      "To fact-check the given statement, we need to verify its accuracy against known records or documents related to discussions or meetings where the NWT (Northwest Territories) might have discussed the transport of polar bear hides to the U.S. under CITES (Convention on International Trade in Endangered Species of Wild Fauna and Flora) permits, and the consideration of other products like whalebone carvings and seal-skin products.\n",
      "\n",
      "The statement implies a discussion or meeting took place involving the NWT, likely within a governmental or wildlife management context, where the export of certain wildlife products was debated.\n",
      "\n",
      "Without specific details on the meeting, date, or context, directly verifying this statement is challenging. However, given the subject matter, it's plausible that such discussions occurred, as the NWT is a significant jurisdiction for polar bears, and CITES regulates international trade in species that might be threatened by such trade.\n",
      "\n",
      "To directly fact-check this, one would need access to:\n",
      "1. Records or minutes from relevant meetings or discussions involving the NWT government or related wildlife management bodies.\n",
      "2. Documentation or reports related to CITES permit discussions or decisions involving Canada and the U.S.\n",
      "3. Information on the NWT's stance on exporting wildlife products, particularly polar bear hides, during the relevant time period.\n",
      "\n",
      "Given the lack of specific information or context (like a date or specific meeting minutes), I must say: **I don't know**.\n"
     ]
    }
   ],
   "source": [
    "# Query about content from Polar Paper PDF\n",
    "pdf_query = \"Fact check this: (The NWT suggested caution regarding a proposal that polar bear hides be transportable to the U.S. on CITES permits. It was suggested that whalebone carvings and seal-skin products be considered first and then if there are no political problems, possibly consider polar bears. T)) If you dont know, say 'I don't know'.\"\n",
    "\n",
    "print(f\"Query: {pdf_query}\\n\")\n",
    "\n",
    "print(\"--- RAG Response (using our knowledge base) ---\")\n",
    "rag_response = query_engine.query(pdf_query)\n",
    "print(f\"{rag_response}\")\n",
    "\n",
    "print(\"--- Direct LLM Response (without RAG) ---\")\n",
    "direct_response = get_direct_llm_response(pdf_query)\n",
    "print(direct_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9f83c3c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: What does the Toxicology and Monitoring of Pollutant Levels in Polar Bear Tissue says about the CHC levels? IMPORTANT: If you don't know, say 'I don't know'.\n",
      "\n",
      "--- RAG Response (using our knowledge base) ---\n",
      "The levels of CHCs were generally inversely correlated to latitude, and reanalysis of polar bear fat samples showed that the level of most CHCs, especially chlordane compounds, had increased from 1969 to 1984 in Hudson Bay and Baffin Bay bears.\n",
      "\n",
      "--- Direct LLM Response (without RAG) ---\n",
      "I don't know the specific details about what the Toxicology and Monitoring of Pollutant Levels in Polar Bear Tissue says about the CHC levels. If you're looking for accurate information on this topic, I recommend consulting the original research or a reliable scientific summary.\n"
     ]
    }
   ],
   "source": [
    "# Query about a specific technical detail in the paper\n",
    "technical_query = \"What does the Toxicology and Monitoring of Pollutant Levels in Polar Bear Tissue says about the CHC levels? IMPORTANT: If you don't know, say 'I don't know'.\"\n",
    "\n",
    "print(f\"Query: {technical_query}\\n\")\n",
    "\n",
    "print(\"--- RAG Response (using our knowledge base) ---\")\n",
    "rag_response = query_engine.query(technical_query)\n",
    "print(f\"{rag_response}\\n\")\n",
    "\n",
    "print(\"--- Direct LLM Response (without RAG) ---\")\n",
    "direct_response = get_direct_llm_response(technical_query)\n",
    "print(direct_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "47dc7183",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: Who are the authors of the Polar Bear Paper?. IMPORTANT: If you don't know, say 'I don't know'.\n",
      "\n",
      "--- RAG Response (using our knowledge base) ---\n",
      "Steven C. Amstrup and Oystein Wiig are the compilers and editors of the Polar Bear publication, as indicated on page 3. However, the authors of specific papers or research mentioned within the publication include various individuals such as Stirling, Schweinsburg, Kolenosky, Juniper, Robertson, Luttich, Calvelt, Sjare, Taylor, Bunnell, DeMaster, and Smith. Without more specific information about the \"Polar Bear Paper,\" it's challenging to provide a definitive list of authors. Therefore, based on the information available, the compilers and editors are Steven C. Amstrup and Oystein Wiig.\n",
      "\n",
      "--- Direct LLM Response (without RAG) ---\n",
      "I don't know.\n"
     ]
    }
   ],
   "source": [
    "# Query about authors and publication details\n",
    "authors_query = \"Who are the authors of the Polar Bear Paper?. IMPORTANT: If you don't know, say 'I don't know'.\"\n",
    "\n",
    "print(f\"Query: {authors_query}\\n\")\n",
    "\n",
    "print(\"--- RAG Response (using our knowledge base) ---\")\n",
    "rag_response = query_engine.query(authors_query)\n",
    "print(f\"{rag_response}\\n\")\n",
    "\n",
    "print(\"--- Direct LLM Response (without RAG) ---\")\n",
    "direct_response = get_direct_llm_response(authors_query)\n",
    "print(direct_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cc7718f",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook demonstrated a RAG system using LlamaIndex and KlusterAI that incorporates a PDF document as a knowledge source. We've seen:\n",
    "\n",
    "1. **How embeddings work**: We generated and visualized embeddings using the BAAI/bge-m3 model.\n",
    "2. **PDF integration**: We loaded and processed a research paper (the GPT-3 paper) for our knowledge base.\n",
    "3. **RAG vs. Direct LLM**: We compared responses from our RAG system to direct LLM outputs.\n",
    "\n",
    "**Key observations:**\n",
    "- RAG responses include specific information from the PDF that may not be in the LLM's training data\n",
    "- For queries about details in the paper, RAG provides more precise and accurate answers\n",
    "- RAG helps ground the model's responses in the actual content of the document rather than relying on the model's pre-trained knowledge\n",
    "\n",
    "**Next steps:**\n",
    "- Try with your own PDFs or other document types\n",
    "- Experiment with different chunking strategies to optimize retrieval\n",
    "- Customize the query engine with metadata filters\n",
    "- Implement more advanced RAG techniques like HyDE or reranking"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
