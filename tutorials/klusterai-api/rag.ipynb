{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "566e7e41",
   "metadata": {},
   "source": [
    "# RAG + Embeddings Endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8566fe2a",
   "metadata": {},
   "source": [
    "We'll learn how to use our [embeddings endpoint](https://docs.kluster.ai/api-reference/reference/#create-embeddings) in a Retrieval Augmented Generation (RAG) pipeline with PDF document support using [LlamaIndex](https://docs.llamaindex.ai/en/stable/module_guides/models/embeddings/)\n",
    "\n",
    "\n",
    "**Models:**\n",
    "1. **Embeddings**: Leveraging the [BAAI/bge-m3](https://platform.kluster.ai/models) model.\n",
    "2. **Language Model (LLM) for Querying**: Utilizing the [meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8](https://platform.kluster.ai/playground?model=meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8) model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82108a06",
   "metadata": {},
   "source": [
    "### Set up and Pre-requisites\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e42cda8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the necessary packages, including the PDF reader for LlamaIndex\n",
    "%pip install llama-index llama-index-llms-openai-like llama-index-embeddings-openai-like llama-index-readers-file requests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25590afc",
   "metadata": {},
   "source": [
    "### API keys and Model endpoints\n",
    "\n",
    "We'll use getpass to securely input your kluster.ai API key without displaying it.\n",
    "\n",
    "You can get your API Key from your [kluster.ai Account ](https://platform.kluster.ai/apikeys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3cfcc54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import sys\n",
    "import requests\n",
    "import json\n",
    "from getpass import getpass\n",
    "from pprint import pprint\n",
    "\n",
    "# Get API key securely using getpass\n",
    "KLUSTER_API_KEY = getpass(\"Enter your Kluster.ai API Key: \")\n",
    "KLUSTER_BASE_URL = \"https://api.kluster.ai/v1\" # Kluster.ai base URL\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = KLUSTER_API_KEY # LlamaIndex uses this env var for OpenAI-compatible APIs\n",
    "os.environ[\"OPENAI_API_BASE\"] = KLUSTER_BASE_URL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99d4865a",
   "metadata": {},
   "source": [
    "## Embedding Demonstration\n",
    "\n",
    "Let's first demonstrate how to **generate embeddings** directly using the kluster.ai dedicated endpoint.\n",
    "\n",
    "This helps illustrate what embeddings look like and how they're used in RAG systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca0e710d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample text: 'The capital of France is Paris. It is known for the Eiffel Tower.'\n",
      "Model used: BAAI/bge-m3\n",
      "Embedding dimensions: 1024\n",
      "\n",
      "First 10 dimensions of the embedding vector:\n",
      "[0.01739501953125, 0.048370361328125, -0.006679534912109375, 0.0302734375, -0.01477813720703125, -0.00627899169921875, -0.0053253173828125, 0.004657745361328125, -0.019866943359375, 0.03375244140625]\n",
      "\n",
      "Token usage: 17 tokens\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "# Configure kluster.ai client\n",
    "client = OpenAI(\n",
    "    base_url=KLUSTER_BASE_URL,\n",
    "    api_key=KLUSTER_API_KEY\n",
    ")\n",
    "\n",
    "# Generate embedding for our example text about Paris\n",
    "sample_text = \"The capital of France is Paris. It is known for the Eiffel Tower.\"\n",
    "\n",
    "response = client.embeddings.create(\n",
    "    model=\"BAAI/bge-m3\",\n",
    "    input=sample_text,\n",
    "    encoding_format=\"float\"\n",
    ")\n",
    "\n",
    "# Print the first 10 dimensions of the embedding vector\n",
    "print(f\"Sample text: '{sample_text}'\")\n",
    "print(f\"Model used: {response.model}\")\n",
    "print(f\"Embedding dimensions: {len(response.data[0].embedding)}\")\n",
    "print(\"\\nFirst 10 dimensions of the embedding vector:\")\n",
    "print(response.data[0].embedding[:10])\n",
    "\n",
    "# Show token usage information\n",
    "print(f\"\\nToken usage: {response.usage.prompt_tokens} tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91011ad2",
   "metadata": {},
   "source": [
    "## Adding a PDF Document\n",
    "\n",
    "In order to make sure our RAG is working, we need to ensure we have a document that can be used as a **knowledge base**. \n",
    "\n",
    "We'll store our PDF in the `sample_pdfs` directory in the same folder as this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d7b17b05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample PDF already exists at sample_pdfs/polar_bears.pdf\n"
     ]
    }
   ],
   "source": [
    "import urllib.request\n",
    "import os\n",
    "\n",
    "# Create a directory for our PDFs if it doesn't exist\n",
    "pdf_dir = \"sample_pdfs\"\n",
    "os.makedirs(pdf_dir, exist_ok=True)\n",
    "\n",
    "# Download a sample PDF about Polar Bears (you can replace with your own PDFs)\n",
    "sample_pdf_url = \"https://portals.iucn.org/library/sites/library/files/documents/SSC-OP-007.pdf\"  \n",
    "pdf_path = os.path.join(pdf_dir, \"polar_bears.pdf\")\n",
    "\n",
    "if not os.path.exists(pdf_path):\n",
    "    print(f\"Downloading sample PDF to {pdf_path}...\")\n",
    "    urllib.request.urlretrieve(sample_pdf_url, pdf_path)\n",
    "    print(\"Download complete!\")\n",
    "else:\n",
    "    print(f\"Sample PDF already exists at {pdf_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d8dd11",
   "metadata": {},
   "source": [
    "### Load document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "458616cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading PDF from sample_pdfs...\n",
      "Loaded 115 document(s) from PDF file\n"
     ]
    }
   ],
   "source": [
    "# Import the necessary document loader from llama_index\n",
    "from llama_index.core import Document\n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "\n",
    "# Load documents from the PDF file\n",
    "print(f\"Loading PDF from {pdf_dir}...\")\n",
    "pdf_reader = SimpleDirectoryReader(input_dir=pdf_dir)\n",
    "documents = pdf_reader.load_data()\n",
    "\n",
    "print(f\"Loaded {len(documents)} document(s) from PDF file\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df42ac97",
   "metadata": {},
   "source": [
    "## Configure LlamaIndex Components\n",
    "\n",
    "To set up [LlamaIndex](https://docs.llamaindex.ai/en/stable/) with **kluster.ai** we neesd to setup `OpenAILike` for the LLM and `OpenAILikeEmbedding` for the embedding model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d3b654",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.openai_like import OpenAILike\n",
    "from llama_index.embeddings.openai_like import OpenAILikeEmbedding\n",
    "from llama_index.core import Settings\n",
    "\n",
    "# Configure the LLM from kluster.ai with LlamaIndex\n",
    "llm = OpenAILike(\n",
    "    model=\"meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8\",\n",
    "    api_base=KLUSTER_BASE_URL,\n",
    "    api_key=KLUSTER_API_KEY,\n",
    "    is_chat_model=True\n",
    ")\n",
    "\n",
    "# Configure the embedding model from kluster.ai\n",
    "embed_model = OpenAILikeEmbedding(\n",
    "    model_name=\"BAAI/bge-m3\",\n",
    "    api_base=KLUSTER_BASE_URL, \n",
    "    api_key=KLUSTER_API_KEY\n",
    ")\n",
    "\n",
    "# Set the global settings for LlamaIndex\n",
    "Settings.llm = llm\n",
    "Settings.embed_model = embed_model\n",
    "Settings.chunk_size = 512 # Set chunk size for document splitting\n",
    "Settings.chunk_overlap = 20 # Set chunk overlap for document splitting\n",
    "\n",
    "print(\"LlamaIndex LLM and Kluster AI Embedding Model configured.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9cc04f1",
   "metadata": {},
   "source": [
    "## Create an Index\n",
    "\n",
    "Now we'll create a `VectorStoreIndex` from our PDF document.\n",
    "\n",
    "- **Index**: A searchable structure built from your documents for fast similarity search.\n",
    "- **Vector Store**: Stores the embeddings (vectors) for each document chunk, enabling rapid retrieval.\n",
    "\n",
    "**Why this matters**: Creating a `VectorStoreIndex` allows our RAG pipeline to quickly find and use the most relevant content from the PDF, grounding LLM responses in real document data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "390b784f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating index from PDF document...\n",
      "Index created successfully!\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import VectorStoreIndex\n",
    "\n",
    "# Create the index from the PDF document\n",
    "print(\"Creating index from PDF document...\")\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    documents\n",
    ")\n",
    "print(\"Index created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1239a8c0",
   "metadata": {},
   "source": [
    "### Query the Index and Compare with Non-RAG Responses\n",
    "\n",
    "Now we'll compare responses using RAG (with our knowledge base) versus direct LLM responses without context.\n",
    "First, let's start by creating the `query engine`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e415d1f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query engines prepared. Ready to compare RAG vs non-RAG responses!\n"
     ]
    }
   ],
   "source": [
    "# Create a query engine for RAG\n",
    "query_engine = index.as_query_engine()\n",
    "\n",
    "# Function to get a direct response from the LLM without using RAG\n",
    "def get_direct_llm_response(query):\n",
    "    \"\"\"Get a response directly from the LLM without using RAG\"\"\"\n",
    "    return llm.complete(query).text\n",
    "\n",
    "print(\"Query engines prepared. Ready to compare RAG vs non-RAG responses!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f48f95e",
   "metadata": {},
   "source": [
    "### Test your RAG\n",
    "\n",
    "Now we'll **compare two queries**: one using our RAG knowledge base and one using only the LLM.\n",
    "We'll ask specific questions that require information from the PDF, which the LLM alone is unlikely to answer accurately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd55f027",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: Fact check this: <quote> The NWT suggested caution regarding a proposal that polar bear hides be transportable to the U.S. on CITES permits. It was suggested that whalebone carvings and seal-skin products be considered first and then if there are no political problems, possibly consider polar bears.</quote> If you dont know, say 'I don't know'.\n",
      "\n",
      "--- RAG Response (using our knowledge base) ---\n",
      "The statement is true. The given context information contains the exact quote on page_label: 10, confirming that the NWT indeed suggested caution regarding the proposal to transport polar bear hides to the U.S. on CITES permits and recommended considering whalebone carvings and seal-skin products first.\n",
      "--- Direct LLM Response (without RAG) ---\n",
      "To fact-check the given quote, we need to verify its content and context. The quote appears to refer to a discussion or meeting involving the Northwest Territories (NWT) government or representatives, concerning the potential export of polar bear hides to the United States under CITES (Convention on International Trade in Endangered Species of Wild Fauna and Flora) permits. It also mentions considering whalebone carvings and seal-skin products for export before polar bear products.\n",
      "\n",
      "1. **CITES and Polar Bears**: Polar bears are listed under CITES, and their trade is regulated. Exporting their hides or products requires CITES permits.\n",
      "\n",
      "2. **NWT and Wildlife Trade**: The Northwest Territories, being a Canadian territory with a significant Inuit population, has a history of discussing the trade of wildlife products, including those from polar bears, seals, and whales. The territory's economy and culture are closely tied to the harvesting and trade of these resources.\n",
      "\n",
      "3. **Historical Context**: Discussions about the export of wildlife products, including those from polar bears, have occurred in various forums, including government meetings and international CITES conferences.\n",
      "\n",
      "Given the information typically available and without a specific document or meeting minutes to directly verify the quote, it is plausible that such a discussion took place. The NWT has been involved in discussions regarding the management and trade of wildlife products, including polar bear hides.\n",
      "\n",
      "To directly fact-check the quote, one would need to identify the specific meeting, document, or context in which this statement was made. \n",
      "\n",
      "However, based on the general context and known practices, the statement is **plausible**. \n",
      "\n",
      "To give a definitive answer, I would need more specific details about when and where this was discussed. Without that, I can say the content is consistent with known practices and discussions regarding wildlife trade in the NWT.\n",
      "\n",
      "If I had to choose between \"I don't know\" and a more definitive response based on plausibility and context, I would say the statement is plausible, but I couldn't confirm it without more information. Therefore, a cautious response is: **Plausible, but context is needed for confirmation**.\n"
     ]
    }
   ],
   "source": [
    "# Query about content from Polar Paper PDF\n",
    "pdf_query = \"Fact check this: <quote> The NWT suggested caution regarding a proposal that polar bear hides be transportable to the U.S. on CITES permits. It was suggested that whalebone carvings and seal-skin products be considered first and then if there are no political problems, possibly consider polar bears.</quote> If you don't know, say 'I don't know'.\"\n",
    "\n",
    "print(f\"Query: {pdf_query}\\n\")\n",
    "\n",
    "print(\"--- RAG Response (using our knowledge base) ---\")\n",
    "rag_response = query_engine.query(pdf_query)\n",
    "print(f\"{rag_response}\")\n",
    "\n",
    "print(\"--- Direct LLM Response (without RAG) ---\")\n",
    "direct_response = get_direct_llm_response(pdf_query)\n",
    "print(direct_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7b52358",
   "metadata": {},
   "source": [
    "We continue to test queries against the knowledge base to evaluate how well the RAG system retrieves and grounds answers using the PDF document. \n",
    "This helps demonstrate the effectiveness of retrieval-augmented generation compared to direct LLM responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9f83c3c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: What does the Toxicology and Monitoring of Pollutant Levels in Polar Bear Tissue says about the CHC levels? IMPORTANT: If you don't know, say 'I don't know'.\n",
      "\n",
      "--- RAG Response (using our knowledge base) ---\n",
      "The levels of CHCs were generally inversely correlated to latitude, and reanalysis of polar bear fat samples showed that the level of most CHCs, especially chlordane compounds, had increased from 1969 to 1984 in Hudson Bay and Baffin Bay bears.\n",
      "\n",
      "--- Direct LLM Response (without RAG) ---\n",
      "I don't know the specific details about what the Toxicology and Monitoring of Pollutant Levels in Polar Bear Tissue says about the CHC levels. If you're looking for accurate information on this topic, I recommend consulting the original research or a reliable scientific summary.\n"
     ]
    }
   ],
   "source": [
    "# Query about a specific technical detail in the paper\n",
    "technical_query = \"What does the Toxicology and Monitoring of Pollutant Levels in Polar Bear Tissue says about the CHC levels? IMPORTANT: If you don't know, say 'I don't know'.\"\n",
    "\n",
    "print(f\"Query: {technical_query}\\n\")\n",
    "\n",
    "print(\"--- RAG Response (using our knowledge base) ---\")\n",
    "rag_response = query_engine.query(technical_query)\n",
    "print(f\"{rag_response}\\n\")\n",
    "\n",
    "print(\"--- Direct LLM Response (without RAG) ---\")\n",
    "direct_response = get_direct_llm_response(technical_query)\n",
    "print(direct_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "47dc7183",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: Who are the authors of the Polar Bear Paper?. IMPORTANT: If you don't know, say 'I don't know'.\n",
      "\n",
      "--- RAG Response (using our knowledge base) ---\n",
      "Steven C. Amstrup and Oystein Wiig are the compilers and editors of the Polar Bear publication, as mentioned on page 3. However, the authors of specific papers or research mentioned in the document include Stirling, Schweinsburg, Kolenosky, Juniper, Robertson, Luttich, Calvelt, Sjare, Taylor, Bunnell, DeMaster, and Smith. Without more specific information about the \"Polar Bear Paper\", it's difficult to provide a definitive answer. Therefore, a more accurate response would be that the compilers and editors are Steven C. Amstrup and Oystein Wiig, but there are multiple authors for the various research papers cited.\n",
      "\n",
      "--- Direct LLM Response (without RAG) ---\n",
      "I don't know.\n"
     ]
    }
   ],
   "source": [
    "# Query about authors and publication details\n",
    "authors_query = \"Who are the authors of the Polar Bear Paper?. IMPORTANT: If you don't know, say 'I don't know'.\"\n",
    "\n",
    "print(f\"Query: {authors_query}\\n\")\n",
    "\n",
    "print(\"--- RAG Response (using our knowledge base) ---\")\n",
    "rag_response = query_engine.query(authors_query)\n",
    "print(f\"{rag_response}\\n\")\n",
    "\n",
    "print(\"--- Direct LLM Response (without RAG) ---\")\n",
    "direct_response = get_direct_llm_response(authors_query)\n",
    "print(direct_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cc7718f",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook demonstrated a RAG system using LlamaIndex and KlusterAI that incorporates a PDF document as a knowledge source. We've seen:\n",
    "\n",
    "1. **How embeddings work**: We generated and visualized embeddings using the BAAI/bge-m3 model.\n",
    "2. **PDF integration**: We loaded and processed a research paper (the GPT-3 paper) for our knowledge base.\n",
    "3. **RAG vs. Direct LLM**: We compared responses from our RAG system to direct LLM outputs.\n",
    "\n",
    "**Key observations:**\n",
    "- RAG responses include specific information from the PDF that may not be in the LLM's training data\n",
    "- For queries about details in the paper, RAG provides more precise and accurate answers\n",
    "- RAG helps ground the model's responses in the actual content of the document rather than relying on the model's pre-trained knowledge\n",
    "\n",
    "**Next steps:**\n",
    "- Try with your own PDFs or other document types\n",
    "- Experiment with different chunking strategies to optimize retrieval"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
