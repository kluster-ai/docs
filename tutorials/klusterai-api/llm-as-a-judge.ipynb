{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be189fde-4e5b-4f80-bae1-ded86a5075a7",
   "metadata": {
    "id": "be189fde-4e5b-4f80-bae1-ded86a5075a7"
   },
   "source": [
    "# Evaluating LLM performance without ground truth using an LLM judge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b17a77d9",
   "metadata": {},
   "source": [
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/kluster-ai/klusterai-cookbook/blob/main/examples/llm-as-a-judge.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "355a788f-a618-42a2-98c1-3d0e68ff766c",
   "metadata": {},
   "source": [
    "How can we test a model's accuracy when the ground truth is unavailable? One approach could be to test the predictions made by the base model against a larger model, which, comparatively, should do better.\n",
    "\n",
    "This tutorial uses a base model (`Llama-3.1-8B-Instruct-Turbo`) to classify a dataset based on a description. Next, we will use a larger model (`klusterai/Meta-Llama-3.3-70B-Instruct-Turbo`) as a judge, tasked to determine whether the base model's predicitions are correct. Since the dataset also contains the ground truth, the notebook also assesses how well the judge model performed.\n",
    "\n",
    "A great breakdown on calculating a model's accuracy can be found in our <a href=\"/tutorials/klusterai-api/model-comparison/\" target=\"_blank\"> model comparison notebook</a>.\n",
    "\n",
    "You'll be using the same dataset as in our <a href=\"/tutorials/klusterai-api/text-classification/text-classification-openai-api/\" target=\"_blank\">text classification notebook</a>, which is an extract from the IMDB top 1000 movies dataset categorized into 21 different genres.\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "Before getting started, ensure you have the following:\n",
    "\n",
    "- **A kluster.ai account** - sign up on the <a href=\"https://platform.kluster.ai/signup\" target=\"_blank\">kluster.ai platform</a> if you don't have one\n",
    "- **A kluster.ai API key** - after signing in, go to the <a href=\"https://platform.kluster.ai/apikeys\" target=\"_blank\">**API Keys**</a> section and create a new key. For detailed instructions, check out the <a href=\"/get-started/get-api-key/\" target=\"_blank\">Get an API key</a> guide\n",
    "\n",
    "## Setup\n",
    "\n",
    "In this notebook, we'll use Python's `getpass` module to input the key safely. After execution, please provide your unique kluster.ai API key (ensure no spaces)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4911e5eb-7463-4a6b-8a99-9313cecd9d4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your kluster.ai API key:  ········\n"
     ]
    }
   ],
   "source": [
    "from getpass import getpass\n",
    "\n",
    "api_key = getpass(\"Enter your kluster.ai API key: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b786c1f-360a-4929-80d5-cf57fdc5d3c7",
   "metadata": {},
   "source": [
    "Next, ensure you've installed the OpenAI Python library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bcc3d475-8f49-4fc4-9a5e-c6eb6866d2b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca8f9a8-a368-4c1b-b4f2-95b232099981",
   "metadata": {},
   "source": [
    "With the OpenAI Python library installed, we import the necessary dependencies for the tutorial:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b89a4feb-37a9-430d-a742-cd58495b4eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "import pandas as pd\n",
    "import time\n",
    "import json\n",
    "import os\n",
    "from IPython.display import clear_output, display\n",
    "import matplotlib.pyplot as plt\n",
    "import urllib.request\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "465e7cfd-1265-4501-b34e-9dac359bf785",
   "metadata": {},
   "source": [
    "Then, initialize the `client` by pointing it to the kluster.ai endpoint and passing your API key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "zG9y_WO5rYaj",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the client\n",
    "client = OpenAI(\n",
    "    base_url=\"https://api.kluster.ai/v1\",\n",
    "    api_key=api_key,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f1e29b4-4bbd-4cd2-b65c-723041c911e2",
   "metadata": {},
   "source": [
    "## Get the data\n",
    "\n",
    "Now that you've initialized an OpenAI-compatible client pointing to kluster.ai, we can talk about the data.\n",
    "\n",
    "This notebook uses a dataset from the Top 1000 IMDb Movies dataset, which contains descriptions and genres for each movie. In some cases, a movie can have more than one label. When calculating the accuracy, we'll consider the prediction correct if the predicted genre matches at least one of the genres listed in the dataset, the ground truth. This ground truth allows the notebook to calculate the accuracy and measure how well a given LLM has performed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2de9a5fe-9ebc-4c3a-8455-fdb66bd1771e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Series_Title</th>\n",
       "      <th>Genre</th>\n",
       "      <th>Overview</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The Shawshank Redemption</td>\n",
       "      <td>Drama</td>\n",
       "      <td>Two imprisoned men bond over a number of years...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The Godfather</td>\n",
       "      <td>Crime, Drama</td>\n",
       "      <td>An organized crime dynasty's aging patriarch t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The Dark Knight</td>\n",
       "      <td>Action, Crime, Drama</td>\n",
       "      <td>When the menace known as the Joker wreaks havo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Series_Title                 Genre  \\\n",
       "0  The Shawshank Redemption                 Drama   \n",
       "1             The Godfather          Crime, Drama   \n",
       "2           The Dark Knight  Action, Crime, Drama   \n",
       "\n",
       "                                            Overview  \n",
       "0  Two imprisoned men bond over a number of years...  \n",
       "1  An organized crime dynasty's aging patriarch t...  \n",
       "2  When the menace known as the Joker wreaks havo...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# IMDB Top 1000 dataset:\n",
    "url = \"https://raw.githubusercontent.com/kluster-ai/klusterai-cookbook/refs/heads/main/data/imdb_top_1000.csv\"\n",
    "urllib.request.urlretrieve(url,filename='imdb_top_1000.csv')\n",
    "\n",
    "# Load and process the dataset based on URL content\n",
    "df = pd.read_csv('imdb_top_1000.csv', usecols=['Series_Title', 'Overview', 'Genre'])\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d30ad63-8b1e-4b67-8d1c-621be45a9c23",
   "metadata": {},
   "source": [
    "## Perform batch inference\n",
    "\n",
    "To execute the batch inference job, we'll create the following functions:\n",
    "\n",
    "1. **Create the batch job file** - we'll generate a JSON lines file with the desired requests to be processed by the model. Consequently, we'll create a file for the assistant model and one for the judge model. You can also work with a single file by providing the different models for each request\n",
    "2. **Upload the batch job file** - once it is ready, we'll upload it to the <a href=\"https://platform.kluster.ai/signup\" target=\"_blank\">kluster.ai platform</a> using the API, where it will be processed. We'll receive a unique ID associated with our file\n",
    "3. **Start the batch job** - after the file is uploaded, we'll initiate the job to process the uploaded data, using the file ID obtained before\n",
    "4. **Monitor job progress** - (optional) track the status of the batch job to ensure it has been successfully completed\n",
    "5. **Retrieve results** - once the job has completed execution, we can access and process the resultant data\n",
    "\n",
    "Next, we will run the functions for the base model and feed the results to the pipeline using the judge model.\n",
    "\n",
    "This notebook is prepared for you to follow along. Run the cells below to watch it all come together.\n",
    "\n",
    "### Create the batch job file\n",
    "\n",
    "The following snippets prepare the JSONL file, where each line represents a different request. The function is set to reuse between the base and judge models.\n",
    "\n",
    "Note that each separate batch request can have its own model. Also, we are using a temperature of `0.5`, but feel free to change it and play around with the different outcomes (but we are only asking to respond with a single word, the genre)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f0903230-7384-4d7f-98dc-1c1c705c2933",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure the directory exists\n",
    "os.makedirs(\"llm_as_judge\", exist_ok=True)\n",
    "\n",
    "# Create the batch job file with the prompt and content for the model\n",
    "def create_batch_file(data, model, system_prompt):\n",
    "    batch_requests = []\n",
    "    \n",
    "    for index, row in data.iterrows():\n",
    "        content = row[\"Overview\"]\n",
    "\n",
    "        request = {\n",
    "            \"custom_id\": f\"{model}-{index}-analysis\",\n",
    "            \"method\": \"POST\",\n",
    "            \"url\": \"/v1/chat/completions\",\n",
    "            \"body\": {\n",
    "                \"model\": model,\n",
    "                \"temperature\": 0.5,\n",
    "                \"messages\": [\n",
    "                    {\"role\": \"system\", \"content\": system_prompt},\n",
    "                    {\"role\": \"user\", \"content\": content},\n",
    "                ],\n",
    "            },\n",
    "        }\n",
    "        \n",
    "        batch_requests.append(request)\n",
    "\n",
    "    return batch_requests\n",
    "\n",
    "\n",
    "# Save file\n",
    "def save_batch_file(batch_requests, model):\n",
    "    filename = f\"llm_as_judge/batch_job_{model}_request.jsonl\"\n",
    "    with open(filename, \"w\") as file:\n",
    "        for request in batch_requests:\n",
    "            file.write(json.dumps(request) + \"\\n\")\n",
    "    return filename\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eabae7d0-4d1e-431a-98b4-4caed514d4d0",
   "metadata": {},
   "source": [
    "### Upload batch job file to kluster.ai\n",
    "\n",
    "Once we've prepared our input file, it's time to upload them to the kluster.ai platform. To do so, you can use the `files.create` endpoint of the client, where the purpose is set to `batch`. This will return the file ID, which we need to log for the next steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "89408bd4-51b2-490e-b5ce-da002c41921c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_batch_file(data_dir):\n",
    "  print(f\"Creating request for {data_dir}\")\n",
    "  \n",
    "  with open(data_dir, 'rb') as file:\n",
    "    upload_response = client.files.create(\n",
    "    file=file,\n",
    "    purpose=\"batch\"\n",
    "  )\n",
    "\n",
    "  # Print job ID\n",
    "  file_id = upload_response.id\n",
    "  print(f\"File uploaded successfully. File ID: {file_id}\")\n",
    "\n",
    "  return upload_response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bee042b2-aed6-423d-8581-e56b3aeb312d",
   "metadata": {},
   "source": [
    "### Start the job\n",
    "\n",
    "Once all the files have been successfully uploaded, we're ready to start (create) the batch jobs by providing the file ID. To start each job, we use the `batches.create` method, for which we need to set the endpoint to `/v1/chat/completions`. This will return each batch job details, with each ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ef3e5446-ad11-4d4f-8c02-e27e32905445",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create batch job with completions endpoint\n",
    "def create_batch_job(file_id):\n",
    "  batch_job = client.batches.create(\n",
    "    input_file_id=file_id,\n",
    "    endpoint=\"/v1/chat/completions\",\n",
    "    completion_window=\"24h\"\n",
    "  )\n",
    "\n",
    "  print(f\"Batch job created with ID {batch_job.id}\")\n",
    "  return batch_job"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fb0c8a4-5666-4740-88ea-e5633bc7e2d5",
   "metadata": {},
   "source": [
    "### Check job progress\n",
    "\n",
    "Once your batch jobs have been created, you can track their progress.\n",
    "\n",
    "To monitor the job's progress, we can use the `batches.retrieve` method and pass the batch job ID. The response contains a `status` field that tells whether it is completed and the subsequent status of each job separately. We can repeat this process for every batch job ID we get in the previous step.\n",
    "\n",
    "The following snippet checks the status of all batch jobs every 10 seconds until the entire batch is completed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "72c2c7ba-d933-4ebe-997d-6d529292e21f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def monitor_batch_job(job):\n",
    "    completed = False\n",
    "\n",
    "    # Loop until all jobs are completed\n",
    "    while not completed:\n",
    "        completed = True\n",
    "        output_lines = []\n",
    "\n",
    "        updated_job = client.batches.retrieve(job.id)\n",
    "        status = updated_job.status\n",
    "\n",
    "        # If job is completed\n",
    "        if status == \"completed\":\n",
    "            output_lines.append(\"Job completed!\")\n",
    "        # If job failed, cancelled or expired\n",
    "        elif status in [\"failed\", \"cancelled\", \"expired\"]:\n",
    "            output_lines.append(f\"Job ended with status: {status}\")\n",
    "            break\n",
    "        # If job is ongoing\n",
    "        else:\n",
    "            completed = False\n",
    "            completed = updated_job.request_counts.completed\n",
    "            total = updated_job.request_counts.total\n",
    "            output_lines.append(\n",
    "                f\"Job status: {status} - Progress: {completed}/{total}\"\n",
    "            )\n",
    "\n",
    "        # Check every 10 seconds\n",
    "        if not completed:\n",
    "            time.sleep(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac21c8ec-284f-4d46-8e43-b1a6cdab0425",
   "metadata": {},
   "source": [
    "## Get the results\n",
    "\n",
    "When the batch job is completed, we'll retrieve the results and review the responses generated for each request. The results are parsed. To fetch the results from the platform, you must retrieve the `output_file_id` from the batch job and then use the `files.content` endpoint, providing that specific file ID. We will repeat this for every single batch job id. Note that the job status must be `completed` to retrieve the results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e23944f9-9f8d-477c-9bd6-0667e9def190",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parse results as a JSON object\n",
    "def parse_json_objects(data_string):\n",
    "    if isinstance(data_string, bytes):\n",
    "        data_string = data_string.decode('utf-8')\n",
    "\n",
    "    json_strings = data_string.strip().split('\\n')\n",
    "    json_objects = []\n",
    "\n",
    "    for json_str in json_strings:\n",
    "        try:\n",
    "            json_obj = json.loads(json_str)\n",
    "            json_objects.append(json_obj)\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"Error parsing JSON: {e}\")\n",
    "\n",
    "    return json_objects\n",
    "\n",
    "# Retrieve results with job ID\n",
    "def retrieve_results(batch_job):\n",
    "    job = client.batches.retrieve(batch_job.id)\n",
    "    result_file_id = job.output_file_id\n",
    "    result = client.files.content(result_file_id).content\n",
    "\n",
    "    # Parse JSON results\n",
    "    return parse_json_objects(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "669c99c7-9d84-4fbb-b410-181464500af3",
   "metadata": {},
   "source": [
    "Now that the basic inference pipeline has been established, let's run it first for the base model.\n",
    "\n",
    "## Batch inference for the base model\n",
    "\n",
    "This example uses Llama 3.1 8B as the base model. If you'd like to test different models, feel free to modify the scripts accordingly.\n",
    "\n",
    "Please refer to the <a href=\"/get-started/models/#model-comparison-table\" target=\"_blank\">Supported models</a> section for a list of the models we support.\n",
    "\n",
    "For the base model, the prompt is pretty similar to that of the <a href=\"/tutorials/klusterai-api/text-classification/text-classification-openai-api/\" target=\"_blank\">text classification notebook</a>, where we ask to classify each move genre based of a description, and providing a specific set of options as possible genres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0787daf6-8386-4829-96b4-fea90df02808",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Job status: in_progress - Progress: 194/1000'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# System prompt\n",
    "SYSTEM_PROMPT_BASE = \"\"\"\n",
    "    You are a helpful assistant who classifies movie genres based on the provided description. Choose one of the following options: \n",
    "    Action, Adventure, Animation, Biography, Comedy, Crime, Drama, Family, Fantasy, Film-Noir, History, Horror, Music, Musical, Mystery, Romance, Sci-Fi, Sport, Thriller, War, Western.\n",
    "    Provide your response as a single word with the matching genre. Don't include punctuation.\n",
    "    \"\"\"\n",
    "\n",
    "# Model\n",
    "model_name = \"Llama3.1-8B\"\n",
    "model = \"klusterai/Meta-Llama-3.1-8B-Instruct-Turbo\"\n",
    "\n",
    "# Create batch file\n",
    "batch_request = create_batch_file(df, model, SYSTEM_PROMPT_BASE)\n",
    "filename = save_batch_file(batch_request, model_name)\n",
    "print(f\"Batch file created {filename}\")\n",
    "\n",
    "# Upload batch file\n",
    "batch_file = upload_batch_file(filename)\n",
    "\n",
    "# Create batch job\n",
    "batch_job = create_batch_job(batch_file.id)\n",
    "\n",
    "# Monitor batch job\n",
    "monitor_batch_job(batch_job)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e4908c0-7224-4f9f-81fc-4774d47d153b",
   "metadata": {},
   "source": [
    "Next, we can preview what one of the requests in a batch job files looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "426dbf80-2f88-44eb-937c-2124961f6474",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"custom_id\": \"klusterai/Meta-Llama-3.1-8B-Instruct-Turbo-0-analysis\", \"method\": \"POST\", \"url\": \"/v1/chat/completions\", \"body\": {\"model\": \"klusterai/Meta-Llama-3.1-8B-Instruct-Turbo\", \"temperature\": 0.5, \"messages\": [{\"role\": \"system\", \"content\": \"\\n    You are a helpful assistant who classifies movie genres based on the provided description. Choose one of the following options: \\n    Action, Adventure, Animation, Biography, Comedy, Crime, Drama, Family, Fantasy, Film-Noir, History, Horror, Music, Musical, Mystery, Romance, Sci-Fi, Sport, Thriller, War, Western.\\n    Provide your response as a single word with the matching genre. Don't include punctuation.\\n    \"}, {\"role\": \"user\", \"content\": \"Two imprisoned men bond over a number of years, finding solace and eventual redemption through acts of common decency.\"}]}}\n"
     ]
    }
   ],
   "source": [
    "!head -n 1 $filename"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5471eefa-277a-445a-91dc-f2790a3d4bd3",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee44b12-96db-4cbe-905f-7c66040caa89",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "285597a1-7b03-4036-a8f9-6ad98c0ce28d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "626e0642-4ee7-4afd-950d-22fbdff9fb84",
   "metadata": {},
   "source": [
    "This example uses Llama 3.1 8B as the base model and the larger Llama 3.3 70B as the judge model (the artificial ground truth). If you'd like to test different models, feel free to modify the scripts accordingly.\n",
    "\n",
    "Please refer to the <a href=\"/get-started/models/#model-comparison-table\" target=\"_blank\">Supported models</a> section for a list of the models we support.\n",
    "\n",
    "For the base model, the prompt is pretty similar to that of the <a href=\"/tutorials/klusterai-api/text-classification/text-classification-openai-api/\" target=\"_blank\">text classification notebook</a>. For the judge model, we must be very specific about the task to be executed, providing unambiguous guidelines on what constitutes a correct and incorrect prediction by the base model. For example, you must also consider cases in which the base model offers a response that is not formatted correctly. You might need to tune each prompt to ensure the judge model accurately measures the base model response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b4b098f6-1425-480d-a69b-a5bad930f2f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Model\n",
    "model = \"klusterai/Meta-Llama-3.1-8B-Instruct-Turbo\"\n",
    "\n",
    "SYSTEM_PROMPT_JUDGE = \"\"\"\n",
    "    You will receive a movie description, a list of possible genres, and a predicted movie genre made by another LLM.\n",
    "    Your task is to evaluate whether the predicted genre is ‘correct’ or ‘incorrect’ based on the following steps and requirements.\n",
    "    \n",
    "    Steps to Follow:\n",
    "    1. Carefully read the movie description.\n",
    "    2. Determine your own classification of the genre for the movie. Do not rely on the LLM's answer since it may be incorrect. Do not rely on individual words to identify the genre; read the whole description to identify the genre.\n",
    "    3. Read the LLM answer (enclosed in double quotes) and evaluate if it is correct by following the Evaluation Criteria below.\n",
    "    4. Provide your evaluation as 'correct' or 'incorrect'.\n",
    "    \n",
    "    Evaluation Criteria:\n",
    "    - Ensure the LLM answer (enclosed in double quotes) is one of the provided genres. If it is not listed, the evaluation should be ‘incorrect’.\n",
    "    - If the LLM answer (enclosed in double quotes) does not align with the movie description, the evaluation should be ‘incorrect’.\n",
    "    - The first letter of the LLM answer (enclosed in double quotes) must be capitalized (e.g., Drama). If it has any other capitalization, the evaluation should be ‘incorrect’.\n",
    "    - All other letters in the LLM answer (enclosed in double quotes) must be lowercase. Otherwise, the evaluation should be ‘incorrect’.\n",
    "    - If the LLM answer consists of multiple words, the evaluation should be ‘incorrect’.\n",
    "    - If the LLM answer includes punctuation, spaces, or additional characters, the evaluation should be ‘incorrect’.\n",
    "    \n",
    "    Output Rules:\n",
    "    - Provide the evaluation with no additional text, punctuation, or explanation.\n",
    "    - The output should be in lowercase.\n",
    "    \n",
    "    Final Answer Format:\n",
    "    evaluation\n",
    "    \n",
    "    Example:\n",
    "    correct\n",
    "    \"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9f15864-1b6b-477a-a0cf-75863b917499",
   "metadata": {},
   "source": [
    "## Build our evaluation pipeline\n",
    "\n",
    "In this section, we'll create several utility functions that will help us:\n",
    "\n",
    "1. Prepare our data for batch processing\n",
    "2. Send requests to the kluster.ai API\n",
    "3. Monitor the progress of our evaluation\n",
    "4. Collect and analyze results\n",
    "\n",
    "These functions will make our evaluation process more efficient and organized. Let's go through each one and understand its purpose.\n",
    "\n",
    "1. **`create_tasks()`** - formats our data for the API\n",
    "2. **`save_tasks()`** - prepares batch files for processing\n",
    "3. **`monitor_job_status()`** - tracks evaluation progress\n",
    "4. **`get_results()`** - collects and processes model outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0ae3e6f-2534-4541-812a-bcfc62a747bc",
   "metadata": {},
   "source": [
    "### Create and manage batch files\n",
    "\n",
    "A batch file in our context is a collection of requests that we'll send to our models for evaluation. Think of it as a organized list of tasks we want our models to complete.\n",
    "\n",
    "We'll take the following steps to create batch files:\n",
    "\n",
    "1. **Creating tasks** - we'll convert each movie description into a format LLMs can process\n",
    "2. **Organizing data** -we'll add necessary metadata and instructions for each task\n",
    "3. **Saving files** - we'll store these tasks in a structured format (JSONL) for processing\n",
    "\n",
    "Let's break down the key components of our batch file creation:\n",
    "- **`custom_id`** - helps us track individual requests\n",
    "- **`system_prompt`** - provides instructions to the model\n",
    "- **`content`** - the actual text we want to classify\n",
    "\n",
    "This structured approach allows us to efficiently process multiple requests in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fVtwyqZ_nEq7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tasks(user_contents, system_prompt, task_type, model):\n",
    "    tasks = []\n",
    "    for index, user_content in enumerate(user_contents):\n",
    "        task = {\n",
    "            \"custom_id\": f\"{task_type}-{index}\",\n",
    "            \"method\": \"POST\",\n",
    "            \"url\": \"/v1/chat/completions\",\n",
    "            \"body\": {\n",
    "                \"model\": model,\n",
    "                \"temperature\": 0,\n",
    "                \"messages\": [\n",
    "                    {\"role\": \"system\", \"content\": system_prompt},\n",
    "                    {\"role\": \"user\", \"content\": user_content},\n",
    "                ],\n",
    "            }\n",
    "        }\n",
    "        tasks.append(task)\n",
    "    return tasks\n",
    "\n",
    "def save_tasks(tasks, task_type):\n",
    "    filename = f\"batch_tasks_{task_type}.jsonl\"\n",
    "    with open(filename, 'w') as file:\n",
    "        for task in tasks:\n",
    "            file.write(json.dumps(task) + '\\n')\n",
    "    return filename"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30456bd4-380b-4797-9eb9-6fd486389766",
   "metadata": {},
   "source": [
    "### Upload files to kluster.ai\n",
    "\n",
    "Now that we've prepared our batch files, we'll upload them to the <a href=\"https://platform.kluster.ai/\" target=\"_blank\">kluster.ai platform</a> for batch inference. This step is crucial for:\n",
    "\n",
    "1. Getting our data to the models\n",
    "2. Setting up the processing queue\n",
    "3. Preparing for inference\n",
    "\n",
    "Once the upload is complete, the following actions will take place:\n",
    "\n",
    "1. The platform queues our requests\n",
    "2. Models process them efficiently\n",
    "3. Results are made available for collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "l5eu5UyAnEtk",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_batch_job(file_name):\n",
    "    print(f\"Creating batch job for {file_name}\")\n",
    "    batch_file = client.files.create(\n",
    "        file=open(file_name, \"rb\"),\n",
    "        purpose=\"batch\"\n",
    "    )\n",
    "\n",
    "    batch_job = client.batches.create(\n",
    "        input_file_id=batch_file.id,\n",
    "        endpoint=\"/v1/chat/completions\",\n",
    "        completion_window=\"24h\"\n",
    "    )\n",
    "\n",
    "    return batch_job"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d292ef95-2f40-442e-8075-e8953d431f1d",
   "metadata": {},
   "source": [
    "### Check job progress\n",
    "\n",
    "This function provides real-time monitoring of batch job progress:\n",
    "\n",
    "- Continuously checks job status via the kluster.ai API\n",
    "- Displays current completion count (completed/total requests)\n",
    "- Updates status every 10 seconds until job is finished\n",
    "- Automatically clears previous output for clean progress tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "SuH0CfoqjP3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_json_objects(data_string):\n",
    "    if isinstance(data_string, bytes):\n",
    "        data_string = data_string.decode('utf-8')\n",
    "\n",
    "    json_strings = data_string.strip().split('\\n')\n",
    "    json_objects = []\n",
    "\n",
    "    for json_str in json_strings:\n",
    "        try:\n",
    "            json_obj = json.loads(json_str)\n",
    "            json_objects.append(json_obj)\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"Error parsing JSON: {e}\")\n",
    "\n",
    "    return json_objects\n",
    "\n",
    "def monitor_job_status(client, job_id, task_type):\n",
    "    all_completed = False\n",
    "\n",
    "    while not all_completed:\n",
    "        all_completed = True\n",
    "        output_lines = []\n",
    "\n",
    "        updated_job = client.batches.retrieve(job_id)\n",
    "\n",
    "        if updated_job.status.lower() != \"completed\":\n",
    "            all_completed = False\n",
    "            completed = updated_job.request_counts.completed\n",
    "            total = updated_job.request_counts.total\n",
    "            output_lines.append(f\"{task_type.capitalize()} job status: {updated_job.status} - Progress: {completed}/{total}\")\n",
    "        else:\n",
    "            output_lines.append(f\"{task_type.capitalize()} job completed!\")\n",
    "\n",
    "        # Clear the output and display updated status\n",
    "        clear_output(wait=True)\n",
    "        for line in output_lines:\n",
    "            display(line)\n",
    "\n",
    "        if not all_completed:\n",
    "            time.sleep(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd47b7f-ff7e-4b87-a50c-4119bc03add6",
   "metadata": {},
   "source": [
    "### Collect and process results\n",
    "\n",
    "The `get_results()` function below does the following:\n",
    "\n",
    "1. Retrieves the completed batch job results\n",
    "2. Extracts the model's response content from each result\n",
    "3. Returns a list of all model responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "caA5djbLrJ4O",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_results(client, job_id):\n",
    "    batch_job = client.batches.retrieve(job_id)\n",
    "    result_file_id = batch_job.output_file_id\n",
    "    result = client.files.content(result_file_id).content\n",
    "    results = parse_json_objects(result)\n",
    "    answers = []\n",
    "    \n",
    "    for res in results:\n",
    "        result = res['response']['body']['choices'][0]['message']['content']\n",
    "        answers.append(result)\n",
    "    \n",
    "    return answers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c120731-3a44-465e-8ec6-a2d746ac2901",
   "metadata": {
    "id": "udPtLfTaisSw"
   },
   "source": [
    "## Data acquisition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abb279d1-ca98-4933-aaec-c41bc1a279f3",
   "metadata": {},
   "source": [
    "Now that we have covered the core general functions and workflow used for batch inference, in this guide, we’ll be using the IMDb Top 1000 dataset, which contains information about top-rated movies, including their descriptions and genres. Let's download it and see what it looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "yC9wJlV4rwOh",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Series_Title</th>\n",
       "      <th>Overview</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>700</th>\n",
       "      <td>Wait Until Dark</td>\n",
       "      <td>A recently blinded woman is terrorized by a trio of thugs while they search for a heroin-stuffed doll they believe is in her apartment.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>701</th>\n",
       "      <td>Guess Who's Coming to Dinner</td>\n",
       "      <td>A couple's attitudes are challenged when their daughter introduces them to her African-American fianc\u001a.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>702</th>\n",
       "      <td>Bonnie and Clyde</td>\n",
       "      <td>Bored waitress Bonnie Parker falls in love with an ex-con named Clyde Barrow and together they start a violent crime spree through the country, stealing cars and robbing banks.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Series_Title                                                                                                                                                                          Overview\n",
       "700               Wait Until Dark                                           A recently blinded woman is terrorized by a trio of thugs while they search for a heroin-stuffed doll they believe is in her apartment.\n",
       "701  Guess Who's Coming to Dinner                                                                           A couple's attitudes are challenged when their daughter introduces them to her African-American fianc\u001a.\n",
       "702              Bonnie and Clyde  Bored waitress Bonnie Parker falls in love with an ex-con named Clyde Barrow and together they start a violent crime spree through the country, stealing cars and robbing banks."
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# IMDB Top 1000 dataset:\n",
    "url = \"https://raw.githubusercontent.com/kluster-ai/klusterai-cookbook/refs/heads/main/data/imdb_top_1000.csv\"\n",
    "urllib.request.urlretrieve(url,filename='imdb_top_1000.csv')\n",
    "\n",
    "# Load and process the dataset based on URL content\n",
    "df = pd.read_csv('imdb_top_1000.csv', usecols=['Series_Title', 'Overview', 'Genre']).tail(300)\n",
    "df[['Series_Title','Overview']].head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "135f2093-5d10-45c7-a627-3850b55cc4ed",
   "metadata": {},
   "source": [
    "## Performing batch inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6417258-c279-446c-8060-6f05f07a1572",
   "metadata": {},
   "source": [
    "In this section, we will perform batch inference using the previously defined helper functions and the IMDb dataset. The goal is to classify movie genres based on their descriptions using a Large Language Model (LLM).\n",
    "\n",
    "We define the input prompts for the LLM, which consist of a system prompt outlining the task and user content, which includes a list of movie descriptions from our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "132fc26f-efe9-408d-b3f8-63e76c734f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_dict = {\n",
    "    \"ASSISTANT_PROMPT\" : '''\n",
    "        You are a helpful assitant that classifies movie genres based on the movie description. Choose one of the following options: \n",
    "        Action, Adventure, Animation, Biography, Comedy, Crime, Drama, Family, Fantasy, Film-Noir, History, Horror, Music, Musical, Mystery, Romance, Sci-Fi, Sport, Thriller, War, Western.\n",
    "        Provide your response as a single word with the matching genre. Don't include punctuation.\n",
    "    ''',\n",
    "    \"USER_CONTENTS\" : df['Overview'].tolist()\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f99550a3-006a-430d-8f0c-4b4a07b66f79",
   "metadata": {},
   "source": [
    "Next, we'll create and save the tasks, submit the batch inference job, and monitor its progress. Once the process is complete, the predictions will be integrated into the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "qNhmrmHdnp7g",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Assistant job completed!'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "task_list = create_tasks(user_contents=prompt_dict[\"USER_CONTENTS\"], \n",
    "                         system_prompt=prompt_dict[\"ASSISTANT_PROMPT\"], \n",
    "                         model=\"klusterai/Meta-Llama-3.1-8B-Instruct-Turbo\", \n",
    "                         task_type='assistant')\n",
    "filename = save_tasks(task_list, task_type='assistant')\n",
    "job = create_batch_job(filename)\n",
    "monitor_job_status(client=client, job_id=job.id, task_type='assistant')\n",
    "df['predicted_genre'] = get_results(client=client, job_id=job.id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e68b9c6-5d0a-4641-bc89-cbe432656ea2",
   "metadata": {},
   "source": [
    "## LLM as a judge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e74823-addd-480a-925b-a90198db62d3",
   "metadata": {},
   "source": [
    "This section evaluates the performance of the initial LLM predictions. We use another LLM as a judge to assess whether the predicted genres align with the movie descriptions.\n",
    "\n",
    "First, we define the input prompts for the LLM judge. These prompts include the movie description, a list of possible genres, and the genre predicted by the first LLM. The judge LLM evaluates the correctness of the predictions based on specific criteria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "18327c55-35ad-44a5-bdb2-eae0cbb63cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_dict = {\n",
    "    \"JUDGE_PROMPT\" : '''\n",
    "        You will be provided with a movie description, a list of possible genres, and a predicted movie genre made by another LLM. Your task is to evaluate whether the predicted genre is ‘correct’ or ‘incorrect’ based on the following steps and requirements.\n",
    "        \n",
    "        Steps to Follow:\n",
    "        1. Carefully read the movie description.\n",
    "        2. Determine your own classification of the genre for the movie. Do not rely on the LLM's answer since it may be incorrect. Do not rely on individual words to identify the genre; read the whole description to identify the genre.\n",
    "        3. Read the LLM answer (enclosed in double quotes) and evaluate if it is the correct answer by following the Evaluation Criteria mentioned below.\n",
    "        4. Provide your evaluation as 'correct' or 'incorrect'.\n",
    "        \n",
    "        Evaluation Criteria:\n",
    "        - Ensure the LLM answer (enclosed in double quotes) is one of the provided genres. If it is not listed, the evaluation should be ‘incorrect’.\n",
    "        - If the LLM answer (enclosed in double quotes) does not align with the movie description, the evaluation should be ‘incorrect’.\n",
    "        - The first letter of the LLM answer (enclosed in double quotes) must be capitalized (e.g., Drama). If it has any other capitalization, the evaluation should be ‘incorrect’.\n",
    "        - All other letters in the LLM answer (enclosed in double quotes) must be lowercase. Otherwise, the evaluation should be ‘incorrect’.\n",
    "        - If the LLM answer consists of multiple words, the evaluation should be ‘incorrect’.\n",
    "        - If the LLM answer includes punctuation, spaces, or additional characters, the evaluation should be ‘incorrect’.\n",
    "        \n",
    "        Output Rules:\n",
    "        - Provide the evaluation with no additional text, punctuation, or explanation.\n",
    "        - The output should be in lowercase.\n",
    "        \n",
    "        Final Answer Format:\n",
    "        evaluation\n",
    "        \n",
    "        Example:\n",
    "        correct\n",
    "    ''',\n",
    "    \"USER_CONTENTS\" : [f'''Movie Description: {row['Overview']}.\n",
    "        Available Genres: Action, Adventure, Animation, Biography, Comedy, Crime, Drama, Family, Fantasy, Film-Noir, History, Horror, Music, Musical, Mystery, Romance, Sci-Fi, Sport, Thriller, War, Western\n",
    "        LLM answer: \"{row['predicted_genre']}\"\n",
    "        ''' for _, row in df.iterrows()\n",
    "        ]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5750125-8bf6-4a29-8850-60e86e2d767b",
   "metadata": {},
   "source": [
    "Following the same set of steps as the previous inference, we will create and save the tasks, submit the batch inference job, and monitor its progress. Once the process is complete, the predictions will also be integrated into the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "337e3e75-21ae-4e5e-91de-eb637a0f9b24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Judge job completed!'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "task_list = create_tasks(user_contents=prompt_dict[\"USER_CONTENTS\"], \n",
    "                         system_prompt=prompt_dict[\"JUDGE_PROMPT\"], \n",
    "                         task_type='judge', \n",
    "                         model=\"klusterai/Meta-Llama-3.3-70B-Instruct-Turbo\")\n",
    "filename = save_tasks(task_list, task_type='judge')\n",
    "job = create_batch_job(filename)\n",
    "monitor_job_status(client=client, job_id=job.id, task_type='judge')\n",
    "df['judge_evaluation'] = get_results(client=client, job_id=job.id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f5abf74-d428-4f08-9b62-2f4ec61e6c21",
   "metadata": {},
   "source": [
    "Now, we will calculate the LLM classification accuracy based on what the LLM judge considers correct or incorrect. For this purpose, we will compute the accuracy. If you are unfamiliar with accuracy metrics, please refer to our previous <a href=\"https://github.com/kluster-ai/klusterai-cookbook/blob/main/examples/model-comparison.ipynb\" target=\"_blank\">notebook</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "243b7784-ebf0-4d58-a859-73dc08dc2212",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM Judge-determined accuracy:  0.86\n"
     ]
    }
   ],
   "source": [
    "print('LLM Judge-determined accuracy: ',df['judge_evaluation'].value_counts(normalize=True)['correct'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daa5a38e-a9aa-457f-92b4-e9cbf5af810f",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89ecd20b-fda8-4ad4-bd30-bf4d434ee469",
   "metadata": {},
   "source": [
    "According to the LLM judge, the baseline model's accuracy was 82%. This demonstrates how, in situations where we lack ground truth, we can leverage a large-language model to evaluate the responses of another model. By doing so, we can establish a ground truth or an evaluation metric to assess model performance, refine prompts, or understand how well the model performs.\n",
    "\n",
    "This approach is particularly valuable when dealing with large datasets containing thousands of entries, where manual evaluation would be impractical. Automating this process saves significant time and reduces costs by eliminating the need for extensive human annotations. Ultimately, it provides a scalable and efficient way to gain meaningful insights into model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62503e26-6f37-4b61-b920-474d1eccf893",
   "metadata": {},
   "source": [
    "### (Optional) Validation against ground truth"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e95fb48d-79a8-4f3c-a85f-8ba5c0dda486",
   "metadata": {},
   "source": [
    "According to the LLM judge, the baseline model's accuracy is 82%. But how accurate is this evaluation? In this particular case, the IMDb Top 1000 dataset provides ground truth labels, allowing us to calculate the accuracy of the predicted genres directly. Let's compare and see how close the results are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "31503346-67e8-4e16-a44a-1bc91f67bceb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM ground truth accuracy:  0.7833333333333333\n"
     ]
    }
   ],
   "source": [
    "print('LLM ground truth accuracy: ',df.apply(lambda row: row['predicted_genre'] in row['Genre'].split(', '), axis=1).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "018c2e90-8fa1-4861-87f7-cf09a52cd25a",
   "metadata": {},
   "source": [
    "Although the ground truth accuracy is not exactly identical to the evaluation provided by the LLM judge, in situations where we lack ground truth, using an LLM as an evaluator offers a valuable way to assess how well our baseline model is performing."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
