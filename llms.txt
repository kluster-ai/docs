# llms.txt
# Generated automatically. Do not edit directly.

Documentation: https://docs.kluster.ai/

# List of doc pages:
Doc-Page: https://docs.kluster.ai/api-reference/reference/
Doc-Page: https://docs.kluster.ai/get-started/get-api-key/
Doc-Page: https://docs.kluster.ai/get-started/integrations/crewai/
Doc-Page: https://docs.kluster.ai/get-started/integrations/eliza/
Doc-Page: https://docs.kluster.ai/get-started/integrations/immersive-translate/
Doc-Page: https://docs.kluster.ai/get-started/integrations/langchain/
Doc-Page: https://docs.kluster.ai/get-started/integrations/litellm/
Doc-Page: https://docs.kluster.ai/get-started/integrations/sillytavern/
Doc-Page: https://docs.kluster.ai/get-started/openai-compatibility/
Doc-Page: https://docs.kluster.ai/get-started/start-api/

# Full content for each doc page

Doc-Content: https://docs.kluster.ai/api-reference/reference/
--- BEGIN CONTENT ---
---
title: API Reference
description: Explore the kluster.ai API reference to get a comprehensive overview on the available endpoints, request and response formats, and integration examples.
hide:
 - navigation
template: api.html
---

# API reference

## Chat

### Create chat completion

`POST https://api.kluster.ai/v1/chat/completions`

To create a chat completion, send a request to the `chat/completions` endpoint.

<div class="grid" markdown>
<div markdown>

**Request**

`model` ++"string"++ <span class="required" markdown>++"required"++</span>

ID of the model to use. You can use the `models` endpoint to retrieve the [list of supported models](#list-supported-models){target=\_blank}.

---

`messages` ++"array"++ <span class="required" markdown>++"required"++</span>

A list of messages comprising the conversation so far. The `messages` object can be one of `system`, `user`, or `assistant`.

??? child "Show possible types"

    System message ++"object"++
    
    ??? child "Show properties"

        `content` ++"string or array"++

        The contents of the system message.  

        ---
       
        `role` ++"string or null"++ <span class="required" markdown>++"required"++</span>

        The role of the messages author, in this case, `system`.

    ---

    User message ++"object"++

    ??? child "Show properties"

        `content` ++"string or array"++

        The contents of the user message.  

        ---
       
        `role` ++"string or null"++ <span class="required" markdown>++"required"++</span>

        The role of the messages author, in this case, `user`.

    ---

    Assistant message ++"object"++

    ??? child "Show properties"

        `content` ++"string or array"++

        The contents of the assistant message.  

        ---

        `role` ++"string or null"++ <span class="required" markdown>++"required"++</span>

        The role of the messages author, in this case, `assistant`.

`frequency_penalty` ++"number or null"++

Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood of repeating the same line verbatim. Defaults to `0`.

---

`logit_bias` ++"map"++

Modify the likelihood of specified tokens appearing in the completion. Defaults to `null`.

Accepts a JSON object that maps tokens (specified by their token ID in the tokenizer) to an associated bias value from -100 to 100. Mathematically, the bias is added to the logits generated by the model prior to sampling. The exact effect will vary per model, but values between -1 and 1 should decrease or increase the likelihood of selection; values like -100 or 100 should result in a ban or exclusive selection of the relevant token.

---

`logprobs` ++"boolean or null"++
   
Whether to return log probabilities of the output tokens or not. If true, returns the log probabilities of each output token returned in the `content` of `message`. Defaults to `false`.

---

`top_logprobs` ++"integer or null"++

An integer between 0 and 20 specifying the number of most likely tokens to return at each token position, each with an associated log probability. `logprobs` must be set to `true` if this parameter is used.

---

`max_completion_tokens` ++"integer or null"++

An upper bound for the number of tokens that can be generated for a completion, including visible output tokens and reasoning tokens.

---

`presence_penalty` ++"number or null"++

Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics. Defaults to `0`.

---

`seed` ++"integer or null"++

If specified, our system will make a best effort to sample deterministically, such that repeated requests with the same `seed` and parameters should return the same result. Determinism is not guaranteed.

---

`stop` ++"string or array or null"++

Up to four sequences where the API will stop generating further tokens. Defaults to `null`.

---

`stream` ++"boolean or null"++

If set, partial message deltas will be sent. Tokens will be sent as data-only server-sent events as they become available, with the stream terminated by a `data: [DONE]` message. Defaults to `false`.

---

`temperature` ++"number or null"++

The sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. Defaults to `1`.

It is generally recommended to alter this or `top_p` but not both.

---

`top_p` ++"number or null"++

An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered. Defaults to `1`.

It is generally recommended to alter this or `temperature` but not both.

---

**Returns**

The created [Chat completion](#batch-object) object.

</div>
<div markdown>

=== "Python"

    ```python title="Example request"
    from openai import OpenAI

    # Configure OpenAI client
    client = OpenAI(
        base_url="https://api.kluster.ai/v1", 
        api_key="INSERT_API_KEY" # Replace with your actual API key
    )

    chat_completion = client.chat.completions.create(
        model="klusterai/Meta-Llama-3.1-8B-Instruct-Turbo",
        messages=[
            {"role": "system", "content": "You are a helpful assistant."},
            {"role": "user", "content": "What is the capital of Argentina?"},
        ],
    )

    print(chat_completion.to_dict())
    ```

=== "curl"

    ```bash title="Example request"
    curl -s https://api.kluster.ai/v1/chat/completions \
        -H "Authorization: $API_KEY" \
        -H "Content-Type: application/json" \
        -d '{
            "model": "klusterai/Meta-Llama-3.1-8B-Instruct-Turbo",
            "messages": [
                {
                    "role": "system",
                    "content": "You are a helpful assistant."
                },
                {
                    "role": "user",
                    "content": "What is the capital of Argentina?"
                }
            ]
        }'
    ```

```Json title="Response"
{
    "id": "chat-d187c103e189483485b3bcd3eb899c62",
    "object": "chat.completion",
    "created": 1736136422,
    "model": "klusterai/Meta-Llama-3.1-8B-Instruct-Turbo",
    "choices": [
        {
            "index": 0,
            "message": {
                "role": "assistant",
                "content": "The capital of Argentina is Buenos Aires.",
                "tool_calls": []
            },
            "logprobs": null,
            "finish_reason": "stop",
            "stop_reason": null
        }
    ],
    "usage": {
        "prompt_tokens": 48,
        "total_tokens": 57,
        "completion_tokens": 9
    },
    "prompt_logprobs": null
}
```

</div>
</div>

---

### Chat completion object

<div class="grid" markdown>
<div markdown>

`id` ++"string"++

Unique identifier for the chat completion.

---

`object` ++"string"++

The object type, which is always `chat.completion`.

---

`created` ++"integer"++

The Unix timestamp (in seconds) of when the chat completion was created.

---

`model` ++"string"++

The model used for the chat completion. You can use the `models` endpoint to retrieve the [list of supported models](#list-supported-models){target=\_blank}.

---

`choices` ++"array"++

A list of chat completion choices.

??? child "Show properties"

    `index` ++"integer"++

    The index of the choice in the list of returned choices.

    ---

    `message` ++"object"++

    A chat completion message generated by the model. Can be one of `system`, `user`, or `assistant`.

    ??? child "Show properties"

        `content` ++"string or array"++

        The contents of the message.  

        ---

        `role` ++"string or null"++

        The role of the messages author. Can be one of `system`, `user`, or `assistant`
    
    ---

    `logprobs` ++"boolean or null"++

    Whether to return log probabilities of the output tokens or not. If true, returns the log probabilities of each output token returned in the `content` of `message`. Defaults to `false`.

    ---

    `finish_reason` ++"string"++

    The reason the model stopped generating tokens. This will be `stop` if the model hit a natural stop point or a provided stop sequence, `length` if the maximum number of tokens specified in the request was reached, `content_filter` if content was omitted due to a flag from our content filters, `tool_calls` if the model called a tool, or `function_call` (_deprecated_) if the model called a function.

    --- 

    `stop_reason` ++"string or null"++

    The reason the model stopped generating text.

---

`usage` ++"object"++

Usage statistics for the completion request.

??? child "Show properties"

    `completion_tokens` ++"integer"++

    Number of tokens in the generated completion.

    ---

    `prompt_tokens` ++"integer"++

    Number of tokens in the prompt.

    ---

    `total_tokens` ++"integer"++

    Total number of tokens used in the request (prompt + completion).

</div>
<div markdown>

```Json title="Chat completion object"
{
    "id": "chat-d187c103e189483485b3bcd3eb899c62",
    "object": "chat.completion",
    "created": 1736136422,
    "model": "klusterai/Meta-Llama-3.1-8B-Instruct-Turbo",
    "choices": [
        {
            "index": 0,
            "message": {
                "role": "assistant",
                "content": "The capital of Argentina is Buenos Aires.",
                "tool_calls": []
            },
            "logprobs": null,
            "finish_reason": "stop",
            "stop_reason": null
        }
    ],
    "usage": {
        "prompt_tokens": 48,
        "total_tokens": 57,
        "completion_tokens": 9
    },
    "prompt_logprobs": null
}
```

</div>
</div>

---

## Batch

### Submit a Batch job

`POST https://api.kluster.ai/v1/batches`

To submit a Batch job, send a request to the `batches` endpoint.

<div class="grid" markdown>
<div markdown>

**Request**

`input_file_id` ++"string"++ <span class="required" markdown>++"required"++</span>

The ID of an [uploaded file](#upload-files){target=\_blank} that contains requests for the new Batch.

Your input file must be formatted as a [JSONL file](https://jsonlines.org/){target=\_blank}, and must be uploaded with the purpose `batch`. The file can contain up to 50,000 requests and currently a maximum of 6GB per file.

---

`endpoint` ++"string"++ <span class="required" markdown>++"required"++</span>

The endpoint to be used for all requests in the Batch. Currently, only `/v1/chat/completions` is supported.

---

`completion_window` ++"string"++ <span class="required" markdown>++"required"++</span>

The supported completion windows are 1, 3, 6, 12, and 24 hours to accommodate a range of use cases and budget requirements. The code samples provided utilize the 24-hour completion window.

Learn more about how completion window selection affects cost by visiting the pricing section of the [kluster.ai website](https://www.kluster.ai){target=\_blank}.

---

`metadata` ++"Object or null"++

Custom metadata for the Batch.

---

**Returns**

The created [Batch](#batch-object) object.

</div>
<div markdown>

=== "Python"

    ```python title="Example request"
    from openai import OpenAI

    # Configure OpenAI client
    client = OpenAI(
        base_url="https://api.kluster.ai/v1",
        api_key="INSERT_API_KEY",  # Replace with your actual API key
    )

    batch_request = client.batches.create(
        input_file_id="myfile-123",
        endpoint="/v1/chat/completions",
        completion_window="24h",
    )

    print(batch_request.to_dict())
    ```

=== "curl"

    ```bash title="Example request"
    curl -s https://api.kluster.ai/v1/batches \
        -H "Authorization: Bearer $API_KEY" \
        -H "Content-Type: application/json" \
        -d '{
        "input_file_id": "myfile-123",
        "endpoint": "/v1/chat/completions",
        "completion_window": "24h"
        }'
    ```

```Json title="Response"
{
    "id": "mybatch-123",
    "completion_window": "24h",
    "created_at": 1733832777,
    "endpoint": "/v1/chat/completions",
    "input_file_id": "myfile-123",
    "object": "batch",
    "status": "validating",
    "cancelled_at": null,
    "cancelling_at": null,
    "completed_at": null,
    "error_file_id": null,
    "errors": null,
    "expired_at": null,
    "expires_at": 1733919177,
    "failed_at": null,
    "finalizing_at": null,
    "in_progress_at": null,
    "metadata": {},
    "output_file_id": null,
    "request_counts": {
        "completed": 0,
        "failed": 0,
        "total": 0
    }
}
```

</div>
</div>

---

### Retrieve a Batch

`GET https://api.kluster.ai/v1/batches/{batch_id}`

To retrieve a Batch job, send a request to the `batches` endpoint with your `batch_id`.

You can also monitor jobs in the [**Batch** tab](https://platform.kluster.ai/batch){target=\_blank} of the kluster.ai platform UI.

<div class="grid" markdown>
<div markdown>

**Path parameters**

`batch_id` ++"string"++ <span class="required" markdown>++"required"++</span>

The ID of the Batch to retrieve.

---

**Returns**

The [Batch](#batch-object) object matching the specified `batch_id`.

</div>
<div markdown>

=== "Python"

    ```python title="Example request"
    from openai import OpenAI

    # Configure OpenAI client
    client = OpenAI(
        base_url="https://api.kluster.ai/v1",
        api_key="INSERT_API_KEY",  # Replace with your actual API key
    )

    client.batches.retrieve("mybatch-123")
    ```

=== "curl"

    ```bash title="Example request"
    curl -s https://api.kluster.ai/v1/batches/mybatch-123 \
        -H "Authorization: Bearer $API_KEY" \
        -H "Content-Type: application/json"
    ```

```Json title="Response"
{
  "id": "mybatch-123",
  "object": "batch",
  "endpoint": "/v1/chat/completions",
  "errors": null,
  "input_file_id": "myfile-123",
  "completion_window": "24h",
  "status": "completed",
  "output_file_id": "myfile-123-output",
  "error_file_id": null,
  "created_at": "1733832777",
  "in_progress_at": "1733832777",
  "expires_at": "1733919177",
  "finalizing_at": "1733832781",
  "completed_at": "1733832781",
  "failed_at": null,
  "expired_at": null,
  "cancelling_at": null,
  "cancelled_at": null,
  "request_counts": {
    "total": 4,
    "completed": 4,
    "failed": 0
  },
  "metadata": {}
}
```

</div>
</div>

---

### Cancel a Batch

`POST https://api.kluster.ai/v1/batches/{batch_id}/cancel`

To cancel a Batch job that is currently in progress, send a request to the `cancel` endpoint with your `batch_id`. Note that cancellation may take up to 10 minutes to complete, during which time the status will show as `cancelling`.

<div class="grid" markdown>
<div markdown>

**Path parameters**

`batch_id` ++"string"++ <span class="required" markdown>++"required"++</span>

The ID of the Batch to cancel.

---

**Returns**

The [Batch](#batch-object) object matching the specified ID.

</div>
<div markdown>

=== "Python"

    ```python title="Example"
    from openai import OpenAI

    # Configure OpenAI client
    client = OpenAI(
        base_url="https://api.kluster.ai/v1",  
        api_key="INSERT_API_KEY" # Replace with your actual API key
    )

    client.batches.cancel("mybatch-123") # Replace with your batch id
    ```

=== "curl"

    ```bash title="Example"
    curl -s https://api.kluster.ai/v1/batches/$BATCH_ID/cancel \
        -H "Authorization: Bearer $API_KEY" \
        -H "Content-Type: application/json" \
        -X POST
    ```

```Json title="Response"
{
  "id": "mybatch-123",
  "object": "batch",
  "endpoint": "/v1/chat/completions",
  "errors": null,
  "input_file_id": "myfile-123",
  "completion_window": "24h",
  "status": "cancelling",
  "output_file_id": "myfile-123-output",
  "error_file_id": null,
  "created_at": "1730821906",
  "in_progress_at": "1730821911",
  "expires_at": "1730821906",
  "finalizing_at": null,
  "completed_at": null,
  "failed_at": null,
  "expired_at": null,
  "cancelling_at": "1730821906",
  "cancelled_at": null,
  "request_counts": {
    "total": 3,
    "completed": 3,
    "failed": 0
  },
  "metadata": {}
}
```

</div>
</div>

---

### List all Batch jobs

`GET https://api.kluster.ai/v1/batches`

To list all Batch jobs, send a request to the `batches` endpoint without specifying a `batch_id`. To constrain the query response, you can also use a `limit` parameter.

<div class="grid" markdown>
<div markdown>

**Query parameters**

`after` ++"string"++

A cursor for use in pagination. `after` is an object ID that defines your place in the list. For instance, if you make a list request and receive 100 objects, ending with `obj_foo`, your subsequent call can include `after=obj_foo` in order to fetch the next page of the list.

---

`limit` ++"integer"++

A limit on the number of objects to be returned. Limit can range between 1 and 100. Default is 20.

---

**Returns**

A list of paginated [Batch](#batch-object) objects.

The status of a Batch object can be one of the following:

<style>
table th:first-child {
  width: 10em;
}
</style>

| Status        | Description                                                             |
|---------------|-------------------------------------------------------------------------|
| `validating`  | The input file is being validated.                                      |
| `failed`      | The input file failed the validation process.                           |
| `in_progress` | The input file was successfully validated and the Batch is in progress. |
| `finalizing`  | The Batch job has completed and the results are being finalized.        |
| `completed`   | The Batch has completed and the results are ready.                      |
| `expired`     | The Batch was not completed within the 24-hour time window.             |
| `cancelling`  | The Batch is being cancelled (may take up to 10 minutes).               |
| `cancelled`   | The Batch was cancelled.                                                |

</div>

<div markdown>

=== "Python"

    ```python title="Example request"
    from openai import OpenAI

    # Configure OpenAI client
    client = OpenAI(
        base_url="https://api.kluster.ai/v1", 
        api_key="INSERT_API_KEY" # Replace with your actual API key
    )

    print(client.batches.list(limit=2).to_dict())
    ```

=== "curl"

    ```bash title="Example request" 
    curl -s https://api.kluster.ai/v1/batches \
        -H "Authorization: Bearer $API_KEY"
    ```

```Json title="Response"
{
"object": "list",
"data": [
    {
    "id": "mybatch-123",
    "object": "batch",
    "endpoint": "/v1/chat/completions",
    "errors": null,
    "input_file_id": "myfile-123",
    "completion_window": "24h",
    "status": "completed",
    "output_file_id": "myfile-123-output",
    "error_file_id": null,
    "created_at": "1733832777",
    "in_progress_at": "1733832777",
    "expires_at": "1733919177",
    "finalizing_at": "1733832781",
    "completed_at": "1733832781",
    "failed_at": null,
    "expired_at": null,
    "cancelling_at": null,
    "cancelled_at": null,
    "request_counts": {
        "total": 4,
        "completed": 4,
        "failed": 0
    },
    "metadata": {}
    },
{ ... },
],
"first_id": "mybatch-123",
"last_id": "mybatch-789",
"has_more": false,
"count": 1,
"page": 1,
"page_count": -1,
"items_per_page": 9223372036854775807
}
```

</div>
</div>

---

### Batch object

<div class="grid" markdown>
<div markdown>

`id` ++"string"++

The ID of the Batch.

---

`object` ++"string"++

The object type, which is always `batch`.

---

`endpoint` ++"string"++

The kluster.ai API endpoint used by the Batch.

---

`errors` ++"object"++

??? child "Show properties"
    
    `object` ++"string"++

    The object type, which is always `list`.

    ---

    `data` ++"array"++

    ??? child "Show properties"

        `code` ++"string"++

        An error code identifying the error type.

        ---

        `message` ++"string"++

        A human-readable message providing more details about the error.

        ---

        `param` ++"string or null"++

        The name of the parameter that caused the error, if applicable.

        ---
    
        `line` ++"integer or null"++

        The line number of the input file where the error occurred, if applicable.
---

`input_file_id` ++"string"++

The ID of the input file for the Batch.

---

`completion_window` ++"string"++

The time frame within which the Batch should be processed.

---

`status` ++"string"++

The current status of the Batch.

---

`output_file_id` ++"string"++

The ID of the file containing the outputs of successfully executed requests.

---

`error_file_id` ++"string"++

The ID of the file containing the outputs of requests with errors.

---

`created_at` ++"integer"++

The Unix timestamp (in seconds) for when the Batch was created.

---

`in_progress_at` ++"integer"++

The Unix timestamp (in seconds) for when the Batch started processing.

---

`expires_at` ++"integer"++

The Unix timestamp (in seconds) for when the Batch will expire.

---

`finalizing_at` ++"integer"++

The Unix timestamp (in seconds) for when the Batch started finalizing.

---

`completed_at` ++"integer"++

The Unix timestamp (in seconds) for when the Batch was completed.

---

`failed_at` ++"integer"++

The Unix timestamp (in seconds) for when the Batch failed.

---

`expired_at` ++"integer"++

The Unix timestamp (in seconds) for when the Batch expired.

---

`cancelling_at` ++"integer"++

The Unix timestamp (in seconds) for when the Batch started cancelling.

---

`cancelled_at` ++"integer"++

The Unix timestamp (in seconds) for when the Batch was cancelled.

---

`request_counts` ++"object"++

The request counts for different statuses within the Batch.

??? child "Show properties"

    `total` ++"integer"++

    Total number of requests in the Batch.

    ---

    `completed` ++"integer"++

    Number of requests that have been completed successfully.

    ---

    `failed` ++"integer"++

    Number of requests that have failed.   


<!--
---

`metadata` ++"Object or null"++

Set of 16 key-value pairs that can be attached to an object. This is useful for storing additional information about the object in a structured format. Keys can be a maximum of 64 characters long, and values can be a maximum of 512 characters long.
-->

</div>
<div markdown>

```Json title="Batch object"
{
    "id": "mybatch-123",
    "completion_window": "24h",
    "created_at": 1733832777,
    "endpoint": "/v1/chat/completions",
    "input_file_id": "myfile-123",
    "object": "batch",
    "status": "validating",
    "cancelled_at": null,
    "cancelling_at": null,
    "completed_at": null,
    "error_file_id": null,
    "errors": null,
    "expired_at": null,
    "expires_at": 1733919177,
    "failed_at": null,
    "finalizing_at": null,
    "in_progress_at": null,
    "metadata": {},
    "output_file_id": null,
    "request_counts": {
        "completed": 0,
        "failed": 0,
        "total": 0
    }
}
```

</div>
</div>

---

### The request input object

<div class="grid" markdown>
<div markdown>

The per-line object of the Batch input file.

`custom_id` ++"string"++

A developer-provided per-request ID.

---

`method` ++"string"++

The HTTP method to be used for the request. Currently, only POST is supported.

---

`url` ++"string"++

The `/v1/chat/completions` endpoint.

---

`body` ++"map"++

The JSON body of the input file.

</div>
<div markdown>

```Json title="Request input object"
[
    {
        "custom_id": "request-1",
        "method": "POST",
        "url": "/v1/chat/completions",
        "body": {
            "model": "klusterai/Meta-Llama-3.1-8B-Instruct-Turbo",
            "messages": [
                {
                    "role": "system",
                    "content": "You are a helpful assistant."
                },
                {
                    "role": "user",
                    "content": "What is the capital of Argentina?"
                }
            ],
            "max_tokens": 1000
        }
    }
]
```

</div>
</div>

---

### The request output object

<div class="grid" markdown>
<div markdown>

The per-line object of the Batch output files.

`id` ++"string"++

A unique identifier for the batch request.

---

`custom_id` ++"string"++

A developer-provided per-request ID that will be used to match outputs to inputs.

---

`response` ++"object or null"++

??? child "Show properties"

    `status_code` ++"integer"++

    The HTTP status code of the response.

    ---

    `request_id` ++"string"++

    A unique identifier for the request. You can reference this request ID if you need to contact support for assistance.

    ---

    `body` ++"map"++

    The JSON body of the response.

---

`error` ++"object or null"++

For requests that failed with a non-HTTP error, this will contain more information on the cause of the failure.

??? child "Show properties"

    `code` ++"string"++ 
   
    A machine-readable error code.
   
    ---

    `message` ++"string"++
   
    A human-readable error message. 

</div>
<div markdown>

```Json title="Request output object"
{
    "id": "batch-req-123",
    "custom_id": "request-1",
    "response": {
        "status_code": 200,
        "request_id": "req-123",
        "body": {
            "id": "chatcmpl-5a5ba6c6-2f95-4136-815b-23275c4f1efb",
            "object": "chat.completion",
            "created": 1737472126,
            "model": "klusterai/Meta-Llama-3.1-8B-Instruct-Turbo",
            "choices": [
                {
                    "index": 0,
                    "message": {
                        "role": "assistant",
                        "content": "The capital of Argentina is Buenos Aires.",
                        "tool_calls": []
                    },
                    "logprobs": null,
                    "finish_reason": "stop",
                    "stop_reason": null
                }
            ],
            "usage": {
                "prompt_tokens": 48,
                "total_tokens": 57,
                "completion_tokens": 9,
                "prompt_tokens_details": null
            },
            "prompt_logprobs": null
        }
    }
}
```

</div>
</div>

---

## Files

### Upload files

`POST https://api.kluster.ai/v1/files/`

Upload a [JSON Lines](https://jsonlines.org/){target=\_blank} file to the `files` endpoint.

You can also view all your uploaded files in the [**Files** tab](https://platform.kluster.ai/files){target=\_blank} of the kluster.ai platform.

<div class="grid" markdown>
<div markdown>

**Request**

`file` ++"file"++ <span class="required" markdown>++"required"++</span>

The File object (not file name) to be uploaded.

---

`purpose` ++"string"++ <span class="required" markdown>++"required"++</span>

The intended purpose of the uploaded file. Use `batch` for the Batch API.

---

**Returns**

The uploaded [File](#file-object) object.

</div>
<div markdown>

=== "Python"

    ```python title="Example request"
    from openai import OpenAI

    # Configure OpenAI client
    client = OpenAI(
        base_url="https://api.kluster.ai/v1", 
        api_key="INSERT_API_KEY" # Replace with your actual API key
    )

    batch_input_file = client.files.create(
        file=open(file_name, "rb"),
        purpose="batch"
    )

    print(batch_input_file.to_dict())
    ```

=== "curl"

    ```bash title="Example request"
    curl -s https://api.kluster.ai/v1/files \
        -H "Authorization: $API_KEY" \
        -H "Content-Type: multipart/form-data" \
        -F "file=@mybatchtest.jsonl" \
        -F "purpose=batch"
    ```

```Json title="Response"
{
  "id": "myfile-123",
  "bytes": 2797,
  "created_at": "1733832768",
  "filename": "mybatchtest.jsonl",
  "object": "file",
  "purpose": "batch"
}
```

</div>
</div>

---

### Retrieve file content

`GET https://api.kluster.ai/v1/files/{output_file_id}/content`

To retrieve the content of your Batch jobs output file, send a request to the `files` endpoint specifying the `output_file_id`. The output file will be a JSONL file, where each line contains the `custom_id` from your input file request, and the corresponding response.

<div class="grid" markdown>
<div markdown>

**Path parameters**

`file_id` ++"string"++ <span class="required" markdown>++"required"++</span>

The ID of the file to use for this request

---

**Returns**

The file content. Refer to the [input](/api-reference/reference/#the-request-input-object){target=\_blank} and [output](/api-reference/reference/#the-request-output-object){target=\_blank} format specifications for batch requests.

</div>
<div markdown>

=== "Python"

    ```python title="Example request"
    from openai import OpenAI

    # Configure OpenAI client
    client = OpenAI(
        base_url="https://api.kluster.ai/v1", 
        api_key="INSERT_API_KEY" # Replace with your actual API key
    )

    # Get the status of the Batch, which returns the output_file_id
    batch_status = client.batches.retrieve(batch_request.id)

    # Check if the Batch completed successfully
    if batch_status.status.lower() == "completed":
        # Retrieve the results
        result_file_id = batch_status.output_file_id
        results = client.files.content(result_file_id).content

        # Save results to a file
        result_file_name = "batch_results.jsonl"
        with open(result_file_name, "wb") as file:
            file.write(results)
        print(f"Results saved to {result_file_name}")
    else:
        print(f"Batch failed with status: {batch_status.status}")
    ```

=== "curl"

    ```bash title="Example request"
    curl -s https://api.kluster.ai/v1/files/kluster-output-file-123/content \
        -H "Authorization: Bearer $API_KEY" > batch_output.jsonl
    ```

</div>
</div>

---

### File object

<div class="grid" markdown>
<div markdown>

`id` ++"string"++

The file identifier, which can be referenced in the API endpoints.

---

`object` ++"string"++

The object type, which is always `file`.

---

`bytes` ++"integer"++

The size of the file, in bytes.

---

`created_at` ++"integer"++

The Unix timestamp (in seconds) for when the file was created.

---

`filename` ++"string"++

The name of the file.

---

`purpose` ++"string"++

The intended purpose of the file. Currently, only `batch` is supported.

</div>
<div markdown>

```Json title="File object"
{
  "id": "myfile-123",
  "bytes": 2797,
  "created_at": "1733832768",
  "filename": "mybatchtest.jsonl",
  "object": "file",
  "purpose": "batch"
}
```

</div>
</div>

---

## Models

### List supported models

`GET https://api.kluster.ai/v1/models`

Lists the currently available models.

You can use this endpoint to retrieve a list of all available models for the kluster.ai API. Currently supported models include:

- `klusterai/Meta-Llama-3.1-8B-Instruct-Turbo`
- `klusterai/Meta-Llama-3.1-405B-Instruct-Turbo`
- `klusterai/Meta-Llama-3.3-70B-Instruct-Turbo`
- `deepseek-ai/DeepSeek-R1`

<div class="grid" markdown>
<div markdown>

**Returns**

`id` ++"string"++

The model identifier, which can be referenced in the API endpoints.

---

`created` ++"integer"++

The Unix timestamp (in seconds) when the model was created.

---

`object` ++"string"++

The object type, which is always `model`.

---

`owned_by` ++"string"++

The organization that owns the model.

</div>
<div markdown>

=== "Python"

    ```python title="Example request"
    from openai import OpenAI

    # Configure OpenAI client
    client = OpenAI(
        base_url="http://api.kluster.ai/v1",
        api_key="INSERT_API_KEY" # Replace with your actual API key
    )

    print(client.models.list().to_dict())
    ```

=== "curl"

    ```bash title="Example request"
    curl https://api.kluster.ai/v1/models \
        -H "Authorization: Bearer $API_KEY" 
    ```

```Json title="Response"
{
  "object": "list",
  "data": [
    {
      "id": "klusterai/Meta-Llama-3.1-405B-Instruct-Turbo",
      "created": 1731336418,
      "object": "model",
      "owned_by": "klusterai"
    },
    {
      "id": "klusterai/Meta-Llama-3.1-8B-Instruct-Turbo",
      "created": 1731336610,
      "object": "model",
      "owned_by": "klusterai"
    },
    {
      "id": "klusterai/Meta-Llama-3.3-70B-Instruct-Turbo",
      "created": 1733777629,
      "object": "model",
      "owned_by": "klusterai"
    },
    {
      "id": "deepseek-ai/DeepSeek-R1",
      "created": 1737385699,
      "object": "model",
      "owned_by": "klusterai"
    }
  ],
}
```

</div>
</div>

---

## Fine-tuning

Fine-tuning is the process of refining a pre-trained model on specialized data. By adjusting the parameters with new, domain-specific examples, the model performs better on targeted tasks while retaining the general knowledge learned in its original training.

### Supported models

Currently, two base models are supported for Fine-tuning:

- **`klusterai/Meta-Llama-3.1-8B-Instruct-Turbo`** - has a `64,000` tokens max context window, best for long-context tasks, cost-sensitive scenarios
- **`klusterai/Meta-Llama-3.3-70B-Instruct-Turbo`** - has a `32,000` tokens max context window, best for complex reasoning, high-stakes accuracy

### Create a Fine-tuning job

`POST https://api.kluster.ai/v1/fine_tuning/jobs`

To initiate a Fine-tuning job for one of the supported models, first upload the dataset file (see [Files section](#files) for instructions).

<div class="grid" markdown> 
<div markdown>

**Request**

`training_file` ++"string"++ <span class="required" markdown>++"required"++</span>

ID of an [uploaded file](#files) that will serve as training data. This file must have `purpose="fine-tune"`.

---

`model` ++"string"++ <span class="required" markdown>++"required"++</span>

The base model ID to fine-tune. Must be a fine-tunable model, for example `meta-llama/Meta-Llama-3.1-8B-Instruct` or `meta-llama/Meta-Llama-3.3-70B-Instruct-Turbo`.

---

`validation_file` ++"string or null"++

Optionally specify a separate file to serve as your validation dataset.

---

`hyperparameters` ++"object or null"++

Optionally specify an object containing hyperparameters for Fine-tuning:

??? child "Show properties"

    `batch_size` ++"number"++

    The number of training examples processed in one forward/backward pass. Larger batch sizes reduce the frequency of weight updates per epoch, leading to more stable gradients but slower updates. Gradient accumulation is used, so larger batches may increase the duration of the job.

    ---

    `learning_rate_multiplier` ++"number"++

    A multiplier for the base step size used in model weight updates. Lower values slow training but improve precision (helping avoid overshooting optimal weights or overfitting). Higher values speed up convergence but risk instability. Adjust carefully to balance training efficiency and model performance.

    ---

    `n_epochs` ++"number"++

    The number of times the entire training dataset is passed through the model. More epochs can improve learning but risk overfitting if the model memorizes training data. Monitor validation metrics to determine the optimal number.


---

`nickname` ++"string or null"++

Add a custom suffix that will be appended to the output model name. This can help identify a fine tuned model.

---

**Returns**

A [Fine-tuning job object](#fine-tuning-job-object).

</div> 
<div markdown>

=== "Python"

    ```python title="Example request"
    from openai import OpenAI
    
    # Configure OpenAI client
    client = OpenAI(
        base_url="https://api.kluster.ai/v1",
        api_key="INSERT_API_KEY"  # Replace with your actual API key
    )
    
    job = client.fine_tuning.jobs.create(
        training_file="INSERT_TRAINING_FILE_ID",  # ID from uploaded training file
        model="meta-llama/Meta-Llama-3.1-8B-Instruct",
        hyperparameters={
            "batch_size": 4,
            "learning_rate_multiplier": 1,
            "n_epochs": 3
        }
    )
    print(job.to_dict())
    ```

=== "curl"

    ```bash title="Example request"
    curl -X POST https://api.kluster.ai/v1/fine_tuning/jobs \
        -H "Authorization: Bearer INSERT_API_KEY" \
        -H "Content-Type: application/json" \
        -d '{
            "training_file": "INSERT_TRAINING_FILE_ID",
            "model": "meta-llama/Meta-Llama-3.1-8B-Instruct",
            "hyperparameters": {
                "batch_size": 4,
                "learning_rate_multiplier": 1,
                "n_epochs": 3
            }
        }'
    ```

```json title="Response"
{
  "object": "fine_tuning.job",
  "id": "67ae81b59b08392687ea5f69",
  "model": "meta-llama/Llama-3.1-8B-Instruct",
  "created_at": 1739489717,
  "result_files": [],
  "status": "queued",
  "training_file": "67ae81587772e8a89c8fd5cf",
  "hyperparameters": {
    "batch_size": 4,
    "learning_rate_multiplier": 1,
    "n_epochs": 3
  },
  "method": {
    "type": "supervised",
    "supervised": {
      "batch_size": 4,
      "learning_rate_multiplier": 1,
      "n_epochs": 3
    }
  },
  "integrations": []
}
```

</div> 
</div>

### Retrieve a Fine-tuning job

`GET https://api.kluster.ai/v1/fine_tuning/jobs/{fine_tuning_job_id}`

Fetch details of a single Fine-tuning job by specifying its `fine_tuning_job_id`.

<div class="grid" markdown> 
<div markdown>

**Path parameters**

`fine_tuning_job_id` ++"string"++ <span class="required" markdown>++"required"++</span>

The ID of the Fine-tuning job to retrieve.

---

**Returns**

A [Fine-tuning job object](#fine-tuning-job-object).

</div> 
<div markdown>

=== "Python"

    ```python title="Example request"
    from openai import OpenAI
    client = OpenAI(
        base_url="https://api.kluster.ai/v1",
        api_key="INSERT_API_KEY"
    )
    job_details = client.fine_tuning.jobs.retrieve("INSERT_JOB_ID")
    print(job_details.to_dict())
    ```
=== "curl"

    ```bash title="Example request"
    curl -s https://api.kluster.ai/v1/fine_tuning/jobs/INSERT_JOB_ID \
        -H "Authorization: Bearer INSERT_API_KEY"
    ```

```json title="Response"
{
  "object": "fine_tuning.job",
  "id": "67ae81b59b08392687ea5f69",
  "model": "meta-llama/Llama-3.1-8B-Instruct",
  "created_at": 1739489717,
  "result_files": [],
  "status": "running",
  "training_file": "67ae81587772e8a89c8fd5cf",
  "hyperparameters": {
    "batch_size": 4,
    "learning_rate_multiplier": 1,
    "n_epochs": 3
  },
  "method": {
    "type": "supervised",
    "supervised": {
      "batch_size": 4,
      "learning_rate_multiplier": 1,
      "n_epochs": 3
    }
  },
  "integrations": []
}
```

</div> 
</div>

### List all Fine-tuning jobs

`GET https://api.kluster.ai/v1/fine_tuning/jobs`

Retrieve a paginated list of all Fine-tuning jobs.

<div class="grid" markdown> 
<div markdown>

**Query parameters**

`after` ++"string"++

A cursor for use in pagination.

---

`limit` ++"integer"++

A limit on the number of objects returned (1 to 100). Default is 20.

---

**Returns**

A paginated list of [Fine-tuning job objects](#fine-tuning-job-object).

</div> 
<div markdown>

=== "Python"

    ```python title="Example request"
    from openai import OpenAI

    client = OpenAI(
        base_url="https://api.kluster.ai/v1",
        api_key="INSERT_API_KEY"
    )

    jobs = client.fine_tuning.jobs.list(limit=3)
    print(jobs.to_dict())
    ```

=== "curl"

    ```bash title="Example request"
    curl -s https://api.kluster.ai/v1/fine_tuning/jobs \
        -H "Authorization: Bearer $API_KEY"
    ```

```Json title="Response"
{
  "object": "list",
  "data": [
    {
      "object": "fine_tuning.job",
      "id": "67ae81b59b08392687ea5f69",
      "model": "meta-llama/Llama-3.1-8B-Instruct",
      "created_at": 1739489717,
      "result_files": [],
      "status": "running",
      "training_file": "67ae81587772e8a89c8fd5cf",
      "hyperparameters": {
        "batch_size": 4,
        "learning_rate_multiplier": 1,
        "n_epochs": 3
      },
      "method": {
        "type": "supervised",
        "supervised": {
          "batch_size": 4,
          "learning_rate_multiplier": 1,
          "n_epochs": 3
        }
      },
      "integrations": []
    },
    {
      "object": "fine_tuning.job",
      "id": "67ae7f7d965c187d5cda039f",
      "model": "meta-llama/Llama-3.1-8B-Instruct",
      "created_at": 1739489149,
      "result_files": [],
      "status": "cancelled",
      "training_file": "67ae7f7c965c187d5cda0397",
      "hyperparameters": {
        "batch_size": 1,
        "learning_rate_multiplier": 1,
        "n_epochs": 10
      },
      "method": {
        "type": "supervised",
        "supervised": {
          "batch_size": 1,
          "learning_rate_multiplier": 1,
          "n_epochs": 10
        }
      },
      "integrations": []
    }
  ],
  "first_id": "67ae81b59b08392687ea5f69",
  "last_id": "67abefddbee1f22fb0a742ef",
  "has_more": true
}
```

</div> 
</div>

### Cancel a Fine-tuning job

`POST https://api.kluster.ai/v1/fine_tuning/jobs/{fine_tuning_job_id}/cancel`

To cancel a job that is in progress, send a `POST` request to the `cancel` endpoint with the job ID.

<div class="grid" markdown> 
<div markdown>

**Path parameters**

`fine_tuning_job_id` ++"string"++ <span class="required" markdown>++"required"++</span> 

The ID of the Fine-tuning job to cancel.

---

**Returns**

The [Fine-tuning job object](#fine-tuning-job-object) with updated status.

</div> 
<div markdown>

=== "Python"

    ```python title="Example request"
    from openai import OpenAI
    client = OpenAI(
        base_url="https://api.kluster.ai/v1",
        api_key="INSERT_API_KEY"
    )
    cancelled_job = client.fine_tuning.jobs.cancel("67ae7f7d965c187d5cda039f")
    print(cancelled_job.to_dict())
    ```
=== "curl"

    ```bash title="Example request"
    curl -X POST https://api.kluster.ai/v1/fine_tuning/jobs/67ae7f7d965c187d5cda039f/cancel \
        -H "Authorization: Bearer INSERT_API_KEY" \
        -H "Content-Type: application/json"
    ```

```json title="Response"
{
  "id": "67ae7f7d965c187d5cda039f",
  "object": "fine_tuning.job",
  "model": "meta-llama/Meta-Llama-3.1-8B-Instruct",
  "fine_tuned_model": null,
  "status": "cancelling",
  "created_at": 1738382911,
  "training_file": "file-123abc",
  "validation_file": null,
  "hyperparameters": {
    "batch_size": 4,
    "learning_rate_multiplier": 1,
    "n_epochs": 3
  },
  "metrics": {},
  "error": null
}
```

</div> 
</div>

### Fine-tuning job object 

<div class="grid" markdown> 
<div markdown>

`object` ++"string"++

The object type, which is always `fine_tuning.job`.

---

`id` ++"string"++

Unique identifier for the Fine-tuning job.

---

`model` ++"string"++

ID of the base model being fine-tuned.

---

`created_at` ++"integer"++

Unix timestamp (in seconds) when the Fine-tuning job was created.

---

`finished_at` ++"integer"++

Unix timestamp (in seconds) when the Fine-tuning job was completed.

---

`fine_tuned_model` ++"string or null"++

The ID of the resulting fine-tuned model if the job succeeded; otherwise `null`.

---

`result_files` ++"array"++

Array of file IDs associated with the Fine-tuning job results.

---

`status` ++"string"++

The status of the Fine-tuning job, e.g. `pending`, `running`, `succeeded`, `failed`, or `cancelled`.

---

`training_file` ++"string"++

ID of the uploaded file used for training data.

---

`hyperparameters` ++"object"++

Training hyperparameters used in the job (e.g., `batch_size`, `n_epochs`, `learning_rate_multiplier`).

---

`method` ++"object"++

Details about the Fine-tuning method used, including type and specific parameters.

---

`trained_tokens` ++"integer"++

The total number of tokens processed during training.

---

`integrations` ++"array"++

Array of integrations associated with the Fine-tuning job.

</div> 
<div markdown>

```json title="Example"
{
  "object": "fine_tuning.job",
  "id": "67ad3877720af9f9ba78b684",
  "model": "meta-llama/Llama-3.1-8B-Instruct",
  "created_at": 1739405431,
  "finished_at": 1739405521,
  "fine_tuned_model": "ft:meta-llama:Llama-3.1-8B-Instruct:personal:805b5d69",
  "result_files": [],
  "status": "succeeded",
  "training_file": "67ad38760272045e7006171b",
  "hyperparameters": {
    "batch_size": 4,
    "learning_rate_multiplier": 1,
    "n_epochs": 2
  },
  "method": {
    "type": "supervised",
    "supervised": {
      "batch_size": 4,
      "learning_rate_multiplier": 1,
      "n_epochs": 2
    }
  },
  "trained_tokens": 3065,
  "integrations": []
}
```

</div> 
</div>
--- END CONTENT ---

Doc-Content: https://docs.kluster.ai/get-started/get-api-key/
--- BEGIN CONTENT ---
---
title: Get a kluster.ai API key
description: Follow step-by-step instructions to generate and manage API keys, enabling secure access to kluster's services and seamless integration with your applications.
---

# Generate your kluster.ai API key

The API key is a unique identifier that authenticates requests associated with your account. You must have at least one API key to access [kluster.ai](https://www.kluster.ai/){target=\_blank}'s services.

This guide will help you obtain an API key, the first step to leveraging kluster.ai's powerful and cost-effective AI capabilities.

## Create an account

If you haven't already created an account with kluster.ai, visit the [registration page](https://platform.kluster.ai/signup){target=\_blank} and take the following steps:

1. Enter your full name
2. Provide a valid email address
3. Create a secure password
4. Click the **Sign up** button

![Signup Page](/images/get-started/get-api-key/get-api-key-1.webp)

## Generate a new API key

After you've signed up or logged into the platform through the [login page](https://platform.kluster.ai/login){target=\_blank}, take the following steps:

1. Select **API Keys** on the left-hand side menu
2. In the [**API Keys**](https://platform.kluster.ai/apikeys){target=\_blank} section, click the **Issue New API Key** button

    ![Issue New API Key](/images/get-started/get-api-key/get-api-key-2.webp)

3. Enter a descriptive name for your API key in the popup, then click **Create Key**

    ![Generate API Key](/images/get-started/get-api-key/get-api-key-3.webp)

## Copy and secure your API key

1. Once generated, your API key will be displayed
2. Copy the key and store it in a secure location, such as a password manager

    !!! warning "Warning"
        For security reasons, you won't be able to view the key again. If lost, you will need to generate a new one.

![Copy API key](/images/get-started/get-api-key/get-api-key-4.webp)

!!! abstract "Security tips"
    - **Keep it secret** - do not share your API key publicly or commit it to version control systems
    - **Use environment variables** - store your API key in environment variables instead of hardcoding them
    - **Regenerate if compromised** - if you suspect your API key has been exposed, regenerate it immediately from the **API Keys** section

## Managing your API keys

The **API Key Management** section allows you to efficiently manage your kluster.ai API keys. You can create, view, and delete API keys by navigating to the [**API Keys**](https://platform.kluster.ai/apikeys){target=\_blank} section. Your API keys will be listed in the **API Key Management** section.

To delete an API key, take the following steps:

1. Locate the API key you wish to delete in the list
2. Click the trash bin icon ( :octicons-trash-24: ) in the **Actions** column
3. Confirm the deletion when prompted

![Delete API key](/images/get-started/get-api-key/get-api-key-5.webp)

!!! warning "Warning"
    Once deleted, the API key cannot be used again and you must generate a new one if needed.

## Next steps

Now that you have your API key, you can start integrating kluster.ai's LLMs into your applications. Refer to our [Getting Started](/get-started/start-api/){target=\_blank} guide for detailed instructions on using the API.
--- END CONTENT ---

Doc-Content: https://docs.kluster.ai/get-started/integrations/crewai/
--- BEGIN CONTENT ---
---
title: Using CrewAI with the kluster.ai API
description: Learn how to integrate kluster.ai with CrewAI, a new framework for orchestrating autonomous AI agents, to launch and configure your AI agent chatbot.
---

# Using CrewAI with the kluster.ai API

[CrewAI](https://www.crewai.com/){target=\_blank} is a multi-agent platform that organizes specialized AI agents—each with defined roles, tools, and goals—within a structured process to tackle complex tasks efficiently. CrewAI agents streamline workflows and deliver reliable, scalable solutions by coordinating tasks and ensuring smooth collaboration.

This guide walks you through integrating [kluster.ai](https://www.kluster.ai/){target=\_blank} with CrewAI, from installation to creating and running a simple AI agent chatbot that leverages the kluster.ai API.

## Prerequisites

Before starting, ensure you have the following prerequisites:

- **A kluster.ai account** - sign up on the [kluster.ai platform](https://platform.kluster.ai/signup){target=\_blank} if you don't have one
- **A kluster.ai API key** - after signing in, go to the [**API Keys**](https://platform.kluster.ai/apikeys){target=\_blank} section and create a new key. For detailed instructions, check out the [Get an API key](/get-started/get-api-key/){target=\_blank} guide
- **CrewAI installed** - the [Installation Guide](https://docs.crewai.com/installation){target=\_blank} on the CrewAI website will walk you through installing CrewAI, setting up a virtual Python environment, and creating a new project. Note that CrewAI requires a Python version >=`3.10` and <`3.13`

## Create a project with the CrewAI CLI

Open your Python virtual environment, and then follow these steps to use the CrewAI CLI to create a new project:

1. **Create a project** - following the installation guide, create your first project with the following command:
```bash
crewai create crew INSERT_PROJECT_NAME
```
2. **Select model and provider** - during setup, the CLI will ask you to choose a provider and a model. Select `openai` as the provider and then choose any available model. Because you'll configure kluster.ai as a custom model, your initial model choice won't affect the final integration. The CLI will prompt you for an OpenAI API key, but this isn’t required. Simply press enter to skip

## Build a simple AI agent

After finishing the CLI setup, you will see a `src` directory with files `crew.py` and `main.py`. This guide won't use these sample files because they include extra features outside the scope. Follow these steps to continue:

1. **Create your first file** - Create a `hello_crew.py` file in `src/YOUR_PROJECT_NAME` to correspond to a simple AI agent chatbot

2. **Import modules and select model** - open `hello_crew.py` to add imports and define a custom LLM for kluster.ai by setting the following parameters:
    - **provider** - you can specify `openai_compatible`
    - **model** - choose one of kluster.ai's available models based on your use case. Regardless of which model you choose, prepend its name with `openai/` to ensure CrewAI, which relies on LiteLLM, processes your requests correctly. For more details, see [kluster.ai's models](/api-reference/reference/#list-supported-models){target=\_blank}
    - **base_url** - use `https://api.kluster.ai/v1` to send requests to the kluster.ai endpoint
    - **api_key** - replace `INSERT_API_KEY` in the code below with your kluster.ai API key. If you don't have one yet, refer to the [Get an API key guide](/get-started/get-api-key/){target=\_blank}
  
    ```python title="hello_crew.py"
    import random

from crewai import LLM, Agent, Crew, Process, Task
from crewai.project import CrewBase, agent, crew, task


@CrewBase
class HelloWorldCrew:
    # Override any default YAML references
    agents_config = {}
    tasks_config = {}

    def __init__(self):
        """
        When this crew is instantiated, create a custom LLM with your base_url.
        """
        self.custom_llm = LLM(
            provider="openai_compatible", 
            model="openai/klusterai/Meta-Llama-3.3-70B-Instruct-Turbo",
            base_url="https://api.kluster.ai/v1",
            api_key="INSERT_KLUSTER_API_KEY"
        )
    ```

    This example overrides `agents_config` and `tasks_config` with empty dictionaries to tell CrewAI to ignore all YAML files and rely solely on your code, keeping this guide as streamlined as possible. 

3. **Define your agent** - set the agent's role, goal, and backstory, and assign the custom LLM (via the kluster.ai API) for generating creative greetings:

    ```python title="hello_crew.py"
    @agent
    def hello_agent(self) -> Agent:
        """
        A super simple agent with a single purpose: greet the user in a friendly, varied way.
        """
        return Agent(
            role="HelloWorldAgent",
            goal="Greet the user in a fun and creative way.",
            backstory="I'm a friendly agent who greets everyone in a slightly different manner!",
            llm=self.custom_llm,
            verbose=True
        )
    ```

4. **Give the agent a task** - define a task that prompts the agent for a unique, creative greeting using randomness to avoid repetition. Passing this prompt to `hello_agent()` ensures varied responses. CrewAI requires an `expected_output` field, defined here as a short greeting:

    ```python title="hello_crew.py"
    def hello_task(self) -> Task:
        """
        A task that asks the agent to produce a dynamic greeting.
        """
        random_factor = random.randint(100000, 999999)
        prompt = f"""
        You are a friendly greeting bot. 
        Please produce a short, creative greeting that changes each time. 
        Random factor: {random_factor}
        Example: "Hey there, how's your day going?"
        """

        return Task(
            description=prompt,
            expected_output="A short, creative greeting",
            agent=self.hello_agent()
        )
    ```

5. **Tie it all together with a `@crew` method** - Add the following method to return the assembled Crew object with a single agent and task. This method enables CrewAI to coordinate the agent and task you defined:

    ```python title="hello_crew.py"
    def hello_crew(self) -> Crew:
        """
        Our entire 'Hello World' crew—only 1 agent + 1 task in sequence.
        """
        return Crew(
            agents=self.agents,  
            tasks=self.tasks,    
            process=Process.sequential,
            verbose=True
        )
    ```

6. **Set up the entry point for the agent** - Create a new file named `hello_main.py`. In `hello_main.py`, import and initialize the `HelloWorldCrew` class, call its `hello_crew()` method, and then `kickoff()` to launch the task sequence:

    ```python title="hello_main.py"
    #!/usr/bin/env python
    from hello_crew import HelloWorldCrew


    def run():
        """
        Kick off the HelloWorld crew with no inputs.
        """
        HelloWorldCrew().hello_crew().kickoff(inputs={})

    if __name__ == "__main__":
        run()

    ```

??? code "Complete script"
    ```python title="hello_crew.py"
    import random

from crewai import LLM, Agent, Crew, Process, Task
from crewai.project import CrewBase, agent, crew, task


@CrewBase
class HelloWorldCrew:
    # Override any default YAML references
    agents_config = {}
    tasks_config = {}

    def __init__(self):
        """
        When this crew is instantiated, create a custom LLM with your base_url.
        """
        self.custom_llm = LLM(
            provider="openai_compatible", 
            model="openai/klusterai/Meta-Llama-3.3-70B-Instruct-Turbo",
            base_url="https://api.kluster.ai/v1",
            api_key="INSERT_KLUSTER_API_KEY"
        )

    @agent
    def hello_agent(self) -> Agent:
        """
        A super simple agent with a single purpose: greet the user in a friendly, varied way.
        """
        return Agent(
            role="HelloWorldAgent",
            goal="Greet the user in a fun and creative way.",
            backstory="I'm a friendly agent who greets everyone in a slightly different manner!",
            llm=self.custom_llm,
            verbose=True
        )

    @task
    def hello_task(self) -> Task:
        """
        A task that asks the agent to produce a dynamic greeting.
        """
        random_factor = random.randint(100000, 999999)
        prompt = f"""
        You are a friendly greeting bot. 
        Please produce a short, creative greeting that changes each time. 
        Random factor: {random_factor}
        Example: "Hey there, how's your day going?"
        """

        return Task(
            description=prompt,
            expected_output="A short, creative greeting",
            agent=self.hello_agent()
        )

    @crew
    def hello_crew(self) -> Crew:
        """
        Our entire 'Hello World' crew—only 1 agent + 1 task in sequence.
        """
        return Crew(
            agents=self.agents,  
            tasks=self.tasks,    
            process=Process.sequential,
            verbose=True
        )
    ```

## Run the agent

To run your agent, ensure you are in the same directory as your `hello_main.py` file, then use the following command:

```bash
python hello_main.py
```

Upon running the script, you'll see output that looks like the following:

<div id="termynal" data-termynal>
<span data-ty></span># Agent: HelloWorldAgent</span>
<span data-ty>## Task:</span>
<span data-ty>You are a friendly greeting bot.</span>
<span data-ty>Please produce a short, creative greeting that changes each time.</span>
<span data-ty>Random factor: 896380</span>
<span data-ty>Example: "Hey there, how's your day going?"</span>
<br>
<span data-ty></span># Agent: HelloWorldAgent</span>
<span data-ty>## Final Answer:</span>
<span data-ty>Hello, it's a beautiful day to shine, how's your sparkle today?</span>
</div>

And that's it! You've now successfully configured your AI agent harnessing CrewAI and the power of the kluster.ai API!
--- END CONTENT ---

Doc-Content: https://docs.kluster.ai/get-started/integrations/eliza/
--- BEGIN CONTENT ---
---
title: Using eliza with the kluster.ai API
description: Learn how to integrate kluster.ai with eliza, a fast, lightweight, and flexible AI agent framework, to launch and configure your own AI agent chatbot. 
---

# Using eliza with the kluster.ai API

[eliza](https://elizaos.github.io/eliza/){target=\_blank} is an open-source framework designed to create and manage AI agents that can handle a variety of tasks, from simple chat interactions to more complex automation.

In this guide, you'll learn how to integrate [kluster.ai](https://www.kluster.ai/) into eliza so you can leverage its powerful models and quickly set up your AI-driven workflows.

## Prerequisites

Before starting, ensure you have the following kluster prerequisites:

- **A kluster.ai account** - sign up on the [kluster.ai platform](https://platform.kluster.ai/signup){target=\_blank} if you don't have one
- **A kluster.ai API key** - after signing in, go to the [**API Keys**](https://platform.kluster.ai/apikeys){target=\_blank} section and create a new key. For detailed instructions, check out the [Get an API key](/get-started/get-api-key/){target=\_blank} guide

Next, you can clone and install the eliza repository by following the installation instructions on the [eliza Quick Start guide](https://elizaos.github.io/eliza/docs/quickstart/){target=\_blank}. Pay careful attention to the eliza prerequisites, including the minimum supported versions of Node.js and pnpm. You can pause at the **Configure Environment** section in the Quick Start guide, as those steps will be addressed in this guide.

## Configure your environment

After you have eliza installed, it's simple to utilize kluster.ai with eliza. Only three main changes to the `.env` file are required. You can run the following command to generate a `.env` file from the provided example. 

```bash
cp .env.example .env
```

Then, set the following variables in the `.env` file: 

  - **OPENAI_API_KEY** - replace `INSERT_API_KEY` in the code below with your own kluster.ai API key. If you don't have one yet, refer to the [Get an API key guide](/get-started/get-api-key/){target=\_blank}
  - **OPENAI_API_URL** - use `https://api.kluster.ai/v1` to send requests to the kluster.ai endpoint
  - **OPENAI_DEFAULT_MODEL** - choose one of kluster.ai's available models based on your use case. Ensure that the model's full name starting with `klusterai/` is listed. For more details, see [kluster.ai's models](/api-reference/reference/#list-supported-models){target=\_blank}. It's also recommended that you set `SMALL_OPENAI_MODEL`, `MEDIUM_OPENAI_MODEL`, `LARGE_OPENAI_MODEL` to the same value. This will allow you seamless experimentation with the different characters because different characters default to using different models

The OpenAI configuration section of your `.env` file should resemble the following:

```bash
# OpenAI Configuration
OPENAI_API_KEY=INSERT_KLUSTER_API_KEY
OPENAI_API_URL=https://api.kluster.ai/v1

# Community Plugin for OpenAI Configuration
OPENAI_DEFAULT_MODEL=klusterai/Meta-Llama-3.3-70B-Instruct-Turbo
SMALL_OPENAI_MODEL=klusterai/Meta-Llama-3.3-70B-Instruct-Turbo
MEDIUM_OPENAI_MODEL=klusterai/Meta-Llama-3.3-70B-Instruct-Turbo
LARGE_OPENAI_MODEL=klusterai/Meta-Llama-3.3-70B-Instruct-Turbo
```

## Run and interact with your first agent

Now that you've configured your environment properly you're ready to run your first agent! eliza comes with a number of characters that you can interact with by prompting or that can autonomously perform tasks like tweeting. This guide relies on the `Dobby` character for its minimal setup requirements. Other agents, particularly those that handle tweets, would necessitate additional steps, such as X login and similar information. 

By default, `Dobby` uses the `openai` model, which has been properly configured to rely on the kluster.ai API, but it doesn't hurt to double-check the `dobby.character.json` file under the `characters` folder. You should see the configuration start with the following:

```json
{
  "name": "Dobby",
  "clients": [],
  "modelProvider": "openai" // json truncated for clarity
}
```

To run the `Dobby` agent, run the following command from the project root directory:

```bash
pnpm start --character="characters/dobby.character.json"
``` 

In another terminal window, run the following command to launch the web UI: 

```bash
pnpm start:client
```

You'll be prompted to open your browser to [http://localhost:5173/](http://localhost:5173/){target=\_blank}. 

<div id="termynal" data-termynal>
   <span data-ty="input"><span class="file-path">pnpm start:client</span>
   <span data-ty>VITE v6.0.11 ready in 824 ms</span>
   <span data-ty>➜  Local:   http://localhost:5173/</span>
   <span data-ty>➜  Network: use --host to expose</span>
   <span data-ty>➜  press h + enter to show help</span>
</div>

You can now interact with Dobby by clicking on the **Chat** button and starting the conversation: 

![Chat with Dobby AI agent](/images/get-started/integrations/eliza/eliza-1.webp)

That's it! You've successfully integrated eliza with the kluster.ai API. You're now ready to harness the power of AI agents with the kluster.ai API!
--- END CONTENT ---

Doc-Content: https://docs.kluster.ai/get-started/integrations/immersive-translate/
--- BEGIN CONTENT ---
---
title: Integrate Immersive Translate
description: Learn how to integrate the Immersive Translate browser extension with kluster.ai in your workflows for seamless, real-time multilingual content handling.
---

# Using Immersive Translate with the kluster.ai API

[Immersive Translate](https://immersivetranslate.com/){target=_blank} is an  AI-powered bilingual translation extension that automatically identifies the main text on any web page and provides parallel translations in real-time. This context-driven approach streamlines reading and collaboration across languages with additional features like efficient document translation, hover translation, and support for 10+ translation services.

In this guide, you'll learn how to integrate Immersive Translate with the [kluster.ai](https://www.kluster.ai/){target=_blank} API—from installation through configuration—so you can seamlessly handle multilingual content within your workflows. You will enable Immersive Translate's core capabilities with kluster.ai's powerful models, helping you build more robust and accessible AI-driven applications.

## Prerequisites

Before starting, ensure you have the following:

- **A kluster.ai account** - sign up on the [kluster.ai platform](https://platform.kluster.ai/signup){target=\_blank} if you don't have one
- **A kluster.ai API key** - after signing in, go to the [**API Keys**](https://platform.kluster.ai/apikeys){target=\_blank} section and create a new key. For detailed instructions, check out the [Get an API key](/get-started/get-api-key/){target=\_blank} guide
- **Installed the Immersive Translate plugin** - you can download the Immersive Translate plugin for your respective browser on the [Immersive Translate homepage](https://immersivetranslate.com/){target=\_blank}

## Configure Immersive Translate to use the kluster.ai API

First, open the Immersive Translate extension and click on the **Options** button in the lower left corner of the extension.

![](/images/get-started/integrations/immersivetranslate/immersive-1.webp)

Then, take the following steps:

1. Navigate to **Translatation Services**
2. Press **Add OpenAI Compatible Service**

![](/images/get-started/integrations/immersivetranslate/immersive-2.webp)

Take the following steps to configure the kluster.ai API as a custom translation service for Immersive Translate:

1. Enter a name
2. For the custom API interface address, enter the following:

    ```text
    https://api.kluster.ai/v1/chat/completions
    ```

3. Paste in your kluster.ai [API key](https://platform.kluster.ai/apikeys){target=\_blank}
4. **Check** the box to enable custom models 
5. Paste in the name of the kluster.ai [supported model](https://docs.kluster.ai/api-reference/reference/#list-supported-models){target=\_blank} you'd like to use
6. Specify a value of `1` for max requests per second to avoid rate limits. Paid kluster.ai API accounts may have higher rate limits
7. Press **Verify Service** in the upper right corner to validate the input values

![](/images/get-started/integrations/immersivetranslate/immersive-2.webp)

You must take one more step before using kluster.ai with Immersive Translate. Although kluster.ai has been added as a provider, it is disabled by default. To enable it, take the following steps:

1. Click on the **Translation Services** section of settings
2. Toggle the switch to enable kluster.ai as a provider

That's it! The next section will demonstrate using Immersive Translate with the kluster.ai API to perform webpage translations.

![](/images/get-started/integrations/immersivetranslate/immersive-3.webp)

## Translate content with Immersive Translate

With Immersive Translate, you can easily translate content with just a few clicks. To do so, navigate to the page with the foreign language content. Open the Immersive Translate plugin and take the following steps:

1. The language of the existing content is auto-detected by the plugin, but it's a good idea to verify it
2. Select the language to translate the content into. This is set by default to your native language 
3. Press **Translate**

![](/images/get-started/integrations/immersivetranslate/immersive-4.webp)

Then, the content translated by the Immersive Translate plugin will begin to appear on the page. 

![](/images/get-started/integrations/immersivetranslate/immersive-5.webp)

And that's it! You've now set up Immersive Translate to use the kluster.ai API and learned how to translate content.
--- END CONTENT ---

Doc-Content: https://docs.kluster.ai/get-started/integrations/langchain/
--- BEGIN CONTENT ---
---
title: Integrate LangChain with kluster.ai
description: This guide walks you through integrating LangChain, a framework designed to simplify the development of LLM powered-applications, with the kluster.ai API.
---

# Using LangChain with the kluster.ai API

[LangChain](https://www.langchain.com/){target=\_blank} offers a range of features—like memory modules for context tracking, retrieval augmentation to feed external data into prompts, and customizable multi-step “chains” to break down complex tasks. By leveraging these capabilities with the [kluster.ai](https://www.kluster.ai/){target=\_blank} API, you can build more robust and context-aware solutions that seamlessly handle everything from short-form answers to intricate conversations.

This guide demonstrates how to integrate the `ChatOpenAI` class from the `langchain_openai` package with the kluster.ai API, then walks through building a multi-turn conversational agent that leverages LangChain’s memory for context-aware interactions.

## Prerequisites

Before starting, ensure you have the following:

- **[A python virtual environment](https://packaging.python.org/en/latest/guides/installing-using-pip-and-virtual-environments/){target=\_blank}** - this is optional but recommended. Ensure that you enter the Python virtual environment before following along with this tutorial
- **LangChain packages installed** - install the [`langchain` packages](https://github.com/langchain-ai/langchain){target=\_blank}:

    ```bash
    pip install langchain langchain_community langchain_core langchain_openai
    ```

    As a shortcut, you can also run:

    ```bash
    pip install "langchain[all]"
    ```

- **A kluster.ai account** - sign up on the [kluster.ai platform](https://platform.kluster.ai/signup){target=\_blank} if you don't have one
- **A kluster.ai API key** - after signing in, go to the [**API Keys**](https://platform.kluster.ai/apikeys){target=\_blank} section and create a new key. For detailed instructions, check out the [Get an API key](/get-started/get-api-key/){target=\_blank} guide

## Integrate with LangChain - Quick Start

It's easy to integrate kluster.ai with LangChain—when configuring the chat model, just point your `ChatOpenAI` instance to the correct base URL and configure the following settings:

  - **Base URL** - use `https://api.kluster.ai/v1` to send requests to the kluster.ai endpoint
  - **API key** - replace `INSERT_API_KEY` in the code below with your kluster.ai API key. If you don’t have one yet, refer to the [Get an API key guide](/get-started/get-api-key/){target=\_blank}
  - **Select your model** - choose one of kluster.ai’s available models based on your use case. For more details, see [kluster.ai’s models](/api-reference/reference/#list-supported-models){target=\_blank}

```python
from langchain_openai import ChatOpenAI

llm = ChatOpenAI(
    base_url="https://api.kluster.ai/v1",
    api_key="INSERT_API_KEY", # Replace with your actual API key
    model="klusterai/Meta-Llama-3.1-8B-Instruct-Turbo",
)

llm.invoke("What is the capital of Nepal?")
```

That's all you need to start with LangChain and the kluster.ai API! Next, this guide will explore building a multi-turn conversational agent that showcases how memory and context can elevate your chatbot to a more interactive, intelligent experience.

## Building a multi-turn conversational agent

This section will explore what LangChain can do beyond a single prompt-and-response interaction. One standout feature of LangChain is its built-in memory, which tracks conversation context across multiple user queries. In the following steps, you'll set up a multi-turn conversational agent that takes advantage of this memory and seamlessly integrates with the kluster.ai API: 

1. First, import the necessary LangChain components for memory management, prompt handling, and kluster.ai integration 
```python
from langchain.chains.conversation.memory import ConversationBufferMemory
from langchain_community.chat_message_histories import ChatMessageHistory
from langchain_core.messages import HumanMessage
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_openai import ChatOpenAI
```
2. Next, create a memory instance to store and manage the conversation’s context, allowing the chatbot to remember previous user messages. Finally, you'll configure the `ChatOpenAI` model to point to kluster.ai’s endpoint (with your API key and chosen model). Remember, you can always change the selected model based on your needs 
```python
message_history = ChatMessageHistory()
memory = ConversationBufferMemory(memory_key="chat_history", return_messages=True)
```
3. Next, define a prompt template that includes a system instruction for the assistant, a placeholder for the conversation history, and an input slot for the user’s query 
```python
prompt = ChatPromptTemplate.from_messages([
    ("system", "You are a helpful assistant."),
    MessagesPlaceholder(variable_name="chat_history"),
    ("human", "{input}")
])
```
4. You'll then create the `ConversationChain` by passing in the LLM, memory, and this prompt template—so every new user query is automatically enriched with the stored conversation context and guided by the assistant’s role
```python
conversation = ConversationChain(
    llm=llm,
    memory=memory,
    prompt=prompt
)
```
5. Now, it's time to prompt the model with the first question. You can prompt it with any question; the example chosen here is designed to demonstrate context awareness between questions
```python
question1 = "Hello! Can you tell me something interesting about the city of Kathmandu?"
print("Question 1:", question1)
response1 = conversation.predict(input=question1)
print("Response 1:", response1)
```
6. Finally, a follow-up question is posed without restating the city name—allowing LangChain’s memory to handle the context implicitly. By capturing and printing both the questions and the responses, you can see how multi-turn interactions work in practice, with each new query informed by the conversation
```python
question2 = "What is the population of that city?"
print("\nQuestion 2:", question2)
response2 = conversation.predict(input=question2)
print("Response 2:", response2)
```

??? code "Complete script"
    ```python title="langchain-advanced.py"
    from langchain.chains import ConversationChain
from langchain.chains.conversation.memory import ConversationBufferMemory
from langchain_community.chat_message_histories import ChatMessageHistory
from langchain_core.messages import HumanMessage
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_openai import ChatOpenAI

# 1. Create a memory instance to store the conversation
message_history = ChatMessageHistory()
memory = ConversationBufferMemory(memory_key="chat_history", return_messages=True)

# 2. Create your LLM, pointing to kluster.ai's endpoint
llm = ChatOpenAI(
    base_url="https://api.kluster.ai/v1",
    api_key="INSERT_API_KEY",
    model="klusterai/Meta-Llama-3.1-8B-Instruct-Turbo",
)

# 3. Define the prompt template, including the system instruction and placeholders
prompt = ChatPromptTemplate.from_messages([
    ("system", "You are a helpful assistant."),
    MessagesPlaceholder(variable_name="chat_history"),
    ("human", "{input}")
])

# 4. Create the conversation chain
conversation = ConversationChain(
    llm=llm,
    memory=memory,
    prompt=prompt
)

# 5. Send the first user prompt
question1 = "Hello! Can you tell me something interesting about the city of Kathmandu?"
print("Question 1:", question1)
response1 = conversation.predict(input=question1)
print("Response 1:", response1)

# 6. Send a follow-up question referencing previous context
question2 = "What is the population of that city?"
print("\nQuestion 2:", question2)
response2 = conversation.predict(input=question2)
print("Response 2:", response2)
    ```

When running the complete script, you should see output that resembles the following:

<div id="termynal" data-termynal>
<span data-ty="input"><span class="file-path"> python langchain.py </span>
<span data-ty=>Question 1: Hello! Can you tell me something interesting about the city of Kathmandu?</span>
<span data-ty>Response 1: Kathmandu, the capital city of Nepal, is indeed a treasure trove of history, culture, and natural beauty. Here's something interesting:</span>
<span data-ty>Kathmandu is home to the famous Boudhanath Stupa, a UNESCO World Heritage Site. It's one of the largest Buddhist stupas in the world and is considered a sacred site by Buddhists. The stupa is over 36 meters (118 feet) high and is built in a unique octagonal shape. Its massive size is so prominent that it can be seen from many parts of the city.</span>
<span data-ty>Another fascinating fact is that Kathmandu has managed to conserve its rich cultural heritage, which dates back to the 12th century. You can see ancient temples, palaces, streets, and marketplaces that have been beautifully preserved and restored.</span>
<span data-ty>Lastly, Kathmandu is also known for its Newar culture, which is the indigenous culture of the city. The Newars have a rich tradition of art, music, and cuisine, which is reflected in the vibrant festivals and celebrations that take place throughout the year.</span>
<span data-ty>Would you like to know more about Kathmandu's culture, history, or maybe some of its modern attractions?</span>
<span data-ty>Question 2: What is the population of that city?</span>
<span data-ty>Response 2: Kathmandu, the capital city of Nepal, has a population of around 374,405 people (as per the 2021 estimates). However, the Kathmandu Valley, which includes the surrounding municipalities and areas, has a population of over 3.2 million people.</span>
<span data-ty>When considering the larger metropolitan area that includes the neighboring cities like Lalitpur (Patan) and Bhaktapur, the population exceeds 5 million people, making it one of the largest urban agglomerations in Nepal.</span>
<span data-ty>It's worth noting that Nepal's population density is relatively high, with many people living in urban areas. The Kathmandu Valley, in particular, is one of the most densely populated regions in the country.</span>
</div>

That’s it! You’ve successfully integrated LangChain with the kluster.ai API, and your configured multi-turn conversational agent is ready to leverage the power of LangChain and the kluster.ai API. For more information about the capabilities of LangChain, be sure to check out the [LangChain docs](https://python.langchain.com/docs/introduction/){target=\_blank}.
--- END CONTENT ---

Doc-Content: https://docs.kluster.ai/get-started/integrations/litellm/
--- BEGIN CONTENT ---
---
title: Integrate LiteLLM with kluster.ai
description: This guide shows how to integrate LiteLLM, an open-source library that simplifies access to 100+ LLMs with load balancing and spend tracking, into kluster.ai.
---

# Integrating LiteLLM with the kluster.ai API

This guide shows you how to integrate [LiteLLM](https://www.litellm.ai/){target=_blank}—an open-source library providing unified access to 100+ large language models—with the [kluster.ai](https://www.kluster.ai/){target=\_blank} API. You can seamlessly develop and deploy robust, AI-driven applications by combining LiteLLM's load balancing, fallback logic, and spend tracking with kluster.ai's powerful models.

## Prerequisites

Before starting, ensure you have the following:

- **[A python virtual environment](https://packaging.python.org/en/latest/guides/installing-using-pip-and-virtual-environments/){target=\_blank}** - This is optional but recommended. Ensure that you enter the Python virtual environment before following along with this tutorial
- [**LiteLLM installed**](https://github.com/BerriAI/litellm){target=\_blank} - to install the library, use the following command:

    ```bash
    pip install litellm
    ```

- **A kluster.ai account** - sign up on the [kluster.ai platform](https://platform.kluster.ai/signup){target=\_blank} if you don't have one
- **A kluster.ai API key** - after signing in, go to the [**API Keys**](https://platform.kluster.ai/apikeys){target=\_blank} section and create a new key. For detailed instructions, check out the [Get an API key](/get-started/get-api-key/){target=\_blank} guide

## Integrate with LiteLLM

In this section, you'll learn how to integrate kluster.ai with LiteLLM. You’ll configure your environment variables, specify a kluster.ai model, and make a simple request using LiteLLM’s OpenAI-like interface.

1. **Import LiteLLM and its dependencies** - Create a new file (e.g., `hello-litellm.py`) and start by importing the necessary Python modules:
```python
import os

from litellm import completion

# Set environment vars, shown in script for readability
os.environ["OPENAI_API_KEY"] = "INSERT_KLUSTER_API_KEY"
os.environ["OPENAI_API_BASE"] = "https://api.kluster.ai/v1"

# Basic Chat
messages = [
    {"role": "system", "content": "You are a helpful assistant."},
    {"role": "user",   "content": "What is the capital of California?"}
]

# Use an "openai/..." model prefix so LiteLLM treats this as an OpenAI-like call
model = "openai/klusterai/Meta-Llama-3.3-70B-Instruct-Turbo"

response = completion(
    model=model,
    messages=messages,
    max_tokens=1000, 
)

print(response)
```
2. **Set your kluster.ai API key and Base URL** - Replace INSERT_API_KEY with your actual API key. If you don't have one yet, refer to the [Get an API key](/get-started/get-api-key/){target=\_blank}
```python
os.environ["OPENAI_API_KEY"] = "INSERT_KLUSTER_API_KEY"
os.environ["OPENAI_API_BASE"] = "https://api.kluster.ai/v1"
```
3. **Define your conversation (system + user messages)** - Set up your initial system prompt and user message. The system message defines your AI assistant’s role, while the user message is the actual question or prompt
```python
messages = [
    {"role": "system", "content": "You are a helpful assistant."},
    {"role": "user",   "content": "What is the capital of California?"}
]
```
4. **Select your kluster.ai model** - Choose one of the kluster.ai [models](/api-reference/reference/#list-supported-models){target=_blank} that best fits your use case. Prepend the model name with `openai/` so LiteLLM recognizes it as an OpenAI-like model request.
```python
model = "openai/klusterai/Meta-Llama-3.3-70B-Instruct-Turbo"
```
5. **Call the LiteLLM completion function** - Finally, invoke the completion function to send your request:
```python
model=model,
    messages=messages,
    max_tokens=1000, 
)

print(response)
```

??? code "View full code file"
    ```python
    import os

from litellm import completion

# Set environment vars, shown in script for readability
os.environ["OPENAI_API_KEY"] = "INSERT_KLUSTER_API_KEY"
os.environ["OPENAI_API_BASE"] = "https://api.kluster.ai/v1"

# Basic Chat
messages = [
    {"role": "system", "content": "You are a helpful assistant."},
    {"role": "user",   "content": "What is the capital of California?"}
]

# Use an "openai/..." model prefix so LiteLLM treats this as an OpenAI-like call
model = "openai/klusterai/Meta-Llama-3.3-70B-Instruct-Turbo"

response = completion(
    model=model,
    messages=messages,
    max_tokens=1000, 
)

print(response)
    ```

Use the following command to run your script:

```python
python hello-litellm.py
```

<div id="termynal" data-termynal>
    <span data-ty="input"><span class="file-path">python hello-litellm.py</span>
    <span data-ty>ModelResponse(id='chatcmpl-9877dfe6-6f1d-483f-a392-d791b89c75d6', created=1739495162, model='klusterai/Meta-Llama-3.3-70B-Instruct-Turbo', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content='The capital of California is Sacramento.', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'refusal': None}, refusal=None))], usage=Usage(completion_tokens=8, prompt_tokens=48, total_tokens=56, completion_tokens_details=None, prompt_tokens_details=None), service_tier=None, prompt_logprobs=None)</span>
</div>

That's it! You've successfully integrated LiteLLM with the kluster.ai API. Continue on to learn how to experiment with more advanced features of LiteLLM.

## Exploring LiteLLM Features

In the previous section, you learned how to use LiteLLM with the kluster.ai API by properly configuring the model via an OpenAI-like call and configuring the API key and API base URL. This section will dive deeper into some of the interesting features offered by LiteLLM and how you can use them in conjunction with the kluster.ai API.

To set up the demo file, go ahead and create a new python file, then take the following steps:

1. Import LiteLLM and its dependencies:  
```python
import os

import litellm.exceptions
from litellm import completion

# Set environment variables for kluster.ai
os.environ["OPENAI_API_KEY"] = "INSERT_API_KEY"  # Replace with your key
os.environ["OPENAI_API_BASE"] = "https://api.kluster.ai/v1"

def main():
    model = "openai/klusterai/Meta-Llama-3.3-70B-Instruct-Turbo"

    messages = [
        {"role": "system", "content": "You are a helpful AI assistant."},
        {"role": "user",   "content": "Explain the significance of the California Gold Rush."},
    ]

    # --- 1) STREAMING CALL: Only print chunk text --------------------------------
    try:
        response_stream = completion(
            model=model,
            messages=messages,
            max_tokens=300,
            temperature=0.3,
            stream=True,  # streaming enabled
        )
    except Exception as err:
        print(f"Error calling model: {err}")
        return

    print("\n--------- STREAMING RESPONSE (text only) ---------")
    streamed_text = []

    # Iterate over each chunk from the streaming generator
    for chunk in response_stream:
        if hasattr(chunk, "choices") and chunk.choices:
            # If the content is None, we replace it with "" (empty string)
            partial_text = getattr(chunk.choices[0].delta, "content", "") or ""
            streamed_text.append(partial_text)
            print(partial_text, end="", flush=True)

    print("\n")  # new line after streaming ends

    # Combine the partial chunks into one string
    complete_first_answer = "".join(streamed_text)

    # Append the entire first answer to the conversation for multi-turn context
    messages.append({"role": "assistant", "content": complete_first_answer})

    # --- 2) SECOND CALL (non-streamed): Print just the text ---------------------
    messages.append({
        "role": "user",
        "content": (
            "Thanks for that. Can you propose a short, 3-minute presentation outline "
            "about the Gold Rush, focusing on its broader implications?"
        ),
    })

    try:
        response_2 = completion(
            model=model,
            messages=messages,
            max_tokens=300,
            temperature=0.6,
            stream=False  # non-streamed
        )
    except Exception as err:
        print(f"Error calling model: {err}")
        return

    print("--------- RESPONSE 2 (non-streamed, text only) ---------")
    second_answer_text = ""
    if response_2.choices and hasattr(response_2.choices[0], "message"):
        second_answer_text = response_2.choices[0].message.get("content", "") or ""

    print(second_answer_text)

if __name__ == "__main__":
    main()
```
2. Set your kluster API key and base URL:
```python
os.environ["OPENAI_API_KEY"] = "INSERT_API_KEY"  # Replace with your key
os.environ["OPENAI_API_BASE"] = "https://api.kluster.ai/v1"
```
3. Set your desired kluster model:
```python
model = "openai/klusterai/Meta-Llama-3.3-70B-Instruct-Turbo"
```
4. Define the system prompt and your first user message:
```python
{"role": "system", "content": "You are a helpful AI assistant."},
        {"role": "user",   "content": "Explain the significance of the California Gold Rush."},
    ]
```

### Streaming Responses

You can enable streaming by simply passing `stream=True` to the `completion()` function. This returns a generator instead of a static response, letting you iterate over partial output chunks as they arrive. In the code sample below, each chunk is accessed in a for chunk in response: loop, and you can extract just the textual content (e.g., `chunk.choices[0].delta.content)` rather than printing all metadata. 

To configure a streaming response, take the following steps:

1. Initiate a streaming request to the model by setting `stream=True` in the `completion()` function. This tells LiteLLM to return partial pieces (chunks) of the response as they become available, rather than waiting for the entire response to be ready.
```python
try:
        response_stream = completion(
            model=model,
            messages=messages,
            max_tokens=300,
            temperature=0.3,
            stream=True,  # streaming enabled
        )
    except Exception as err:
        print(f"Error calling model: {err}")
        return

    print("\n--------- STREAMING RESPONSE (text only) ---------")
    streamed_text = []
```
2. However, if we just return all of the streamed data, it's going to include a lot of excessive noise like token counts, etc. For readability, you probably prefer just the text content of the response. Isolate that from the rest of the streamed response with the following code:
```python
for chunk in response_stream:
        if hasattr(chunk, "choices") and chunk.choices:
            # If the content is None, we replace it with "" (empty string)
            partial_text = getattr(chunk.choices[0].delta, "content", "") or ""
            streamed_text.append(partial_text)
            print(partial_text, end="", flush=True)

    print("\n")  # new line after streaming ends
```

### Multi-Turn Conversation Handling

LiteLLM can facilitate multi-turn conversations by maintaining message history in a sequential chain, enabling the model to consider the context of previous messages. This section demonstrates multi-turn conversation handling by updating the messages list each time we receive a new response from the assistant. This pattern can be repeated for as many turns as you need, continuously appending messages to maintain the conversational flow.

Let's take a closer look at each step:

1. First, we need to combine the streamed chunks of the first message. Since they were streamed, they need to be re-assembled into a single message. After collecting partial responses in `streamed_text`, join them into a single string called `complete_first_answer`.
```python
complete_first_answer = "".join(streamed_text)
```
2. Next, append the assistant’s reply to enhance the context of the conversation. Add this `complete_first_answer` back into messages under the "assistant" role as follows:
```python
messages.append({"role": "assistant", "content": complete_first_answer})
```
3. Then, craft the 2nd message to the assistant. Append a new message object to messages with the user’s next question as follows:
```python
messages.append({
        "role": "user",
        "content": (
            "Thanks for that. Can you propose a short, 3-minute presentation outline "
            "about the Gold Rush, focusing on its broader implications?"
        ),
    })
```
4. Now, ask the model for the response to the 2nd question, this time without the streaming feature enabled. Pass the updated messages to completion() with `stream=False`, prompting LiteLLM to generate a standard (single-shot) response as follows:
```python
response_2 = completion(
            model=model,
            messages=messages,
            max_tokens=300,
            temperature=0.6,
            stream=False  # non-streamed
        )
    except Exception as err:
        print(f"Error calling model: {err}")
        return
```
5. Finally, parse and print the second answer. Extract `response_2.choices[0].message["content"]`, store it in `second_answer_text`, and print to the console for your final output: 
```python
second_answer_text = ""
    if response_2.choices and hasattr(response_2.choices[0], "message"):
        second_answer_text = response_2.choices[0].message.get("content", "") or ""

    print(second_answer_text)
```

### Putting it All Together

You can find the full code file below, demonstrating a comparison of a streamed response vs. a regular response alongside handling a multi-turn conversation. 

??? code "litellm-features.py"
    ```python
    import os

import litellm.exceptions
from litellm import completion

# Set environment variables for kluster.ai
os.environ["OPENAI_API_KEY"] = "INSERT_API_KEY"  # Replace with your key
os.environ["OPENAI_API_BASE"] = "https://api.kluster.ai/v1"

def main():
    model = "openai/klusterai/Meta-Llama-3.3-70B-Instruct-Turbo"

    messages = [
        {"role": "system", "content": "You are a helpful AI assistant."},
        {"role": "user",   "content": "Explain the significance of the California Gold Rush."},
    ]

    # --- 1) STREAMING CALL: Only print chunk text --------------------------------
    try:
        response_stream = completion(
            model=model,
            messages=messages,
            max_tokens=300,
            temperature=0.3,
            stream=True,  # streaming enabled
        )
    except Exception as err:
        print(f"Error calling model: {err}")
        return

    print("\n--------- STREAMING RESPONSE (text only) ---------")
    streamed_text = []

    # Iterate over each chunk from the streaming generator
    for chunk in response_stream:
        if hasattr(chunk, "choices") and chunk.choices:
            # If the content is None, we replace it with "" (empty string)
            partial_text = getattr(chunk.choices[0].delta, "content", "") or ""
            streamed_text.append(partial_text)
            print(partial_text, end="", flush=True)

    print("\n")  # new line after streaming ends

    # Combine the partial chunks into one string
    complete_first_answer = "".join(streamed_text)

    # Append the entire first answer to the conversation for multi-turn context
    messages.append({"role": "assistant", "content": complete_first_answer})

    # --- 2) SECOND CALL (non-streamed): Print just the text ---------------------
    messages.append({
        "role": "user",
        "content": (
            "Thanks for that. Can you propose a short, 3-minute presentation outline "
            "about the Gold Rush, focusing on its broader implications?"
        ),
    })

    try:
        response_2 = completion(
            model=model,
            messages=messages,
            max_tokens=300,
            temperature=0.6,
            stream=False  # non-streamed
        )
    except Exception as err:
        print(f"Error calling model: {err}")
        return

    print("--------- RESPONSE 2 (non-streamed, text only) ---------")
    second_answer_text = ""
    if response_2.choices and hasattr(response_2.choices[0], "message"):
        second_answer_text = response_2.choices[0].message.get("content", "") or ""

    print(second_answer_text)

if __name__ == "__main__":
    main()
    ```

Upon running it you'll see output like the following:

<div id="termynal" data-termynal>
    <span data-ty="input"><span class="file-path">python streaming-litellm.py</span></span>
    <span data-ty>--------- STREAMING RESPONSE (text only) ---------</span>
    <span data-ty>The California Gold Rush, which occurred from 1848 to 1855, was a pivotal event in American history that had significant economic, social, and cultural impacts on the United States and the world. Here are some of the key reasons why the California Gold Rush was important:</span>
    <span data-ty>1. **Mass Migration and Population Growth**: The Gold Rush triggered a massive influx of people to California, with estimates suggesting that over 300,000 people arrived in the state between 1848 and 1852. This migration helped to populate the western United States and contributed to the country's westward expansion.</span>
    <span data-ty>2. **Economic Boom**: The Gold Rush created a huge economic boom, with thousands of people striking it rich and investing their newfound wealth in businesses, infrastructure, and other ventures. The gold rush helped to stimulate economic growth, create new industries, and establish California as a major economic hub.</span>
    <span data-ty>3. **Technological Innovations**: The Gold Rush drove technological innovations, particularly in the areas of mining and transportation. The development of new mining techniques, such as hydraulic mining, and the construction of roads, bridges, and canals, helped to facilitate the extraction and transportation of gold.</span>
    <span data-ty>4. **Impact on Native American Populations**: The Gold Rush had a devastating impact on Native American populations in California, who were forcibly removed from their lands, killed, or displaced by the influx of miners. The Gold Rush marked the beginning of a long and tragic period of colonization and marginalization for Native American communities in</span>
    <span data-ty>--------- RESPONSE 2 (non-streamed, text only) ---------</span>
    <span data-ty>Here's a suggested 3-minute presentation outline on the California Gold Rush, focusing on its broader implications:</span>
    <span data-ty>**Title:** The California Gold Rush: A Catalyst for Change</span>
    <span data-ty>**Introduction (30 seconds)**</span>
    <span data-ty>* Briefly introduce the California Gold Rush and its significance</span>
    <span data-ty>* Thesis statement: The California Gold Rush was a pivotal event in American history that had far-reaching implications for the country's economy, society, and politics.</span>
    <span data-ty>**Section 1: Economic Implications (45 seconds)**</span>
    <span data-ty>* Discuss how the Gold Rush stimulated economic growth and helped establish California as a major economic hub</span>
    <span data-ty>* Mention the impact on trade, commerce, and industry, including the growth of San Francisco and other cities</span>
    <span data-ty>* Highlight the role of the Gold Rush in shaping the US economy and contributing to the country's westward expansion</span>
    <span data-ty>**Section 2: Social and Cultural Implications (45 seconds)**</span>
    <span data-ty>* Discuss the impact of the Gold Rush on Native American populations, including forced removals, violence, and displacement</span>
    <span data-ty>* Mention the diversity of people who came to California during the Gold Rush, including immigrants from China, Latin America, and Europe</span>
    <span data-ty>* Highlight the social and cultural changes that resulted from this diversity, including the growth of cities and the development of new communities</span>
    <span data-ty>**Section 3: Lasting Legacy (45 seconds)**</span>
    <span data-ty>* Discuss the lasting legacy of the Gold Rush, including its contribution to the development of the US West Coast and the growth of the US economy</span>
    <span data-ty>* Mention the ongoing impact of the Gold</span>
</div>

Both responses appear to trail off abruptly, but that's because we limited the output to `300` tokens each. Feel free to tweak the parameters and rerun the script at your leisure!
--- END CONTENT ---

Doc-Content: https://docs.kluster.ai/get-started/integrations/sillytavern/
--- BEGIN CONTENT ---
---
title: Integrate SillyTavern with kluster.ai
description: This guide walks you through setting up SillyTavern, a customizable LLM interface, with the kluster.ai API to enable AI-powered conversations.
---

# How to integrate SillyTavern with kluster.ai

This guide will help you set up and configure [SillyTavern](https://sillytavernai.com/){target=\_blank}, a customizable LLM interface, with the [kluster.ai](https://www.kluster.ai/){target=\_blank} API. Follow the steps below to integrate these tools seamlessly.

## Prerequisites

Before starting, ensure you have the following:

- **SillyTavern installed** - for installation instructions, refer to the SillyTavern [Installation](https://docs.sillytavern.app/installation/){target=\_blank} guide
- **A kluster.ai account** - sign up on the [kluster.ai platform](https://platform.kluster.ai/signup){target=\_blank} if you don't have one
- **A kluster.ai API key** - after signing in, go to the [**API Keys**](https://platform.kluster.ai/apikeys){target=\_blank} section and create a new key. For detailed instructions, check out the [Get an API key](/get-started/get-api-key/){target=\_blank} guide

## Configure SillyTavern to use kluster.ai

1. Launch SillyTavern and open it in your browser at `http://127.0.0.1:8000/` (default port)
2. Click on the **API Connections** icon (plug) in the top navigation menu
3. In the **API** drop-down menu, select **Chat Completion**
4. In the **Chat Completion Source** option, choose **Custom (OpenAI-compatible)**
5. Enter the **kluster.ai** API endpoint in the **Custom Endpoint (Base URL)** field:

    ```text
    https://api.kluster.ai/v1
    ```

    There should be no trailing slash (`/`) at the end of the URL

6. Paste your **kluster.ai** API Key into the designated field
7. **Enter a Model ID**. For this example, you can enter:

    ```text
    klusterai/Meta-Llama-3.3-70B-Instruct-Turbo
    ```

8. Click the **Connect** button. If you've configured the API correctly, you should see a **🟢 Valid** message next to the button
9. Select one of the kluster.ai-supported models from the **Available Models** drop-down menu

![](/images/get-started/integrations/sillytavern/sillytavern-1.webp)

That's it! You're now ready to start chatting with your bot powered by kluster.ai.

## Test the connection

Now that you've configured kluster.ai with SillyTavern, you can test the API connection by starting a new conversation.

Follow these steps to get started:

1. Click the menu icon on the bottom-left corner of the page
2. Select **Start New Chat** to open a new chat with the model
3. Type a message in the **Type a message** bar at the bottom and send it
4. Verify that the chatbot has returned a response successfully

![](/images/get-started/integrations/sillytavern/sillytavern-2.webp)

!!! tip "Troubleshooting"
    If you encounter errors, revisit the [configuration instructions](#configure-sillytavern-to-use-klusterai) and double-check your API key and base URL and that you've received a **Valid** response after connecting the API (see step 8).
--- END CONTENT ---

Doc-Content: https://docs.kluster.ai/get-started/openai-compatibility/
--- BEGIN CONTENT ---
---
title: Compatibility with OpenAI client libraries
description: Learn how kluster.ai is fully compatible with OpenAI client libraries, enabling seamless integration with your existing applications.
---

# OpenAI compatibility

The [kluster.ai](https://www.kluster.ai/){target=\_blank} API is compatible with [OpenAI](https://platform.openai.com/docs/api-reference/introduction){target=\_blank}'s API and SDKs, allowing seamless integration into your existing applications.

If you already have an application running with the OpenAI client library, you can easily switch to kluster.ai's API with minimal changes. This ensures a smooth transition without the need for significant refactoring or rework.

## Configuring OpenAI to use kluster.ai's API

To start using kluster.ai with OpenAI's client libraries, set your [API key](/get-started/get-api-key/){target=\_blank} and change the base URL to `https://api.kluster.ai/v1`:

=== "Python"

    ```python
    from openai import OpenAI
    import json

    client = OpenAI(
        base_url="https://api.kluster.ai/v1",
        api_key="INSERT_API_KEY",  # Replace with your actual API key
    )
    ```

## Unsupported OpenAI features

While kluster.ai's API is largely compatible with OpenAI's, the following sections outline the specific features and fields that are currently unsupported.

### Chat completions endpoint

When creating a chat completion via the [`POST https://api.kluster.ai/v1/chat/completions` endpoint](/api-reference/reference#create-chat-completion){target=\_blank}, the following request parameters are not supported:

- `messages[].name` - attribute in `system`, `user`, and `assistant` type message objects
- `messages[].refusal` - attribute in `assistant` type message objects
- `messages[].audio` - attribute in `assistant` type message objects
- `messages[].tool_calls` - attribute in `assistant` type message objects
- `store`
- `n`
- `modalities`
- `response_format`
- `service_tier`
- `stream_options`
- `tools`
- `tool_choice`
- `parallel_tool_calls`

The following request parameters are *deprecated*:

- `messages[].function_call` - attribute in `assistant` type message objects <!-- TODO: Once `messages[].tool_calls` is supported, this should be updated to use `messages[].tool_calls instead -->
- `max_tokens` - use `max_completion_tokens` instead
- `function_call` <!-- TODO: Once `tool_choice` is supported, this should be updated to use `tool_choice` instead -->
- `functions` <!-- TODO: Once `tools` is supported, this should be updated to use `tools` instead -->

For more information on these parameters, refer to [OpenAI's API documentation on creating chat completions](https://platform.openai.com/docs/api-reference/chat/create){target=_blank}.

### Chat completion object

The following fields of the [chat completion object](/api-reference/reference/#chat-completion-object) are not supported:

- `system_fingerprint`
- `usage.completion_tokens_details`
- `usage.prompt_tokens_details`

For more information on these parameters, refer to [OpenAI's API documentation on the chat completion object](https://platform.openai.com/docs/api-reference/chat/object){target=_blank}.
--- END CONTENT ---

Doc-Content: https://docs.kluster.ai/get-started/start-api/
--- BEGIN CONTENT ---
---
title: Start building with the kluster.ai API
description: The kluster.ai API getting started guide provides examples and instructions for submitting and managing Batch jobs using kluster.ai's OpenAI-compatible API.
---

# Start using the kluster.ai API

The [kluster.ai](https://www.kluster.ai/){target=\_blank} API provides a straightforward way to work with Large Language Models (LLMs) at scale. It is compatible with OpenAI's API and SDKs, making it easy to integrate into your existing workflows with minimal code changes.

This guide provides copy-and-paste examples for both Python and curl (although all OpenAI's SDKs are supported) and detailed explanations to help you get started quickly.

## Install prerequisites

The OpenAI Python library (version 1.0.0 or higher) is recommended, which can be installed with:

```bash
pip install "openai>=1.0.0"
```

## Get your API key

Navigate to the kluster.ai developer console [**API Keys**](https://platform.kluster.ai/apikeys){target=\_blank} section and create a new key from there. You'll need this for all API requests.

For step-by-step instructions, refer to the [Get an API key](/get-started/get-api-key){target=\_blank} guide.

## API request limits

The following limits apply to API requests based upon your plan tier:

| Restriction                   | Free tier |           Standard tier            |
|-------------------------------|:---------:|:----------------------------------:|
| **Context size**              |    32k    | 164k (deepseek-r1) / 131k (others) |
| **Max output**                |    4k     | 164k (deepseek-r1) / 131k (others) |
| **Concurrent requests**       |     2     |                 10                 |
| **Request limit**             |   1/min   |               60/min               |
| **Realtime request priority** | Standard  |              Standard              |
| **Batch request priority**    | Standard  |                High                |

## Batch job workflow overview

Working with Batch jobs in the kluster.ai API involves the following steps:

1. **Create Batch job file** - prepare a JSON Lines file containing one or more chat completion requests to be executed in the batch
2. **Upload Batch job file** - upload the file to kluster.ai to receive a unique file ID
3. **Start the Batch job** - initiate a new Batch job using the file ID
4. **Monitor job progress** - track the status of your Batch job to ensure successful completion
5. **Retrieve results** - once the job finishes, access and process the results as needed

This streamlined process enables efficient handling of large-scale requests.

In addition to these core steps, this guide will give you hands-on experience with:

- **Cancel a Batch job** - cancel an ongoing Batch job if necessary before it completes
- **List all Batch jobs** - review all of your Batch jobs

## Create Batch jobs as JSON files

To take the first step in the Batch job workflow, you'll need to assemble your Batch requests and add them to a [JSON Lines](https://jsonlines.org/) file (`.jsonl`).

Each request needs to include the following arguments:

- `custom_id` ++"string"++ - a unique request ID that will be used to match outputs to inputs
- `method` ++"string"++ - the HTTP method to be used for the request. Currently, only `POST` is supported
- `url` ++"string"++ -  the `/v1/chat/completions` endpoint
- `body` ++"object"++ - a request body containing:
    - `model` ++"string"++ <span class="required" markdown>++"required"++</span> - name of the `model` to use, can be one of:
        - `klusterai/Meta-Llama-3.1-8B-Instruct-Turbo`
        - `klusterai/Meta-Llama-3.1-405B-Instruct-Turbo`
        - `klusterai/Meta-Llama-3.3-70B-Instruct-Turbo`
        - `deepseek-ai/DeepSeek-R1`

        !!! tip
            You can see the full list of available models programmatically using the [list supported models](#list-supported-models) endpoint.

    - `messages` ++"array"++ <span class="required" markdown>++"required"++</span> - a list of chat messages (`system`, `user`, or `assistant` roles)
    - Any optional [chat completion parameters](/api-reference/reference/#create-chat-completion){target=\_blank}, such as `temperature`, `max_completion_tokens`, etc.

The following examples generate requests and save them in a JSONL file, ready for upload and processing.

=== "Python"

    ```python
    from openai import OpenAI
    import json

    client = OpenAI(
        base_url="https://api.kluster.ai/v1",
        api_key="INSERT_API_KEY",  # Replace with your actual API key
    )

    requests = [
        {
            "custom_id": "request-1",
            "method": "POST",
            "url": "/v1/chat/completions",
            "body": {
                "model": "klusterai/Meta-Llama-3.1-8B-Instruct-Turbo",
                "messages": [
                    {"role": "system", "content": "You are a helpful assistant."},
                    {"role": "user", "content": "What is the capital of Argentina?"},
                ],
                "max_completion_tokens": 1000,
            },
        },
        {
            "custom_id": "request-2",
            "method": "POST",
            "url": "/v1/chat/completions",
            "body": {
                "model": "klusterai/Meta-Llama-3.3-70B-Instruct-Turbo",
                "messages": [
                    {"role": "system", "content": "You are a maths tutor."},
                    {"role": "user", "content": "Explain the Pythagorean theorem."},
                ],
                "max_completion_tokens": 1000,
            },
        },
        {
            "custom_id": "request-4",
            "method": "POST",
            "url": "/v1/chat/completions",
            "body": {
                "model": "klusterai/Meta-Llama-3.3-70B-Instruct-Turbo",
                "messages": [
                    {
                        "role": "system",
                        "content": "You are a multilingual, experienced maths tutor.",
                    },
                    {
                        "role": "user",
                        "content": "Explain the Pythagorean theorem in Spanish",
                    },
                ],
                "max_completion_tokens": 1000,
            },
        },
        # Additional tasks can be added here
    ]

    # Save tasks to a JSONL file (newline-delimited JSON)
    file_name = "mybatchtest.jsonl"
    with open(file_name, "w") as file:
        for request in requests:
            file.write(json.dumps(request) + "\n")
    ```

=== "curl"

    ```bash
    cat << EOF > mybatchtest.jsonl
    {"custom_id": "request-1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "klusterai/Meta-Llama-3.1-8B-Instruct-Turbo", "messages": [{"role": "system", "content": "You are a helpful assistant."}, {"role": "user", "content": "What is the capital of Argentina?"}],"max_completion_tokens":1000}}
    {"custom_id": "request-2", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "klusterai/Meta-Llama-3.3-70B-Instruct-Turbo", "messages": [{"role": "system", "content": "You are an experienced maths tutor."}, {"role": "user", "content": "Explain the Pythagorean theorem."}],"max_completion_tokens":1000}}
    {"custom_id": "request-3", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "klusterai/Meta-Llama-3.1-405B-Instruct-Turbo", "messages": [{"role": "system", "content": "You are an astronomer."}, {"role": "user", "content": "What is the distance between the Earth and the Moon"}],"max_completion_tokens":1000}}
    {"custom_id": "request-4", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "klusterai/Meta-Llama-3.3-70B-Instruct-Turbo", "messages":[{"role": "system", "content": "You are a multilingual, experienced maths tutor."}, {"role": "user", "content": "Explain the Pythagorean theorem in Spanish"}],"max_completion_tokens":1000}}
    EOF
    ```

## Upload Batch job files

Upload your [JSON Lines](https://jsonlines.org/){target=\_blank} file to the `files` endpoint along with the intended purpose of the upload. For Batch jobs, set the `purpose` value to `"batch"`.

The response will contain an `id` field; save this value as you'll need it in the next step, where it's referred to as `input_file_id`.

!!! note
    You can also view all your uploaded files in the [**Files** tab](https://platform.kluster.ai/files){target=\_blank} of the kluster.ai platform.

=== "Python"

    ```python title="Example request"

    batch_input_file = client.files.create(
        file=open(file_name, "rb"),
        purpose="batch"
    )
    ```

=== "curl"

    ```bash title="Example request"
    curl -s https://api.kluster.ai/v1/files \
        -H "Authorization: Bearer $API_KEY" \
        -H "Content-Type: multipart/form-data" \
        -F "file=@mybatchtest.jsonl" \
        -F "purpose=batch"
    ```

```Json title="Response"
{
    "id": "myfile-123",
    "bytes": 2797,
    "created_at": "1733832768",
    "filename": "mybatchtest.jsonl",
    "object": "file",
    "purpose": "batch"
}
```

## Submit a Batch job

Next, submit a Batch job by calling the `batches` endpoint and providing the `id` of the uploaded Batch job file (from the previous section) as the [`input_file_id`, and additional parameters](/api-reference/reference/#submit-a-batch-job){target=\_blank} to specify the job's configuration.

The response includes an `id` that can be used to monitor the job's progress, as demonstrated in the next section.

=== "Python"

    ```python title="Example request"

    batch_request = client.batches.create(
        input_file_id=batch_input_file.id,
        endpoint="/v1/chat/completions",
        completion_window="24h",
    )
    ```

=== "curl"

    ```bash title="Example request"
    curl -s https://api.kluster.ai/v1/batches \
        -H "Authorization: Bearer $API_KEY" \
        -H "Content-Type: application/json" \
        -d '{
        "input_file_id": "myfile-123",
        "endpoint": "/v1/chat/completions",
        "completion_window": "24h"
        }'
    ```

```Json title="Response"
{
    "id": "mybatch-123",
    "completion_window": "24h",
    "created_at": 1733832777,
    "endpoint": "/v1/chat/completions",
    "input_file_id": "myfile-123",
    "object": "batch",
    "status": "validating",
    "cancelled_at": null,
    "cancelling_at": null,
    "completed_at": null,
    "error_file_id": null,
    "errors": null,
    "expired_at": null,
    "expires_at": 1733919177,
    "failed_at": null,
    "finalizing_at": null,
    "in_progress_at": null,
    "metadata": {},
    "output_file_id": null,
    "request_counts": {
        "completed": 0,
        "failed": 0,
        "total": 0
    }
}
```

## Monitor job progress

To monitor your Batch job's progress, make periodic requests to the `batches` endpoint using the `id` of the Batch request (from the previous section) as the [`batch_id`](/api-reference/reference/#retrieve-a-batch){target=\_blank} to check its status. The job is complete when the `status` field returns `"completed"`.

To see a complete list of the supported statuses, refer to the [Retrieve a batch](/api-reference/reference/#retrieve-a-batch){target=\_blank} API reference page.

!!! note
    You can also monitor jobs in the [**Batch** tab](https://platform.kluster.ai/batch) of the kluster.ai platform UI.

=== "Python"

    ```python title="Example request"
    import time

    # Poll the Batch status until it's complete
    while True:
        batch_status = client.batches.retrieve(batch_request.id)
        print("Batch status: {}".format(batch_status.status))
        print(
            f"Completed tasks: {batch_status.request_counts.completed} / {batch_status.request_counts.total}"
        )

        if batch_status.status.lower() in ["completed", "failed", "cancelled"]:
            break

        time.sleep(10)  # Wait for 10 seconds before checking again
    ```

=== "curl"

    ```bash title="Example request"
    curl -s https://api.kluster.ai/v1/batches/mybatch-123 \
        -H "Authorization: Bearer $API_KEY" \
        -H "Content-Type: application/json"
    ```

```Json title="Response"
{
    "id": "mybatch-123",
    "object": "batch",
    "endpoint": "/v1/chat/completions",
    "errors": null,
    "input_file_id": "myfile-123",
    "completion_window": "24h",
    "status": "completed",
    "output_file_id": "myfile-123-output",
    "error_file_id": null,
    "created_at": "1733832777",
    "in_progress_at": "1733832777",
    "expires_at": "1733919177",
    "finalizing_at": "1733832781",
    "completed_at": "1733832781",
    "failed_at": null,
    "expired_at": null,
    "cancelling_at": null,
    "cancelled_at": null,
    "request_counts": {
        "total": 4,
        "completed": 4,
        "failed": 0
    },
    "metadata": {}
}
```

## Retrieve results

To retrieve the content of your Batch jobs output file, send a request to the `files` endpoint specifying the `output_file_id`, which is returned from querying the Batch's status (from the previous section).

The output file will be a JSONL file, where each line contains the `custom_id` from your input file request and the corresponding response.

=== "Python"

    ```python title="Example request"
    # Check if the Batch completed successfully
    if batch_status.status.lower() == "completed":
        # Retrieve the results
        result_file_id = batch_status.output_file_id
        results = client.files.content(result_file_id).content

        # Save results to a file
        result_file_name = "batch_results.jsonl"
        with open(result_file_name, "wb") as file:
            file.write(results)
        print(f"Results saved to {result_file_name}")
    else:
        print(f"Batch failed with status: {batch_status.status}")
    ```

=== "curl"

    ```bash title="Example request"
    curl -s https://api.kluster.ai/v1/files/kluster-output-file-123/content \
        -H "Authorization: Bearer $API_KEY" > batch_output.jsonl
    ```

## List all Batch jobs

To list all of your Batch jobs, send a request to the `batches` endpoint without specifying a `batch_id`. To constrain the query response, you can also use a `limit` parameter.

=== "Python"

    ```python title="Example request"
    from openai import OpenAI

    # Configure OpenAI client
    client = OpenAI(
        base_url="https://api.kluster.ai/v1", 
        api_key="INSERT_API_KEY" # Replace with your actual API key
    )

    print(client.batches.list(limit=2).to_dict())
    ```

=== "curl"

    ```bash title="Example request" 
    curl -s https://api.kluster.ai/v1/batches \
        -H "Authorization: Bearer $API_KEY"
    ```

```Json title="Response"
{
"object": "list",
"data": [
    {
    "id": "mybatch-123",
    "object": "batch",
    "endpoint": "/v1/chat/completions",
    "errors": null,
    "input_file_id": "myfile-123",
    "completion_window": "24h",
    "status": "completed",
    "output_file_id": "myfile-123-output",
    "error_file_id": null,
    "created_at": "1733832777",
    "in_progress_at": "1733832777",
    "expires_at": "1733919177",
    "finalizing_at": "1733832781",
    "completed_at": "1733832781",
    "failed_at": null,
    "expired_at": null,
    "cancelling_at": null,
    "cancelled_at": null,
    "request_counts": {
        "total": 4,
        "completed": 4,
        "failed": 0
    },
    "metadata": {}
    },
{ ... },
],
"first_id": "mybatch-123",
"last_id": "mybatch-789",
"has_more": false,
"count": 1,
"page": 1,
"page_count": -1,
"items_per_page": 9223372036854775807
}
```

## Cancel a Batch job

To cancel a Batch job currently in progress, send a request to the `cancel` endpoint with your `batch_id`. Note that cancellation may take up to 10 minutes to complete, during which time the status will show as `cancelling`. Once complete, the status will show as `cancelled`.

=== "Python"

    ```python title="Example"
    from openai import OpenAI

    client = OpenAI(
        base_url="https://api.kluster.ai/v1",  
        api_key="INSERT_API_KEY" # Replace with your actual API key
    )
    client.batches.cancel("mybatch-123") # Replace with your Batch id
    ```

=== "curl"

    ```bash title="Example"
    curl -s https://api.kluster.ai/v1/batches/$BATCH_ID/cancel \
        -H "Authorization: Bearer $API_KEY" \
        -H "Content-Type: application/json" \
        -X POST
    ```

```Json title="Response"
{
    "id": "mybatch-123",
    "object": "batch",
    "endpoint": "/v1/chat/completions",
    "errors": null,
    "input_file_id": "myfile-123",
    "completion_window": "24h",
    "status": "cancelling",
    "output_file_id": "myfile-123-output",
    "error_file_id": null,
    "created_at": "1730821906",
    "in_progress_at": "1730821911",
    "expires_at": "1730821906",
    "finalizing_at": null,
    "completed_at": null,
    "failed_at": null,
    "expired_at": null,
    "cancelling_at": "1730821906",
    "cancelled_at": null,
    "request_counts": {
        "total": 3,
        "completed": 3,
        "failed": 0
    },
    "metadata": {}
}
```

## Summary

Congratulations! You now have all the tools needed to work with the kluster.ai Batch API. In this guide, you've learned how to:

- Prepare and submit Batch jobs with structured request inputs
- Track your jobs' progress in real-time
- Retrieve and handle job results
- View and manage your Batch jobs
- Cancel jobs when needed
- View supported models

The kluster.ai Batch API is designed to efficiently and reliably handle your large-scale LLM workloads. Do you have questions or suggestions? The [support](mailto:support@kluster.ai){target=\_blank} team would love to hear from you.
--- END CONTENT ---

